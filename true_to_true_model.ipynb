{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-20T14:29:35.240342Z",
     "start_time": "2024-04-20T14:29:35.234125Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.BATCH_SIZE = 50\n",
    "        self.EPOCHS = 30\n",
    "        self.NUM_OF_INPUTS = 20000\n",
    "\n",
    "        self.NUM_CHARACTER_LEVEL_LAYERS = 4\n",
    "        self.NUM_WORD_LEVEL_LAYERS = 12\n",
    "\n",
    "        self.CHARACTER_LEVEL_D_MODEL = 64\n",
    "        self.WORD_LEVEL_D_MODEL = 512\n",
    "\n",
    "        self.NUM_HEADS = 3\n",
    "        self.DFF = 256\n",
    "\n",
    "        self.MAX_WORD_LENGTH = 16\n",
    "        self.MAX_SENTENCE_LENGTH = 64\n",
    "\n",
    "        self.VOCAB_SIZE = 10000\n",
    "        self.CHARACTER_VOCAB_SIZE = 2000\n",
    "\n",
    "\n",
    "config = Config()"
   ],
   "id": "1d94a7e3e9c116fa",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-20T14:25:23.388269Z",
     "start_time": "2024-04-20T14:25:16.905780Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ],
   "id": "4d4605e96cf0317a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-20 21:25:18.659578: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-20 21:25:19.115773: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-20 21:25:21.278292: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Global Attention and FeedForward",
   "id": "6bf6f107d372af26"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "        self.add = tf.keras.layers.Add()"
   ],
   "id": "cd141c3599d41736",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class GlobalSelfAttention(BaseAttention):\n",
    "    def call(self, inputs):\n",
    "        x, mask = inputs[0], inputs[1]\n",
    "\n",
    "        attn_output = self.mha(query=x, value=x, key=x, attention_mask=mask)\n",
    "\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "        return x"
   ],
   "id": "d59c351bb857beb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class FeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, dff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.seq = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(dff, activation='relu'),\n",
    "            tf.keras.layers.Dense(d_model),\n",
    "            tf.keras.layers.Dropout(dropout_rate)\n",
    "        ])\n",
    "        self.add = tf.keras.layers.Add()\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.add([x, self.seq(x)])\n",
    "        x = self.layer_norm(x)\n",
    "        return x"
   ],
   "id": "7bbb59f3cd166473",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Positional Embedding layer",
   "id": "ce082eb8b3293069"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def positional_encoding(length, depth):\n",
    "    depth = depth / 2\n",
    "\n",
    "    positions = np.arange(length)[:, np.newaxis]\n",
    "    depths = np.arange(depth)[np.newaxis, :] / depth\n",
    "\n",
    "    angle_rates = 1 / (10000 ** depths)\n",
    "    angle_rads = positions * angle_rates\n",
    "\n",
    "    pos_encoding = np.concatenate([np.sin(angle_rads), np.cos(angle_rads)], axis=-1)\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "\n",
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True)\n",
    "        self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n",
    "\n",
    "    def compute_mask(self, *args, **kwargs):\n",
    "        return self.embedding.compute_mask(*args, **kwargs)\n",
    "\n",
    "    def call(self, x):\n",
    "        length = tf.shape(x)[1]\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
    "        return x"
   ],
   "id": "5094824d30ca609d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Encoder layers",
   "id": "9a0a371a77c0f8c6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, *, d_model, num_heads, dff, dropout_rate=0.1, name='EncoderLayer'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attention = GlobalSelfAttention(\n",
    "            num_heads=num_heads, key_dim=d_model, dropout=dropout_rate, name=name\n",
    "        )\n",
    "\n",
    "        self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.self_attention(inputs)\n",
    "        x = self.ffn(x)\n",
    "        return x\n"
   ],
   "id": "d14cfd02447443b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "            self, *, num_layers, d_model, num_heads, dff, vocab_size, dropout_rate=0.1,\n",
    "            name='Encoder'\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        self.enc_layers = [\n",
    "            EncoderLayer(\n",
    "                d_model=d_model, num_heads=num_heads, dff=dff,\n",
    "                dropout_rate=dropout_rate,\n",
    "                name=f'encoder_layer_{i + 1}'\n",
    "            )\n",
    "            for i in range(num_layers)\n",
    "        ]\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, mask = inputs\n",
    "        x = self.dropout(x)\n",
    "        for i in range(self.num_layers):\n",
    "            inputs = self.enc_layers[i]((x, mask))\n",
    "        return inputs  # Shape (batch_size, seq_len, d_model)."
   ],
   "id": "cb1b421d1ee7f65d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Hierarchical Transformer Encoder model",
   "id": "d36df6c730339222"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "@keras.utils.register_keras_serializable()\n",
    "class HierarchicalTransformerEncoder(tf.keras.models.Model):\n",
    "    def __init__(self, *, num_character_level_layers, num_word_level_layers,\n",
    "                 character_level_d_model, word_level_d_model, num_heads, dff,\n",
    "                 max_word_length, max_sentence_length,\n",
    "                 vocab_size, character_vocab_size, dropout_rate=0.1, **kwargs):\n",
    "        super(HierarchicalTransformerEncoder, self).__init__(**kwargs)\n",
    "\n",
    "        self.num_character_level_layers = num_character_level_layers\n",
    "        self.num_word_level_layers = num_word_level_layers\n",
    "        self.character_level_d_model = character_level_d_model\n",
    "        self.word_level_d_model = word_level_d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.dff = dff\n",
    "        self.max_word_length = max_word_length\n",
    "        self.max_sentence_length = max_sentence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.character_vocab_size = character_vocab_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.word_pos_embedding = PositionalEmbedding(vocab_size=vocab_size,\n",
    "                                                      d_model=word_level_d_model)\n",
    "        self.character_pos_embedding = PositionalEmbedding(vocab_size=character_vocab_size,\n",
    "                                                           d_model=character_level_d_model)\n",
    "\n",
    "        self.character_level_encoder = Encoder(num_layers=num_character_level_layers,\n",
    "                                               d_model=character_level_d_model,\n",
    "                                               num_heads=num_heads, dff=dff,\n",
    "                                               vocab_size=character_vocab_size,\n",
    "                                               dropout_rate=dropout_rate,\n",
    "                                               name='character_level_encoder')\n",
    "\n",
    "        self.flatten_layer = tf.keras.layers.Flatten(input_shape=(max_word_length, character_level_d_model),\n",
    "                                                     name='flatten_character_level_encoders')\n",
    "        self.linear_layer = tf.keras.layers.Dense(units=word_level_d_model, activation=None,\n",
    "                                                  name='linear_character_level_encoders')\n",
    "\n",
    "        self.combined_layer = tf.keras.layers.Concatenate(axis=-1, name='combined_character_and_word_level_encoders')\n",
    "\n",
    "        self.word_level_encoder = Encoder(num_layers=num_word_level_layers,\n",
    "                                          d_model=(word_level_d_model * 2),\n",
    "                                          num_heads=num_heads, dff=dff,\n",
    "                                          vocab_size=vocab_size,\n",
    "                                          dropout_rate=dropout_rate,\n",
    "                                          name='word_level_encoder')\n",
    "\n",
    "        self.correction_layer = tf.keras.layers.Dense(vocab_size, activation='softmax', name='correction_layer', )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # print(\"Start\")\n",
    "        word_level_inputs, character_level_inputs = inputs\n",
    "\n",
    "        word_embedding_outputs = self.word_pos_embedding(word_level_inputs)  # (batch_size, max_len, word_level_d_model)\n",
    "        word_level_mask = self.word_pos_embedding.compute_mask(word_level_inputs)\n",
    "        word_level_mask = word_level_mask[:, tf.newaxis, :]\n",
    "\n",
    "        character_level_encoder_outputs = tf.map_fn(\n",
    "            lambda sentence: self.character_level_encoder(\n",
    "                (self.character_pos_embedding(sentence),\n",
    "                 self.character_pos_embedding.compute_mask(sentence)[:, tf.newaxis, :])),\n",
    "            character_level_inputs,\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "        # (batch_size, max_sen_len, max_word_length, character_level_d_model)\n",
    "\n",
    "        character_level_encoder_outputs = tf.map_fn(\n",
    "            lambda sentence: tf.map_fn(\n",
    "                lambda word: tf.squeeze(self.linear_layer(self.flatten_layer(tf.expand_dims(word, axis=0))), axis=0),\n",
    "                sentence,\n",
    "                dtype=tf.float32\n",
    "            ),\n",
    "            character_level_encoder_outputs,\n",
    "            dtype=tf.float32)\n",
    "        # (batch_size, max_sen_len, word_level_d_model)\n",
    "        # print(\"Shape của character_level_encoder_outputs:\", character_level_encoder_outputs.shape)\n",
    "\n",
    "        concat_output = self.combined_layer([word_embedding_outputs, character_level_encoder_outputs])\n",
    "        # (batch_size, max_sen_len, word_level_d_model * 2)\n",
    "        # print(\"Shape của concat_output:\", concat_output.shape)\n",
    "\n",
    "        word_level_output = self.word_level_encoder((concat_output, word_level_mask))\n",
    "        # (batch_size, max_sen_len, word_level_d_model + character_level_d_model)\n",
    "        # print(\"Shape của word_level_output:\", word_level_output.shape)\n",
    "\n",
    "        correction_output = self.correction_layer(word_level_output)  # (batch_size, max_sen_len, vocab_size)\n",
    "        # print(\"Shape của correction_output:\", correction_output.shape)\n",
    "\n",
    "        # detection_output = tf.squeeze(self.detection_layer(word_level_output), axis=-1)  # (batch_size, max_sen_len)\n",
    "        # print(\"Shape của detection_output:\", detection_output.shape)\n",
    "        # print(\"End\")\n",
    "        return correction_output\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'num_character_level_layers': self.num_character_level_layers,\n",
    "            'num_word_level_layers': self.num_word_level_layers,\n",
    "            'character_level_d_model': self.character_level_d_model,\n",
    "            'word_level_d_model': self.word_level_d_model,\n",
    "            'num_heads': self.num_heads,\n",
    "            'dff': self.dff,\n",
    "            'max_word_length': self.max_word_length,\n",
    "            'max_sentence_length': self.max_sentence_length,\n",
    "            'vocab_size': self.vocab_size,\n",
    "            'character_vocab_size': self.character_vocab_size,\n",
    "            'dropout_rate': self.dropout_rate\n",
    "        }\n",
    "        base_config = super(HierarchicalTransformerEncoder, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def training_step(model, optimizer, x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        correction_output = model(x)\n",
    "        loss1 = correction_loss(y, correction_output)\n",
    "        # loss2 = detection_loss(y[1], detection_output)\n",
    "        total_loss = loss1\n",
    "\n",
    "    gradients = tape.gradient(total_loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "def correction_loss(true_outputs, pred_outputs):\n",
    "    softmax_loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)(true_outputs, pred_outputs)\n",
    "    return softmax_loss\n",
    "\n",
    "\n",
    "def detection_loss(true_detection_infos, pred_detection_infos):\n",
    "    sigmoid_loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)(true_detection_infos, pred_detection_infos)\n",
    "    return sigmoid_loss"
   ],
   "id": "d86e3b5b919c916a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data preprocessing",
   "id": "c52ce7c4a47ea323"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Custom Tokenizer",
   "id": "779e25cbfa313332"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-20T14:38:30.367998Z",
     "start_time": "2024-04-20T14:38:30.343937Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CustomTokenizer:\n",
    "    def __init__(self, word_vocab_size, max_word_len, max_sentence_len):\n",
    "        self.index_word = {1: '[UNK]'}\n",
    "        self.word_count = {'[UNK]': 1}\n",
    "        self.word_index = {'[UNK]': 1}\n",
    "        self.word_vocab_size = word_vocab_size\n",
    "\n",
    "        self.character_index = {'[UNK]': 1}\n",
    "        self.index_character = {1: '[UNK]'}\n",
    "\n",
    "        self.signs = [',', '.', '!', '\\\"', \"\\'\", \"?\", \";\", \")\", \"(\", ':', \"/\", \"+\", \"-\", \"=\",\"`\",\"~\", \"*\", \"^\", \"@\", \"%\", \"&\", \"_\"]\n",
    "\n",
    "        self.max_word_len = max_word_len\n",
    "        self.max_sentence_len = max_sentence_len\n",
    "\n",
    "    def preprocess_texts(self, texts):\n",
    "        texts_preprocessed = []\n",
    "        for text in texts:\n",
    "            for sign in self.signs:\n",
    "                text = text.replace(sign, ' ')\n",
    "\n",
    "            text = text.lower()\n",
    "            texts_preprocessed.append(text)\n",
    "        return texts_preprocessed\n",
    "\n",
    "    def sort(self):\n",
    "        self.word_count = sorted(list(self.word_count.items())[1:], key=lambda x: x[1], reverse=True)\n",
    "        for i, (word, count) in enumerate(self.word_count):\n",
    "            self.index_word[i + 2] = word\n",
    "            self.word_index[word] = i + 2\n",
    "\n",
    "        self.word_count = dict(self.word_count)\n",
    "\n",
    "    def texts_to_sequences(self, texts):\n",
    "        word_level_sequences = []\n",
    "        character_level_sequences = []\n",
    "\n",
    "        texts = self.preprocess_texts(texts)\n",
    "        for text in texts:\n",
    "            words = text.split(' ')\n",
    "            words = [i for i in words if i != '']\n",
    "\n",
    "            tokens = [self.word_index.get(word, 1)\n",
    "                      if (self.word_index.get(word, 1) <= self.word_vocab_size) else 1\n",
    "                      for word in words]\n",
    "            word_level_sequences.append(tokens)\n",
    "\n",
    "            sentence_tokens = []\n",
    "            word_length = []\n",
    "            for word in words:\n",
    "                tokens = [self.character_index.get(i, 1) for i in word]\n",
    "                word_length.append(min(len(tokens), self.max_word_len))\n",
    "                sentence_tokens.append(tokens)\n",
    "            character_level_sequences.append(sentence_tokens)\n",
    "\n",
    "        return word_level_sequences, character_level_sequences\n",
    "\n",
    "    def sequences_to_texts(self, sequences):\n",
    "        texts = []\n",
    "        for sequence in sequences:\n",
    "            texts.append([self.index_word.get(token, 1) for token in sequence])\n",
    "        return texts\n",
    "\n",
    "    def fit_on_texts(self, texts):\n",
    "\n",
    "        texts = self.preprocess_texts(texts)\n",
    "\n",
    "        for text in texts:\n",
    "\n",
    "            # Update word vocab.\n",
    "            words = text.split(' ')\n",
    "            for word in words:\n",
    "                if word == '': continue\n",
    "                self.word_count[word] = self.word_count.get(word, 0) + 1\n",
    "\n",
    "            # Update character vocab\n",
    "            for character in text:\n",
    "                if character == ' ': continue\n",
    "                if self.character_index.get(character, -1) == -1:\n",
    "                    self.character_index[character] = len(self.character_index) + 1\n",
    "\n",
    "                self.index_character[self.character_index[character]] = character\n",
    "\n",
    "        self.sort()"
   ],
   "id": "fdcb553bf57b6307",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Build Dataset.",
   "id": "4d616b1c896d9c47"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-20T14:37:12.259439Z",
     "start_time": "2024-04-20T14:37:12.236146Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, sentences, tokenizer):\n",
    "        self.sentences = sentences\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def build_dataset(self):\n",
    "        (word_level_sequences, character_level_sequences) = self.tokenizer.texts_to_sequences(self.sentences)\n",
    "\n",
    "        target_word_level_sequences = pad_sequences(word_level_sequences.copy(), maxlen=config.MAX_SENTENCE_LENGTH,\n",
    "                                                    padding='post', truncating='post')\n",
    "\n",
    "        tmp_word_level_sequences = []\n",
    "        for sequence in word_level_sequences:\n",
    "            sequence = self.random_word(sequence)\n",
    "            tmp_word_level_sequences.append(sequence)\n",
    "\n",
    "        # Get word_level sequences\n",
    "        word_level_sequences = tmp_word_level_sequences\n",
    "        word_level_sequences = pad_sequences(word_level_sequences, maxlen=config.MAX_SENTENCE_LENGTH,\n",
    "                                             padding='post', truncating='post')\n",
    "\n",
    "        # Get character_level mask matrix.\n",
    "        tmp_character_level_sequences = []\n",
    "        for character_level_sequence in character_level_sequences:\n",
    "            sentence_tmp = []\n",
    "            # With each sentence\n",
    "            for word in character_level_sequence:\n",
    "                # With each word\n",
    "                character_level_sequence = self.random_word(word, word_level=False)\n",
    "                sentence_tmp.append(character_level_sequence)\n",
    "\n",
    "            while len(sentence_tmp) < config.MAX_SENTENCE_LENGTH:\n",
    "                sentence_tmp.append([])\n",
    "            while len(sentence_tmp) > config.MAX_SENTENCE_LENGTH:\n",
    "                del sentence_tmp[-1]\n",
    "            tmp_character_level_sequences.append(\n",
    "                pad_sequences(sentence_tmp, maxlen=config.MAX_WORD_LENGTH,\n",
    "                              padding='post', truncating='post'))\n",
    "\n",
    "        character_level_sequences = tf.convert_to_tensor(tmp_character_level_sequences)\n",
    "\n",
    "        return (word_level_sequences, character_level_sequences), target_word_level_sequences\n",
    "\n",
    "    def random_word(self, sequence, word_level=True):\n",
    "        output = []\n",
    "\n",
    "        # 15% of the tokens would be replaced\n",
    "        for token in sequence:\n",
    "            prob = random.random()\n",
    "\n",
    "            if prob < 0.15:\n",
    "                prob /= 0.15\n",
    "\n",
    "                # 80% chance change token to mask token\n",
    "                if prob < 0.8:\n",
    "                    output.append(0)\n",
    "\n",
    "                # 10% chance change token to random token\n",
    "                elif prob < 0.9:\n",
    "                    if word_level:\n",
    "                        random_token = random.randrange(len(self.tokenizer.word_index) + 1)\n",
    "                        if random_token > self.tokenizer.word_vocab_size:\n",
    "                            random_token = 0\n",
    "                    else:\n",
    "                        random_token = random.randrange(len(self.tokenizer.character_index) + 1)\n",
    "\n",
    "                    output.append(random_token)\n",
    "\n",
    "                # 10% chance change token to current token\n",
    "                else:\n",
    "                    output.append(token)\n",
    "\n",
    "            else:\n",
    "                output.append(token)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def build_test_data(self, sentences):\n",
    "        word_level_sequences, character_level_sequences = self.tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "        word_level_sequences = pad_sequences(word_level_sequences, maxlen=config.MAX_SENTENCE_LENGTH,\n",
    "                                                    padding='post', truncating='post')\n",
    "        \n",
    "        tmp_character_level_sequences = []\n",
    "        for character_level_sequence in character_level_sequences:         \n",
    "            while len(character_level_sequence) < config.MAX_SENTENCE_LENGTH:\n",
    "                character_level_sequence.append([])\n",
    "            while len(character_level_sequence) > config.MAX_SENTENCE_LENGTH:\n",
    "                del character_level_sequence[-1]\n",
    "            tmp_character_level_sequences.append(\n",
    "                pad_sequences(character_level_sequence, maxlen=config.MAX_WORD_LENGTH,\n",
    "                              padding='post', truncating='post'))\n",
    "\n",
    "        character_level_sequences = tf.convert_to_tensor(tmp_character_level_sequences)\n",
    "        return word_level_sequences, character_level_sequences"
   ],
   "id": "168c64739661941e",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-20T14:30:01.497791Z",
     "start_time": "2024-04-20T14:29:53.950168Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = pd.read_csv('data/vi_processed.csv')\n",
    "\n",
    "input_sentences = []\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    if len(input_sentences) == config.NUM_OF_INPUTS: break\n",
    "    input_sentences.append(row.correct_text)\n",
    "\n",
    "correct_texts = input_sentences[:config.NUM_OF_INPUTS]"
   ],
   "id": "dc8b24f26c3728a9",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-20T14:38:38.422383Z",
     "start_time": "2024-04-20T14:38:36.279321Z"
    }
   },
   "cell_type": "code",
   "source": [
    "word_level_tokenizer = CustomTokenizer(word_vocab_size=config.VOCAB_SIZE, max_word_len=config.MAX_WORD_LENGTH,\n",
    "                                       max_sentence_len=config.MAX_SENTENCE_LENGTH)\n",
    "word_level_tokenizer.fit_on_texts(input_sentences[:20000])"
   ],
   "id": "b04c7f90b2c479e4",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-20T14:39:34.691458Z",
     "start_time": "2024-04-20T14:39:34.683813Z"
    }
   },
   "cell_type": "code",
   "source": "word_level_tokenizer.index_word[5011]",
   "id": "f188aac1586e16d4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xang'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dataset = Dataset(input_sentences, word_level_tokenizer)\n",
    "data = dataset.build_dataset()"
   ],
   "id": "cdc8b0c478d130f1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "(input_word_level_sequences, input_character_level_sequences), target_sequences = data",
   "id": "85fb0e2fd30b4c53",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "input_word_level_sequences.shape",
   "id": "b77f51f84e347870",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model building",
   "id": "f42308d87fe8aa3f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = HierarchicalTransformerEncoder(num_character_level_layers=config.NUM_CHARACTER_LEVEL_LAYERS,\n",
    "                                       num_word_level_layers=config.NUM_WORD_LEVEL_LAYERS,\n",
    "                                       character_level_d_model=config.CHARACTER_LEVEL_D_MODEL,\n",
    "                                       word_level_d_model=config.WORD_LEVEL_D_MODEL,\n",
    "                                       num_heads=config.NUM_HEADS, dff=config.DFF,\n",
    "                                       max_word_length=config.MAX_WORD_LENGTH,\n",
    "                                       max_sentence_length=config.MAX_SENTENCE_LENGTH,\n",
    "                                       vocab_size=config.VOCAB_SIZE,\n",
    "                                       character_vocab_size=config.CHARACTER_VOCAB_SIZE)"
   ],
   "id": "82d3104dfc17ed18",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "@keras.utils.register_keras_serializable()\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, dtype=tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            'd_model': float(self.d_model),\n",
    "            'warmup_steps': self.warmup_steps\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "\n",
    "\n",
    "learning_rate = CustomSchedule(config.WORD_LEVEL_D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
    "                                     epsilon=1e-9)"
   ],
   "id": "fb68d56c770a68ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model.compile(optimizer=optimizer, loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=['acc'])\n",
    "train_size = int(config.NUM_OF_INPUTS * 0.8)\n",
    "model.fit(\n",
    "    [input_word_level_sequences[:train_size], input_character_level_sequences[:train_size]],\n",
    "    target_sequences[:train_size], epochs=config.EPOCHS,\n",
    "    batch_size=config.BATCH_SIZE,\n",
    "    validation_data=([input_word_level_sequences[train_size:], input_character_level_sequences[train_size:]],\n",
    "                     target_sequences[train_size:]))"
   ],
   "id": "280aa628ac5ce4ac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Test model",
   "id": "2f0ddc1ee52a8868"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def print_outputs(outputs):\n",
    "    for sentence in outputs:\n",
    "        out = ''\n",
    "        for word in sentence:\n",
    "            max_index = tf.argmax(word, axis=0).numpy()\n",
    "            word_str = word_level_tokenizer.index_word.get(max_index)\n",
    "            if word_str is not None:\n",
    "                out += word_str + ' '\n",
    "            else:\n",
    "                out += '<UNK> '\n",
    "        print(out)"
   ],
   "id": "1a4a08b6ee2d695e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_outputs = model.predict(\n",
    "    [input_word_level_sequences[train_size:], input_character_level_sequences[train_size:]])"
   ],
   "id": "dea35fe03701c927",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print_outputs(test_outputs)",
   "id": "e9a607df2c04a1b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_sentences = ['Nếu làm được như vậy thì chắc chắn xẽ không còn trường nào tùy tiện thu tiền cao, gây sự lo lắng củb phụ huynh và ai không có tiền thì hhông cần đóng.']\n",
    "test_inputs = dataset.build_test_data(test_sentences)\n",
    "test_outputs = model.predict(test_inputs)\n",
    "print_outputs(test_outputs)"
   ],
   "id": "cd89dcfe80880eaf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Save model and reuse.",
   "id": "87f3aaa83aa899df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model.save('model.keras')\n",
    "# new_model = tf.keras.models.load_model('model.keras')"
   ],
   "id": "66f68273100fa981",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
