{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T05:09:58.775239Z",
     "start_time": "2024-04-18T05:09:58.769826Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.BATCH_SIZE = 100\n",
    "        self.EPOCHS = 15\n",
    "        self.NUM_OF_INPUTS = 3000\n",
    "\n",
    "        self.NUM_CHARACTER_LEVEL_LAYERS = 4\n",
    "        self.NUM_WORD_LEVEL_LAYERS = 12\n",
    "\n",
    "        self.CHARACTER_LEVEL_D_MODEL = 64\n",
    "        self.WORD_LEVEL_D_MODEL = 256\n",
    "\n",
    "        self.NUM_HEADS = 3\n",
    "        self.DFF = 256\n",
    "\n",
    "        self.MAX_WORD_LENGTH = 16\n",
    "        self.MAX_SENTENCE_LENGTH = 64\n",
    "\n",
    "        self.VOCAB_SIZE = 20000\n",
    "        self.CHARACTER_VOCAB_SIZE = 3000\n",
    "\n",
    "\n",
    "config = Config()"
   ],
   "id": "1d94a7e3e9c116fa",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T05:10:01.930620Z",
     "start_time": "2024-04-18T05:09:58.839107Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ],
   "id": "4d4605e96cf0317a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-18 12:09:59.691906: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-18 12:10:00.016227: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-18 12:10:01.022928: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Global Attention and FeedForward",
   "id": "6bf6f107d372af26"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T05:10:01.934513Z",
     "start_time": "2024-04-18T05:10:01.931684Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "        self.add = tf.keras.layers.Add()"
   ],
   "id": "cd141c3599d41736",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T05:10:01.944023Z",
     "start_time": "2024-04-18T05:10:01.935684Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GlobalSelfAttention(BaseAttention):\n",
    "    def call(self, inputs):\n",
    "        x, mask = inputs\n",
    "        attn_output = self.mha(query=x, value=x, key=x, attention_mask=mask)\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "        return x"
   ],
   "id": "d59c351bb857beb",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T05:10:01.953400Z",
     "start_time": "2024-04-18T05:10:01.945469Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, dff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.seq = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(dff, activation='relu'),\n",
    "            tf.keras.layers.Dense(d_model),\n",
    "            tf.keras.layers.Dropout(dropout_rate)\n",
    "        ])\n",
    "        self.add = tf.keras.layers.Add()\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.add([x, self.seq(x)])\n",
    "        x = self.layer_norm(x)\n",
    "        return x"
   ],
   "id": "7bbb59f3cd166473",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Positional Embedding layer",
   "id": "ce082eb8b3293069"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T05:10:01.964830Z",
     "start_time": "2024-04-18T05:10:01.954306Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def positional_encoding(length, depth):\n",
    "    depth = depth / 2\n",
    "\n",
    "    positions = np.arange(length)[:, np.newaxis]\n",
    "    depths = np.arange(depth)[np.newaxis, :] / depth\n",
    "\n",
    "    angle_rates = 1 / (10000 ** depths)\n",
    "    angle_rads = positions * angle_rates\n",
    "\n",
    "    pos_encoding = np.concatenate([np.sin(angle_rads), np.cos(angle_rads)], axis=-1)\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "\n",
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True)\n",
    "        self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n",
    "\n",
    "    def compute_mask(self, *args, **kwargs):\n",
    "        return self.embedding.compute_mask(*args, **kwargs)\n",
    "\n",
    "    def call(self, x):\n",
    "        length = tf.shape(x)[1]\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
    "        return x"
   ],
   "id": "5094824d30ca609d",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Encoder layers",
   "id": "9a0a371a77c0f8c6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T05:10:01.973656Z",
     "start_time": "2024-04-18T05:10:01.965729Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, *, d_model, num_heads, dff, dropout_rate=0.1, name='EncoderLayer'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attention = GlobalSelfAttention(\n",
    "            num_heads=num_heads, key_dim=d_model, dropout=dropout_rate, name=name\n",
    "        )\n",
    "\n",
    "        self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.self_attention(inputs)\n",
    "        x = self.ffn(x)\n",
    "        return x\n"
   ],
   "id": "d14cfd02447443b",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T05:10:01.985343Z",
     "start_time": "2024-04-18T05:10:01.974648Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_sequence(actual_length, MAX_LENGTH):\n",
    "    one_tensor_length = tf.cast(tf.cast(actual_length, tf.float32) * 0.85, tf.int64)\n",
    "\n",
    "    masked_sequence = tf.random.shuffle(tf.sequence_mask(one_tensor_length, maxlen=actual_length, dtype=tf.int64))\n",
    "\n",
    "    padded_sequence = tf.pad(masked_sequence, paddings=[[0, MAX_LENGTH - tf.size(masked_sequence)]], mode='CONSTANT',\n",
    "                             constant_values=0.0)\n",
    "    print(\"output\", padded_sequence.shape)\n",
    "    return padded_sequence\n",
    "\n",
    "\n",
    "def create_random_mask(input_tokens, actual_lengths):\n",
    "    # input_tokens has shape (64,16,10)\n",
    "    # actual_lengths has shape (64,)\n",
    "    batch_size, sentence_len = input_tokens.shape[0], input_tokens.shape[1]\n",
    "    \n",
    "    mask = tf.map_fn(lambda length: generate_sequence(length, sentence_len), actual_lengths, dtype=tf.int64) # return (64,None)\n",
    "    mask = tf.expand_dims(mask, axis=1) # return (64,1,None)\n",
    "    \n",
    "    return mask\n",
    "\n",
    "\n",
    "def create_padding_mask(input_tokens, actual_lengths):\n",
    "    batch_size, sentence_len = tf.shape(input_tokens)[0], tf.shape(input_tokens)[1]\n",
    "\n",
    "    mask = tf.sequence_mask(actual_lengths, maxlen=sentence_len, dtype=tf.float32)\n",
    "\n",
    "    mask = tf.expand_dims(mask, axis=1)\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "            self, *, num_layers, d_model, num_heads, dff, vocab_size, dropout_rate=0.1, masked=False,\n",
    "            name='Encoder'\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.masked = masked\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        self.enc_layers = [\n",
    "            EncoderLayer(\n",
    "                d_model=d_model, num_heads=num_heads, dff=dff,\n",
    "                dropout_rate=dropout_rate,\n",
    "                name=f'encoder_layer_{i + 1}'\n",
    "            )\n",
    "            for i in range(num_layers)\n",
    "        ]\n",
    "        # self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, lengths = inputs\n",
    "\n",
    "        mask = None\n",
    "        if self.masked is True:\n",
    "            mask = create_random_mask(x, lengths)\n",
    "            print(\"Random Mask shape: \", mask.shape)\n",
    "        else:\n",
    "            mask = create_padding_mask(x, lengths)\n",
    "            print(f\"Padding Mask shape: \", mask.shape)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            inputs = self.enc_layers[i]((x, mask))\n",
    "\n",
    "        return inputs  # Shape (batch_size, seq_len, d_model)."
   ],
   "id": "cb1b421d1ee7f65d",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Hierarchical Transformer Encoder model",
   "id": "d36df6c730339222"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T05:10:01.998397Z",
     "start_time": "2024-04-18T05:10:01.986196Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class HierarchicalTransformerEncoder(tf.keras.models.Model):\n",
    "    def __init__(self, *, num_character_level_layers, num_word_level_layers,\n",
    "                 character_level_d_model, word_level_d_model, num_heads, dff,\n",
    "                 max_word_length, max_sentence_length,\n",
    "                 vocab_size, character_vocab_size, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.word_pos_embedding = PositionalEmbedding(vocab_size=vocab_size,\n",
    "                                                      d_model=word_level_d_model)\n",
    "        self.character_pos_embedding = PositionalEmbedding(vocab_size=character_vocab_size,\n",
    "                                                           d_model=character_level_d_model)\n",
    "\n",
    "        self.character_level_encoder = Encoder(num_layers=num_character_level_layers,\n",
    "                                               d_model=character_level_d_model,\n",
    "                                               num_heads=num_heads, dff=dff,\n",
    "                                               vocab_size=character_vocab_size,\n",
    "                                               dropout_rate=dropout_rate,\n",
    "                                               name='character_level_encoder', masked=True)\n",
    "\n",
    "        self.flatten_layer = tf.keras.layers.Flatten(input_shape=(max_word_length, character_level_d_model),\n",
    "                                                     name='flatten_character_level_encoders')\n",
    "        self.linear_layer = tf.keras.layers.Dense(units=word_level_d_model, activation=None,\n",
    "                                                  name='linear_character_level_encoders')\n",
    "\n",
    "        self.combined_layer = tf.keras.layers.Concatenate(axis=-1, name='combined_character_and_word_level_encoders')\n",
    "\n",
    "        self.word_level_encoder = Encoder(num_layers=num_word_level_layers,\n",
    "                                          d_model=(word_level_d_model * 2),\n",
    "                                          num_heads=num_heads, dff=dff,\n",
    "                                          vocab_size=vocab_size,\n",
    "                                          dropout_rate=dropout_rate,\n",
    "                                          name='word_level_encoder')\n",
    "\n",
    "        self.correction_layer = tf.keras.layers.Dense(vocab_size, activation='softmax', name='correction_layer', )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        print(\"Start\")\n",
    "        (word_level_inputs, sentences_lengths), (character_level_inputs, words_lengths) = inputs\n",
    "\n",
    "        word_embedding_outputs = self.word_pos_embedding(word_level_inputs)  # (batch_size, max_len, word_level_d_model)\n",
    "        print(\"Shape của word_embedding_outputs:\", word_embedding_outputs.shape)\n",
    "\n",
    "        character_level_encoder_outputs = tf.map_fn(\n",
    "            lambda sentence: self.character_level_encoder(\n",
    "                (self.character_pos_embedding(sentence[0]), sentence[1])),\n",
    "            (character_level_inputs, words_lengths),\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "        # (batch_size, max_sen_len, max_word_length, character_level_d_model)\n",
    "\n",
    "        character_level_encoder_outputs = tf.map_fn(\n",
    "            lambda sentence: tf.map_fn(\n",
    "                lambda word: tf.squeeze(self.linear_layer(self.flatten_layer(tf.expand_dims(word, axis=0))), axis=0),\n",
    "                sentence,\n",
    "                dtype=tf.float32\n",
    "            ),\n",
    "            character_level_encoder_outputs,\n",
    "            dtype=tf.float32)\n",
    "        # (batch_size, max_sen_len, word_level_d_model)\n",
    "        print(\"Shape của character_level_encoder_outputs:\", character_level_encoder_outputs.shape)\n",
    "\n",
    "        concat_output = self.combined_layer([word_embedding_outputs, character_level_encoder_outputs])\n",
    "        # (batch_size, max_sen_len, word_level_d_model * 2)\n",
    "        print(\"Shape của concat_output:\", concat_output.shape)\n",
    "\n",
    "        word_level_output = self.word_level_encoder((concat_output, sentences_lengths))\n",
    "        # (batch_size, max_sen_len, word_level_d_model + character_level_d_model)\n",
    "        print(\"Shape của word_level_output:\", word_level_output.shape)\n",
    "\n",
    "        correction_output = self.correction_layer(word_level_output)  # (batch_size, max_sen_len, vocab_size)\n",
    "        print(\"Shape của correction_output:\", correction_output.shape)\n",
    "\n",
    "        # detection_output = tf.squeeze(self.detection_layer(word_level_output), axis=-1)  # (batch_size, max_sen_len)\n",
    "        # print(\"Shape của detection_output:\", detection_output.shape)\n",
    "        print(\"End\")\n",
    "        return correction_output\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def training_step(model, optimizer, x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        correction_output = model(x)\n",
    "        loss1 = correction_loss(y, correction_output)\n",
    "        # loss2 = detection_loss(y[1], detection_output)\n",
    "        total_loss = loss1\n",
    "\n",
    "    gradients = tape.gradient(total_loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "def correction_loss(true_outputs, pred_outputs):\n",
    "    softmax_loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)(true_outputs, pred_outputs)\n",
    "    return softmax_loss\n",
    "\n",
    "\n",
    "def detection_loss(true_detection_infos, pred_detection_infos):\n",
    "    sigmoid_loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)(true_detection_infos, pred_detection_infos)\n",
    "    return sigmoid_loss"
   ],
   "id": "d86e3b5b919c916a",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data preprocessing",
   "id": "c52ce7c4a47ea323"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T05:10:04.321458Z",
     "start_time": "2024-04-18T05:10:01.999596Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = pd.read_csv('data/vi_processed.csv')\n",
    "\n",
    "input_sentences = []\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    if len(input_sentences) == config.NUM_OF_INPUTS: break\n",
    "    input_sentences.append(row.correct_text)\n",
    "\n",
    "correct_texts = input_sentences[:config.NUM_OF_INPUTS]"
   ],
   "id": "dc8b24f26c3728a9",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T05:10:04.326682Z",
     "start_time": "2024-04-18T05:10:04.323976Z"
    }
   },
   "cell_type": "code",
   "source": [
    "word_level_tokenizer = Tokenizer(num_words=config.VOCAB_SIZE, oov_token='<UNK>', lower=True, split=' ', )\n",
    "\n",
    "character_level_tokenizer = Tokenizer(num_words=config.CHARACTER_VOCAB_SIZE, lower=True, char_level=True)"
   ],
   "id": "b04c7f90b2c479e4",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T05:10:04.496251Z",
     "start_time": "2024-04-18T05:10:04.327617Z"
    }
   },
   "cell_type": "code",
   "source": [
    "word_level_tokenizer.fit_on_texts(input_sentences)\n",
    "character_level_tokenizer.fit_on_texts(input_sentences)"
   ],
   "id": "7d42bb658ef10684",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T05:10:04.936725Z",
     "start_time": "2024-04-18T05:10:04.497267Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_sequences = word_level_tokenizer.texts_to_sequences(input_sentences)\n",
    "\n",
    "# Get character-level words lengths.\n",
    "input_words_lengths = []\n",
    "\n",
    "# Get character-level sequences.\n",
    "character_level_input_sequences = []\n",
    "\n",
    "for sequence in input_sequences:\n",
    "    character_level_input_sequence = []\n",
    "    words_lengths = []\n",
    "    for word_token in sequence:\n",
    "        word = word_level_tokenizer.index_word[word_token]\n",
    "        word = character_level_tokenizer.texts_to_sequences(word)\n",
    "        word_chars = [each[0] for each in word]\n",
    "        character_level_input_sequence.append(word_chars)\n",
    "        words_lengths.append((len(word_chars) if len(word_chars) <= config.MAX_WORD_LENGTH\n",
    "                              else config.MAX_WORD_LENGTH))\n",
    "\n",
    "    # Add padding for each word.\n",
    "    character_level_input_sequence = pad_sequences(character_level_input_sequence, maxlen=config.MAX_WORD_LENGTH,\n",
    "                                                   padding='post', truncating='post')\n",
    "\n",
    "    character_level_input_sequences.append(character_level_input_sequence)\n",
    "\n",
    "    input_words_lengths.append(words_lengths)\n",
    "\n",
    "# Get word-level sentences lengths.\n",
    "input_sentences_lengths = []\n",
    "for sequence in input_sequences: input_sentences_lengths.append(\n",
    "    (len(sequence) if len(sequence) <= config.MAX_SENTENCE_LENGTH\n",
    "     else config.MAX_SENTENCE_LENGTH))\n",
    "\n",
    "# Add padding for each.\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=config.MAX_SENTENCE_LENGTH, padding='post', truncating='post')\n",
    "\n",
    "character_level_input_sequences = pad_sequences(character_level_input_sequences, maxlen=config.MAX_SENTENCE_LENGTH,\n",
    "                                                padding='post', truncating='post')\n",
    "input_words_lengths = pad_sequences(input_words_lengths, maxlen=config.MAX_SENTENCE_LENGTH, padding='post',\n",
    "                                    truncating='post')\n",
    "\n",
    "input_sequences_np = np.array(input_sequences)\n",
    "character_level_input_sequences_np = np.array(character_level_input_sequences)\n",
    "\n",
    "input_words_lengths_np = np.array(input_words_lengths)\n",
    "input_sentences_lengths_np = np.array(input_sentences_lengths)"
   ],
   "id": "caa0c465401ef03f",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T05:10:04.942909Z",
     "start_time": "2024-04-18T05:10:04.937776Z"
    }
   },
   "cell_type": "code",
   "source": "character_level_input_sequences_np.shape",
   "id": "27b4a077b2079907",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 64, 16)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model building",
   "id": "f42308d87fe8aa3f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T05:10:05.133334Z",
     "start_time": "2024-04-18T05:10:04.944538Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = HierarchicalTransformerEncoder(num_character_level_layers=config.NUM_CHARACTER_LEVEL_LAYERS,\n",
    "                                       num_word_level_layers=config.NUM_WORD_LEVEL_LAYERS,\n",
    "                                       character_level_d_model=config.CHARACTER_LEVEL_D_MODEL,\n",
    "                                       word_level_d_model=config.WORD_LEVEL_D_MODEL,\n",
    "                                       num_heads=config.NUM_HEADS, dff=config.DFF,\n",
    "                                       max_word_length=config.MAX_WORD_LENGTH,\n",
    "                                       max_sentence_length=config.MAX_SENTENCE_LENGTH,\n",
    "                                       vocab_size=config.VOCAB_SIZE,\n",
    "                                       character_vocab_size=config.CHARACTER_VOCAB_SIZE)"
   ],
   "id": "82d3104dfc17ed18",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thanhan/.local/lib/python3.10/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T05:10:05.152766Z",
     "start_time": "2024-04-18T05:10:05.134238Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from CustomSchedule import CustomSchedule\n",
    "\n",
    "learning_rate = CustomSchedule(config.WORD_LEVEL_D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
    "                                     epsilon=1e-9)"
   ],
   "id": "fb68d56c770a68ce",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-04-18T05:10:05.153639Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for e in range(config.EPOCHS):\n",
    "    pbar = tqdm(range(0, len(input_sequences_np), config.BATCH_SIZE))\n",
    "\n",
    "    for i in pbar:\n",
    "        input_batch = [\n",
    "            [input_sequences_np[i:i + config.BATCH_SIZE], input_sentences_lengths_np[i:i + config.BATCH_SIZE]],\n",
    "            [character_level_input_sequences_np[i:i + config.BATCH_SIZE],\n",
    "             input_words_lengths_np[i:i + config.BATCH_SIZE]]\n",
    "        ]\n",
    "        output_batch = input_sequences_np[i:i + config.BATCH_SIZE]\n",
    "\n",
    "        total_loss = training_step(model, optimizer, input_batch, output_batch)\n",
    "\n",
    "        pbar.set_description(f'Epoch {e + 1}/{config.EPOCHS}')\n",
    "        pbar.set_postfix_str(f'loss: {total_loss:.4f}')\n",
    "        pbar.refresh()\n"
   ],
   "id": "3d89bd411edb5399",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n",
      "Shape của word_embedding_outputs: (100, 64, 256)\n",
      "WARNING:tensorflow:From /home/thanhan/.local/lib/python3.10/site-packages/tensorflow/python/util/deprecation.py:660: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n",
      "output (None,)\n",
      "Random Mask shape:  (64, 1, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thanhan/.local/lib/python3.10/site-packages/keras/src/layers/layer.py:856: UserWarning: Layer 'query' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/thanhan/.local/lib/python3.10/site-packages/keras/src/layers/layer.py:856: UserWarning: Layer 'key' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/thanhan/.local/lib/python3.10/site-packages/keras/src/layers/layer.py:856: UserWarning: Layer 'value' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output (None,)\n",
      "Random Mask shape:  (64, 1, None)\n",
      "Shape của character_level_encoder_outputs: (100, 64, 256)\n",
      "Shape của concat_output: (100, 64, 512)\n",
      "Padding Mask shape:  (100, 1, 64)\n",
      "Padding Mask shape:  (100, 1, 64)\n",
      "Shape của word_level_output: (100, 64, 512)\n",
      "Shape của correction_output: (100, 64, 20000)\n",
      "End\n",
      "Start\n",
      "Shape của word_embedding_outputs: (100, 64, 256)\n",
      "output (None,)\n",
      "Random Mask shape:  (64, 1, None)\n",
      "Shape của character_level_encoder_outputs: (100, 64, 256)\n",
      "Shape của concat_output: (100, 64, 512)\n",
      "Padding Mask shape:  (100, 1, 64)\n",
      "Shape của word_level_output: (100, 64, 512)\n",
      "Shape của correction_output: (100, 64, 20000)\n",
      "End\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thanhan/.local/lib/python3.10/site-packages/keras/src/optimizers/base_optimizer.py:583: UserWarning: Gradients do not exist for variables ['kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n",
      "Shape của word_embedding_outputs: (100, 64, 256)\n",
      "output (None,)\n",
      "Random Mask shape:  (64, 1, None)\n",
      "Shape của character_level_encoder_outputs: (100, 64, 256)\n",
      "Shape của concat_output: (100, 64, 512)\n",
      "Padding Mask shape:  (100, 1, 64)\n",
      "Shape của word_level_output: (100, 64, 512)\n",
      "Shape của correction_output: (100, 64, 20000)\n",
      "End\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15:  33%|███▎      | 10/30 [01:08<01:44,  5.24s/it, loss: 9.6960]"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# model.fit(\n",
    "#     [[input_sequences_np, input_sentences_lengths_np], [character_level_input_sequences_np, input_words_lengths_np]],\n",
    "#     [output_sequences_np, correct_infos_np], epochs=config.EPOCHS,\n",
    "#     batch_size=config.BATCH_SIZE)"
   ],
   "id": "280aa628ac5ce4ac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "test_sentence_inputs = ['hom nay troi dep qua']\n",
    "test_word_level_sequences = word_level_tokenizer.texts_to_sequences(test_sentence_inputs)\n",
    "test_sentence_lengths = [(len(sentence) if len(sentence) < config.MAX_SENTENCE_LENGTH else config.MAX_SENTENCE_LENGTH)\n",
    "                         for sentence in test_word_level_sequences]\n",
    "\n",
    "test_output = model.predict([[input_sequences_np[:100], input_sentences_lengths_np[:100]],\n",
    "                             [character_level_input_sequences_np[:100], input_words_lengths_np[:100]]])\n",
    "\n",
    "for sentence in test_output[10:11]:\n",
    "    out = ''\n",
    "    for word in sentence:\n",
    "        index = tf.argmax(word, axis=0).numpy()\n",
    "        print(index)\n",
    "        word_str = word_level_tokenizer.index_word.get(index)\n",
    "        if word_str is not None:\n",
    "            out += word_str + ' '\n",
    "        else:\n",
    "            out += '<UNK> '\n",
    "    print(out)\n",
    "print(tf.argmax(test_output[0][10][5], axis=0).numpy())\n"
   ],
   "id": "845b01dd701239d0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "test_sentence_inputs = [\n",
    "    'việc điều khiển xe sẽ trở nên rễ dàng hơn nếu các bánh xe được đặt theo một góc chính xác theo yêu cầu thiết kế, việc đặt saii góc sẽ tạo ra tính ổn định khi lái kém và khiến lốp xe nhanh mòn.']\n",
    "\n",
    "test_word_level_sequences = word_level_tokenizer.texts_to_sequences(test_sentence_inputs)\n",
    "test_sentence_lengths = [(len(sentence) if len(sentence) < config.MAX_SENTENCE_LENGTH else config.MAX_SENTENCE_LENGTH)\n",
    "                         for sentence in test_word_level_sequences]\n",
    "test_word_level_sequences = pad_sequences(test_word_level_sequences, maxlen=config.MAX_SENTENCE_LENGTH, padding='post',\n",
    "                                          truncating='post')\n",
    "\n",
    "character_level_tokenizer.fit_on_texts(test_sentence_inputs)\n",
    "word_unk_level_tokenizer.fit_on_texts(test_sentence_inputs)\n",
    "test_unk_input_sequences = word_unk_level_tokenizer.texts_to_sequences(test_sentence_inputs)\n",
    "\n",
    "test_character_level_input_sequences = []\n",
    "test_words_lengths = []\n",
    "for sequence in test_unk_input_sequences:\n",
    "    character_level_input_sequence = []\n",
    "    words_lengths = []\n",
    "    for word_token in sequence:\n",
    "        word = word_unk_level_tokenizer.index_word[word_token]\n",
    "        word = character_level_tokenizer.texts_to_sequences(word)\n",
    "        word_chars = [each[0] for each in word]\n",
    "        character_level_input_sequence.append(word_chars)\n",
    "        words_lengths.append((len(word_chars) if len(word_chars) <= config.MAX_WORD_LENGTH\n",
    "                              else config.MAX_WORD_LENGTH))\n",
    "\n",
    "    character_level_input_sequence = pad_sequences(character_level_input_sequence, maxlen=config.MAX_WORD_LENGTH,\n",
    "                                                   padding='post', truncating='post')\n",
    "\n",
    "    test_character_level_input_sequences.append(character_level_input_sequence)\n",
    "\n",
    "    test_words_lengths.append(words_lengths)\n",
    "\n",
    "test_character_level_input_sequences = pad_sequences(test_character_level_input_sequences,\n",
    "                                                     maxlen=config.MAX_SENTENCE_LENGTH,\n",
    "                                                     padding='post', truncating='post')\n",
    "test_words_lengths = pad_sequences(test_words_lengths, maxlen=config.MAX_SENTENCE_LENGTH, padding='post',\n",
    "                                   truncating='post')"
   ],
   "id": "887a9ddaccd357ca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "test_character_level_input_sequences = np.array(test_character_level_input_sequences)\n",
    "test_words_lengths = np.array(test_words_lengths)\n",
    "test_sentence_lengths = np.array(test_sentence_lengths)\n",
    "test_word_level_sequences = np.array(test_word_level_sequences)"
   ],
   "id": "c4bd5623cac4449f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "test_outputs = model.predict(\n",
    "    [[test_word_level_sequences, test_sentence_lengths], [test_character_level_input_sequences, test_words_lengths]])"
   ],
   "id": "dea35fe03701c927",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "for sentence in test_outputs[0]:\n",
    "    out = ''\n",
    "    for word in sentence:\n",
    "        index = tf.argmax(word, axis=0).numpy()\n",
    "        word_str = word_level_tokenizer.index_word.get(index)\n",
    "        if word_str is not None:\n",
    "            out += word_str + ' '\n",
    "        else:\n",
    "            out += '<UNK> '\n",
    "    print(out)"
   ],
   "id": "e9a607df2c04a1b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "test_outputs[1]",
   "id": "45d3183852c77930",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "output_sequences_np[1]",
   "id": "9394e6287f869afc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "input_sequences_np[1]",
   "id": "7a85d91df66ef258",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
