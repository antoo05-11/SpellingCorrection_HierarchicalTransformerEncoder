{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T17:14:47.348136Z",
     "start_time": "2024-04-18T17:14:47.341523Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.BATCH_SIZE = 50\n",
    "        self.EPOCHS = 30\n",
    "        self.NUM_OF_INPUTS = 300\n",
    "\n",
    "        self.NUM_CHARACTER_LEVEL_LAYERS = 4\n",
    "        self.NUM_WORD_LEVEL_LAYERS = 12\n",
    "\n",
    "        self.CHARACTER_LEVEL_D_MODEL = 32\n",
    "        self.WORD_LEVEL_D_MODEL = 128\n",
    "\n",
    "        self.NUM_HEADS = 3\n",
    "        self.DFF = 256\n",
    "\n",
    "        self.MAX_WORD_LENGTH = 16\n",
    "        self.MAX_SENTENCE_LENGTH = 64\n",
    "\n",
    "        self.VOCAB_SIZE = 10000\n",
    "        self.CHARACTER_VOCAB_SIZE = 1000\n",
    "\n",
    "\n",
    "config = Config()"
   ],
   "id": "1d94a7e3e9c116fa",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T17:14:51.055847Z",
     "start_time": "2024-04-18T17:14:47.394686Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ],
   "id": "4d4605e96cf0317a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-19 00:14:48.387575: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-19 00:14:48.724764: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-19 00:14:49.962444: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Tokenizer",
   "id": "25bc207531cfa5b0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T18:38:15.083932Z",
     "start_time": "2024-04-18T18:38:15.081519Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CustomTokenizer:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = Tokenizer()\n",
    "        \n",
    "    def texts_to_sequences(self, texts):\n",
    "        return \n",
    "    def sequences_to_texts(self, sequences):\n",
    "        return\n",
    "    def fit_texts(self, texts):\n",
    "        return "
   ],
   "id": "fdcb553bf57b6307",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Global Attention and FeedForward",
   "id": "6bf6f107d372af26"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T17:14:51.059809Z",
     "start_time": "2024-04-18T17:14:51.057020Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "        self.add = tf.keras.layers.Add()"
   ],
   "id": "cd141c3599d41736",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T17:14:51.068566Z",
     "start_time": "2024-04-18T17:14:51.061630Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GlobalSelfAttention(BaseAttention):\n",
    "    def call(self, inputs):\n",
    "        x, mask = inputs[0], inputs[1]\n",
    "\n",
    "        attn_output = self.mha(query=x, value=x, key=x, attention_mask=mask)\n",
    "\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "        return x"
   ],
   "id": "d59c351bb857beb",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T17:14:51.077342Z",
     "start_time": "2024-04-18T17:14:51.069813Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, dff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.seq = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(dff, activation='relu'),\n",
    "            tf.keras.layers.Dense(d_model),\n",
    "            tf.keras.layers.Dropout(dropout_rate)\n",
    "        ])\n",
    "        self.add = tf.keras.layers.Add()\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.add([x, self.seq(x)])\n",
    "        x = self.layer_norm(x)\n",
    "        return x"
   ],
   "id": "7bbb59f3cd166473",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Positional Embedding layer",
   "id": "ce082eb8b3293069"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T17:14:51.089123Z",
     "start_time": "2024-04-18T17:14:51.079Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def positional_encoding(length, depth):\n",
    "    depth = depth / 2\n",
    "\n",
    "    positions = np.arange(length)[:, np.newaxis]\n",
    "    depths = np.arange(depth)[np.newaxis, :] / depth\n",
    "\n",
    "    angle_rates = 1 / (10000 ** depths)\n",
    "    angle_rads = positions * angle_rates\n",
    "\n",
    "    pos_encoding = np.concatenate([np.sin(angle_rads), np.cos(angle_rads)], axis=-1)\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "\n",
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True)\n",
    "        self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n",
    "\n",
    "    def compute_mask(self, *args, **kwargs):\n",
    "        return self.embedding.compute_mask(*args, **kwargs)\n",
    "\n",
    "    def call(self, x):\n",
    "        length = tf.shape(x)[1]\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
    "        return x"
   ],
   "id": "5094824d30ca609d",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Encoder layers",
   "id": "9a0a371a77c0f8c6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T17:14:51.098438Z",
     "start_time": "2024-04-18T17:14:51.090197Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, *, d_model, num_heads, dff, dropout_rate=0.1, name='EncoderLayer'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attention = GlobalSelfAttention(\n",
    "            num_heads=num_heads, key_dim=d_model, dropout=dropout_rate, name=name\n",
    "        )\n",
    "\n",
    "        self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.self_attention(inputs)\n",
    "        x = self.ffn(x)\n",
    "        return x\n"
   ],
   "id": "d14cfd02447443b",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T17:14:51.116823Z",
     "start_time": "2024-04-18T17:14:51.099593Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "            self, *, num_layers, d_model, num_heads, dff, vocab_size, dropout_rate=0.1,\n",
    "            name='Encoder'\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        self.enc_layers = [\n",
    "            EncoderLayer(\n",
    "                d_model=d_model, num_heads=num_heads, dff=dff,\n",
    "                dropout_rate=dropout_rate,\n",
    "                name=f'encoder_layer_{i + 1}'\n",
    "            )\n",
    "            for i in range(num_layers)\n",
    "        ]\n",
    "        # self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, mask = inputs\n",
    "        print(\"Encoder start\")\n",
    "        print(\"input shape\", x.shape)\n",
    "        print(\"mask shape\", mask.shape)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            inputs = self.enc_layers[i]((x, mask))\n",
    "        print(\"Encoder end\")\n",
    "        return inputs  # Shape (batch_size, seq_len, d_model)."
   ],
   "id": "cb1b421d1ee7f65d",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Hierarchical Transformer Encoder model",
   "id": "d36df6c730339222"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T17:14:51.140227Z",
     "start_time": "2024-04-18T17:14:51.118412Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class HierarchicalTransformerEncoder(tf.keras.models.Model):\n",
    "    def __init__(self, *, num_character_level_layers, num_word_level_layers,\n",
    "                 character_level_d_model, word_level_d_model, num_heads, dff,\n",
    "                 max_word_length, max_sentence_length,\n",
    "                 vocab_size, character_vocab_size, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.word_pos_embedding = PositionalEmbedding(vocab_size=vocab_size,\n",
    "                                                      d_model=word_level_d_model)\n",
    "        self.character_pos_embedding = PositionalEmbedding(vocab_size=character_vocab_size,\n",
    "                                                           d_model=character_level_d_model)\n",
    "\n",
    "        self.character_level_encoder = Encoder(num_layers=num_character_level_layers,\n",
    "                                               d_model=character_level_d_model,\n",
    "                                               num_heads=num_heads, dff=dff,\n",
    "                                               vocab_size=character_vocab_size,\n",
    "                                               dropout_rate=dropout_rate,\n",
    "                                               name='character_level_encoder')\n",
    "\n",
    "        self.flatten_layer = tf.keras.layers.Flatten(input_shape=(max_word_length, character_level_d_model),\n",
    "                                                     name='flatten_character_level_encoders')\n",
    "        self.linear_layer = tf.keras.layers.Dense(units=word_level_d_model, activation=None,\n",
    "                                                  name='linear_character_level_encoders')\n",
    "\n",
    "        self.combined_layer = tf.keras.layers.Concatenate(axis=-1, name='combined_character_and_word_level_encoders')\n",
    "\n",
    "        self.word_level_encoder = Encoder(num_layers=num_word_level_layers,\n",
    "                                          d_model=(word_level_d_model * 2),\n",
    "                                          num_heads=num_heads, dff=dff,\n",
    "                                          vocab_size=vocab_size,\n",
    "                                          dropout_rate=dropout_rate,\n",
    "                                          name='word_level_encoder')\n",
    "\n",
    "        self.correction_layer = tf.keras.layers.Dense(vocab_size, activation='softmax', name='correction_layer', )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        print(\"Start\")\n",
    "        (word_level_inputs, word_level_mask_inputs), (character_level_inputs, character_level_mask_inputs) = inputs\n",
    "\n",
    "        word_embedding_outputs = self.word_pos_embedding(word_level_inputs)  # (batch_size, max_len, word_level_d_model)\n",
    "        print(\"Shape của word_embedding_outputs:\", word_embedding_outputs.shape)\n",
    "\n",
    "        character_level_encoder_outputs = tf.map_fn(\n",
    "            lambda sentence: self.character_level_encoder(\n",
    "                (self.character_pos_embedding(sentence[0]), sentence[1])),\n",
    "            (character_level_inputs, character_level_mask_inputs),\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "        # (batch_size, max_sen_len, max_word_length, character_level_d_model)\n",
    "\n",
    "        character_level_encoder_outputs = tf.map_fn(\n",
    "            lambda sentence: tf.map_fn(\n",
    "                lambda word: tf.squeeze(self.linear_layer(self.flatten_layer(tf.expand_dims(word, axis=0))), axis=0),\n",
    "                sentence,\n",
    "                dtype=tf.float32\n",
    "            ),\n",
    "            character_level_encoder_outputs,\n",
    "            dtype=tf.float32)\n",
    "        # (batch_size, max_sen_len, word_level_d_model)\n",
    "        print(\"Shape của character_level_encoder_outputs:\", character_level_encoder_outputs.shape)\n",
    "\n",
    "        concat_output = self.combined_layer([word_embedding_outputs, character_level_encoder_outputs])\n",
    "        # (batch_size, max_sen_len, word_level_d_model * 2)\n",
    "        print(\"Shape của concat_output:\", concat_output.shape)\n",
    "\n",
    "        word_level_output = self.word_level_encoder((concat_output, word_level_mask_inputs))\n",
    "        # (batch_size, max_sen_len, word_level_d_model + character_level_d_model)\n",
    "        print(\"Shape của word_level_output:\", word_level_output.shape)\n",
    "\n",
    "        correction_output = self.correction_layer(word_level_output)  # (batch_size, max_sen_len, vocab_size)\n",
    "        print(\"Shape của correction_output:\", correction_output.shape)\n",
    "\n",
    "        # detection_output = tf.squeeze(self.detection_layer(word_level_output), axis=-1)  # (batch_size, max_sen_len)\n",
    "        # print(\"Shape của detection_output:\", detection_output.shape)\n",
    "        print(\"End\")\n",
    "        return correction_output\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def training_step(model, optimizer, x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        correction_output = model(x)\n",
    "        loss1 = correction_loss(y, correction_output)\n",
    "        # loss2 = detection_loss(y[1], detection_output)\n",
    "        total_loss = loss1\n",
    "\n",
    "    gradients = tape.gradient(total_loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "def correction_loss(true_outputs, pred_outputs):\n",
    "    softmax_loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)(true_outputs, pred_outputs)\n",
    "    return softmax_loss\n",
    "\n",
    "\n",
    "def detection_loss(true_detection_infos, pred_detection_infos):\n",
    "    sigmoid_loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)(true_detection_infos, pred_detection_infos)\n",
    "    return sigmoid_loss"
   ],
   "id": "d86e3b5b919c916a",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data preprocessing",
   "id": "c52ce7c4a47ea323"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T17:14:53.896095Z",
     "start_time": "2024-04-18T17:14:51.141941Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = pd.read_csv('data/vi_processed.csv')\n",
    "\n",
    "input_sentences = []\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    if len(input_sentences) == config.NUM_OF_INPUTS: break\n",
    "    input_sentences.append(row.correct_text)\n",
    "\n",
    "correct_texts = input_sentences[:config.NUM_OF_INPUTS]"
   ],
   "id": "dc8b24f26c3728a9",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T17:14:53.903210Z",
     "start_time": "2024-04-18T17:14:53.899622Z"
    }
   },
   "cell_type": "code",
   "source": [
    "word_level_tokenizer = Tokenizer(num_words=config.VOCAB_SIZE, oov_token='<UNK>', lower=True, split=' ', )\n",
    "\n",
    "character_level_tokenizer = Tokenizer(num_words=config.CHARACTER_VOCAB_SIZE, lower=True, char_level=True)"
   ],
   "id": "b04c7f90b2c479e4",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T17:14:53.931112Z",
     "start_time": "2024-04-18T17:14:53.904220Z"
    }
   },
   "cell_type": "code",
   "source": [
    "word_level_tokenizer.fit_on_texts(input_sentences)\n",
    "character_level_tokenizer.fit_on_texts(input_sentences)"
   ],
   "id": "7d42bb658ef10684",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T17:14:54.006767Z",
     "start_time": "2024-04-18T17:14:53.932597Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_sequences = word_level_tokenizer.texts_to_sequences(input_sentences)\n",
    "\n",
    "# Get character-level words lengths.\n",
    "input_words_lengths = []\n",
    "\n",
    "# Get character-level sequences.\n",
    "character_level_input_sequences = []\n",
    "\n",
    "for sequence in input_sequences:\n",
    "    character_level_input_sequence = []\n",
    "    words_lengths = []\n",
    "    for word_token in sequence:\n",
    "        word = word_level_tokenizer.index_word[word_token]\n",
    "        word = character_level_tokenizer.texts_to_sequences(word)\n",
    "        word_chars = [each[0] for each in word]\n",
    "        character_level_input_sequence.append(word_chars)\n",
    "        words_lengths.append((len(word_chars) if len(word_chars) <= config.MAX_WORD_LENGTH\n",
    "                              else config.MAX_WORD_LENGTH))\n",
    "\n",
    "    # Add padding for each word.\n",
    "    character_level_input_sequence = pad_sequences(character_level_input_sequence, maxlen=config.MAX_WORD_LENGTH,\n",
    "                                                   padding='post', truncating='post')\n",
    "\n",
    "    character_level_input_sequences.append(character_level_input_sequence)\n",
    "\n",
    "    input_words_lengths.append(words_lengths)\n",
    "\n",
    "# Get word-level sentences lengths.\n",
    "input_sentences_lengths = []\n",
    "for sequence in input_sequences: input_sentences_lengths.append(\n",
    "    (len(sequence) if len(sequence) <= config.MAX_SENTENCE_LENGTH\n",
    "     else config.MAX_SENTENCE_LENGTH))\n",
    "\n",
    "# Add padding for each.\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=config.MAX_SENTENCE_LENGTH, padding='post', truncating='post')\n",
    "\n",
    "character_level_input_sequences = pad_sequences(character_level_input_sequences, maxlen=config.MAX_SENTENCE_LENGTH,\n",
    "                                                padding='post', truncating='post')\n",
    "input_words_lengths = pad_sequences(input_words_lengths, maxlen=config.MAX_SENTENCE_LENGTH, padding='post',\n",
    "                                    truncating='post')\n",
    "\n",
    "input_sequences_np = np.array(input_sequences)\n",
    "character_level_input_sequences_np = np.array(character_level_input_sequences)\n",
    "\n",
    "input_words_lengths_np = np.array(input_words_lengths)\n",
    "input_sentences_lengths_np = np.array(input_sentences_lengths)"
   ],
   "id": "caa0c465401ef03f",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T17:15:17.009323Z",
     "start_time": "2024-04-18T17:14:54.008011Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_sequence(length, max_length):\n",
    "    one_tensor_length = tf.cast(tf.cast(length, tf.float32) * 0.85, tf.int32)\n",
    "\n",
    "    masked_sequence = tf.random.shuffle(tf.sequence_mask(one_tensor_length, maxlen=length, dtype=tf.float32))\n",
    "\n",
    "    padded_sequence = tf.pad(masked_sequence, paddings=[[0, max_length - length]], mode='CONSTANT',\n",
    "                             constant_values=0.0)\n",
    "\n",
    "    return padded_sequence\n",
    "\n",
    "\n",
    "# Character level mask\n",
    "character_level_mask_tensor = tf.map_fn(\n",
    "    lambda sentence: tf.map_fn(\n",
    "        lambda length: generate_sequence(length, config.MAX_WORD_LENGTH),\n",
    "        sentence,\n",
    "        dtype=tf.float32),\n",
    "    tf.convert_to_tensor(input_words_lengths_np),\n",
    "    dtype=tf.float32)\n",
    "character_level_mask_tensor = tf.expand_dims(character_level_mask_tensor, 2)\n",
    "\n",
    "# Word level mask\n",
    "word_level_mask_tensor = tf.map_fn(\n",
    "    lambda length: generate_sequence(length, config.MAX_SENTENCE_LENGTH),\n",
    "    tf.convert_to_tensor(input_sentences_lengths_np),\n",
    "    dtype=tf.float32\n",
    ")\n",
    "word_level_mask_tensor = tf.expand_dims(word_level_mask_tensor, 1)"
   ],
   "id": "d516a590c38aa7ef",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model building",
   "id": "f42308d87fe8aa3f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T17:15:17.160918Z",
     "start_time": "2024-04-18T17:15:17.010306Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = HierarchicalTransformerEncoder(num_character_level_layers=config.NUM_CHARACTER_LEVEL_LAYERS,\n",
    "                                       num_word_level_layers=config.NUM_WORD_LEVEL_LAYERS,\n",
    "                                       character_level_d_model=config.CHARACTER_LEVEL_D_MODEL,\n",
    "                                       word_level_d_model=config.WORD_LEVEL_D_MODEL,\n",
    "                                       num_heads=config.NUM_HEADS, dff=config.DFF,\n",
    "                                       max_word_length=config.MAX_WORD_LENGTH,\n",
    "                                       max_sentence_length=config.MAX_SENTENCE_LENGTH,\n",
    "                                       vocab_size=config.VOCAB_SIZE,\n",
    "                                       character_vocab_size=config.CHARACTER_VOCAB_SIZE)"
   ],
   "id": "82d3104dfc17ed18",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thanhan/.local/lib/python3.10/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T17:15:17.173438Z",
     "start_time": "2024-04-18T17:15:17.162483Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, dtype=tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            'd_model': float(self.d_model),\n",
    "            'warmup_steps': self.warmup_steps\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(d_model=config['d_model'], warmup_steps=config['warmup_steps'])\n",
    "\n",
    "\n",
    "learning_rate = CustomSchedule(config.WORD_LEVEL_D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
    "                                     epsilon=1e-9)"
   ],
   "id": "fb68d56c770a68ce",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T17:15:17.183484Z",
     "start_time": "2024-04-18T17:15:17.174663Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from tqdm import tqdm\n",
    "# \n",
    "# for e in range(config.EPOCHS):\n",
    "#     pbar = tqdm(range(0, len(input_sequences_np), config.BATCH_SIZE))\n",
    "# \n",
    "#     for i in pbar:\n",
    "#         input_batch = [\n",
    "#             [input_sequences_np[i:i + config.BATCH_SIZE], input_sentences_lengths_np[i:i + config.BATCH_SIZE]],\n",
    "#             [character_level_input_sequences_np[i:i + config.BATCH_SIZE],\n",
    "#              input_words_lengths_np[i:i + config.BATCH_SIZE]]\n",
    "#         ]\n",
    "#         output_batch = input_sequences_np[i:i + config.BATCH_SIZE]\n",
    "# \n",
    "#         total_loss = training_step(model, optimizer, input_batch, output_batch)\n",
    "# \n",
    "#         pbar.set_description(f'Epoch {e + 1}/{config.EPOCHS}')\n",
    "#         pbar.set_postfix_str(f'loss: {total_loss:.4f}')\n",
    "#         pbar.refresh()\n"
   ],
   "id": "3d89bd411edb5399",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T17:15:17.192553Z",
     "start_time": "2024-04-18T17:15:17.184972Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print('input_sequences_np shape:', input_sequences_np.shape)\n",
    "print('character_level_input_sequences_np shape:', character_level_input_sequences_np.shape)\n",
    "print('word_level_mask_tensor shape:', word_level_mask_tensor.shape)\n",
    "print('character_level_mask_tensor shape: ', character_level_mask_tensor.shape)"
   ],
   "id": "7db3ce6b17e007e7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_sequences_np shape: (300, 64)\n",
      "character_level_input_sequences_np shape: (300, 64, 16)\n",
      "word_level_mask_tensor shape: (300, 1, 64)\n",
      "character_level_mask_tensor shape:  (300, 64, 1, 16)\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T17:18:57.895674Z",
     "start_time": "2024-04-18T17:15:17.193705Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.compile(optimizer=optimizer, loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=['acc'])\n",
    "\n",
    "model.fit(\n",
    "    [[input_sequences_np, word_level_mask_tensor], [character_level_input_sequences_np, character_level_mask_tensor]],\n",
    "    input_sequences_np, epochs=config.EPOCHS,\n",
    "    batch_size=config.BATCH_SIZE)"
   ],
   "id": "280aa628ac5ce4ac",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "Start\n",
      "Shape của word_embedding_outputs: (50, 64, 128)\n",
      "WARNING:tensorflow:From /home/thanhan/.local/lib/python3.10/site-packages/tensorflow/python/util/deprecation.py:660: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n",
      "Encoder start\n",
      "input shape (64, 16, 32)\n",
      "mask shape (64, 1, 16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thanhan/.local/lib/python3.10/site-packages/keras/src/layers/layer.py:856: UserWarning: Layer 'query' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/thanhan/.local/lib/python3.10/site-packages/keras/src/layers/layer.py:856: UserWarning: Layer 'key' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/thanhan/.local/lib/python3.10/site-packages/keras/src/layers/layer.py:856: UserWarning: Layer 'value' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder end\n",
      "Encoder start\n",
      "input shape (64, 16, 32)\n",
      "mask shape (64, 1, 16)\n",
      "Encoder end\n",
      "Shape của character_level_encoder_outputs: (50, 64, 128)\n",
      "Shape của concat_output: (50, 64, 256)\n",
      "Encoder start\n",
      "input shape (50, 64, 256)\n",
      "mask shape (50, 1, 64)\n",
      "Encoder end\n",
      "Encoder start\n",
      "input shape (50, 64, 256)\n",
      "mask shape (50, 1, 64)\n",
      "Encoder end\n",
      "Shape của word_level_output: (50, 64, 256)\n",
      "Shape của correction_output: (50, 64, 10000)\n",
      "End\n",
      "Start\n",
      "Shape của word_embedding_outputs: (50, 64, 128)\n",
      "Encoder start\n",
      "input shape (64, 16, 32)\n",
      "mask shape (64, 1, 16)\n",
      "Encoder end\n",
      "Shape của character_level_encoder_outputs: (50, 64, 128)\n",
      "Shape của concat_output: (50, 64, 256)\n",
      "Encoder start\n",
      "input shape (50, 64, 256)\n",
      "mask shape (50, 1, 64)\n",
      "Encoder end\n",
      "Shape của word_level_output: (50, 64, 256)\n",
      "Shape của correction_output: (50, 64, 10000)\n",
      "End\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thanhan/.local/lib/python3.10/site-packages/keras/src/optimizers/base_optimizer.py:583: UserWarning: Gradients do not exist for variables ['kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n",
      "Shape của word_embedding_outputs: (50, 64, 128)\n",
      "Encoder start\n",
      "input shape (64, 16, 32)\n",
      "mask shape (64, 1, 16)\n",
      "Encoder end\n",
      "Shape của character_level_encoder_outputs: (50, 64, 128)\n",
      "Shape của concat_output: (50, 64, 256)\n",
      "Encoder start\n",
      "input shape (50, 64, 256)\n",
      "mask shape (50, 1, 64)\n",
      "Encoder end\n",
      "Shape của word_level_output: (50, 64, 256)\n",
      "Shape của correction_output: (50, 64, 10000)\n",
      "End\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m27s\u001B[0m 916ms/step - acc: 0.0000e+00 - loss: 9.3030\n",
      "Epoch 2/30\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 968ms/step - acc: 0.0000e+00 - loss: 9.2896\n",
      "Epoch 3/30\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 1s/step - acc: 7.2173e-05 - loss: 9.2534\n",
      "Epoch 4/30\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m10s\u001B[0m 980ms/step - acc: 3.4970e-05 - loss: 9.1951\n",
      "Epoch 5/30\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 957ms/step - acc: 1.4881e-05 - loss: 9.1165\n",
      "Epoch 6/30\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m11s\u001B[0m 1s/step - acc: 9.5982e-05 - loss: 9.0163\n",
      "Epoch 7/30\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 950ms/step - acc: 0.0169 - loss: 8.8968\n",
      "Epoch 8/30\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 967ms/step - acc: 0.3292 - loss: 8.7377\n",
      "Epoch 9/30\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 961ms/step - acc: 0.5252 - loss: 8.5897\n",
      "Epoch 10/30\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 959ms/step - acc: 0.5414 - loss: 8.3923\n",
      "Epoch 11/30\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m10s\u001B[0m 974ms/step - acc: 0.5264 - loss: 8.2240\n",
      "Epoch 12/30\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 979ms/step - acc: 0.5371 - loss: 8.0138\n",
      "Epoch 13/30\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 949ms/step - acc: 0.5380 - loss: 7.8162\n",
      "Epoch 14/30\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 957ms/step - acc: 0.5201 - loss: 7.6750\n",
      "Epoch 15/30\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 975ms/step - acc: 0.5336 - loss: 7.4786\n",
      "Epoch 16/30\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 967ms/step - acc: 0.5269 - loss: 7.3475\n",
      "Epoch 17/30\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 957ms/step - acc: 0.5461 - loss: 7.1549\n",
      "Epoch 18/30\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 954ms/step - acc: 0.5412 - loss: 7.0409\n",
      "Epoch 19/30\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m10s\u001B[0m 954ms/step - acc: 0.5345 - loss: 6.9473\n",
      "Epoch 20/30\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 994ms/step - acc: 0.5409 - loss: 6.8192\n",
      "Epoch 21/30\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 1s/step - acc: 0.5554 - loss: 6.6684\n",
      "Epoch 22/30\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 991ms/step - acc: 0.5367 - loss: 6.6239\n",
      "Epoch 23/30\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 971ms/step - acc: 0.5239 - loss: 6.5640\n",
      "Epoch 24/30\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m10s\u001B[0m 1s/step - acc: 0.5339 - loss: 6.4273\n",
      "Epoch 25/30\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 965ms/step - acc: 0.5225 - loss: 6.3571\n",
      "Epoch 26/30\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 1s/step - acc: 0.5514 - loss: 6.1520\n",
      "Epoch 27/30\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 986ms/step - acc: 0.5486 - loss: 6.0541\n",
      "Epoch 28/30\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 965ms/step - acc: 0.5475 - loss: 5.9444\n",
      "Epoch 29/30\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 984ms/step - acc: 0.5275 - loss: 5.9037\n",
      "Epoch 30/30\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 1s/step - acc: 0.5358 - loss: 5.7580\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x73cbabdb8970>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T17:20:07.750881Z",
     "start_time": "2024-04-18T17:20:07.743939Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_input_sentences = [\n",
    "    'Nếu làm được như vậy thì chắc chắn sẽ không còn trường nào tùy tiện thu tiền cao, gây sự lo lắng của phụ huynh và ai không có tiền thì không cần đóng.']\n",
    "test_input_sequences = word_level_tokenizer.texts_to_sequences(test_input_sentences)\n",
    "\n",
    "# Get character-level words lengths.\n",
    "test_input_words_lengths = []\n",
    "\n",
    "# Get character-level sequences.\n",
    "test_character_level_input_sequences = []\n",
    "\n",
    "for sequence in test_input_sequences:\n",
    "    character_level_input_sequence = []\n",
    "    test_words_lengths = []\n",
    "    for word_token in sequence:\n",
    "        word = word_level_tokenizer.index_word[word_token]\n",
    "        word = character_level_tokenizer.texts_to_sequences(word)\n",
    "        word_chars = [each[0] for each in word]\n",
    "        character_level_input_sequence.append(word_chars)\n",
    "        test_words_lengths.append((len(word_chars) if len(word_chars) <= config.MAX_WORD_LENGTH\n",
    "                                   else config.MAX_WORD_LENGTH))\n",
    "\n",
    "    # Add padding for each word.\n",
    "    character_level_input_sequence = pad_sequences(character_level_input_sequence, maxlen=config.MAX_WORD_LENGTH,\n",
    "                                                   padding='post', truncating='post')\n",
    "\n",
    "    test_character_level_input_sequences.append(character_level_input_sequence)\n",
    "\n",
    "    test_input_words_lengths.append(test_words_lengths)\n",
    "\n",
    "# Get word-level sentences lengths.\n",
    "test_input_sentences_lengths = []\n",
    "for sequence in test_input_sequences: test_input_sentences_lengths.append(\n",
    "    (len(sequence) if len(sequence) <= config.MAX_SENTENCE_LENGTH\n",
    "     else config.MAX_SENTENCE_LENGTH))\n",
    "\n",
    "# Add padding for each.\n",
    "test_input_sequences = pad_sequences(test_input_sequences, maxlen=config.MAX_SENTENCE_LENGTH, padding='post',\n",
    "                                     truncating='post')\n",
    "\n",
    "test_character_level_input_sequences = pad_sequences(test_character_level_input_sequences,\n",
    "                                                     maxlen=config.MAX_SENTENCE_LENGTH,\n",
    "                                                     padding='post', truncating='post')\n",
    "test_input_words_lengths = pad_sequences(test_input_words_lengths, maxlen=config.MAX_SENTENCE_LENGTH, padding='post',\n",
    "                                         truncating='post')\n",
    "\n",
    "test_input_sequences_np = np.array(test_input_sequences)\n",
    "test_character_level_input_sequences_np = np.array(test_character_level_input_sequences)\n",
    "\n",
    "test_input_words_lengths_np = np.array(test_input_words_lengths)\n",
    "test_input_sentences_lengths_np = np.array(test_input_sentences_lengths)"
   ],
   "id": "887a9ddaccd357ca",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T17:58:12.274478Z",
     "start_time": "2024-04-18T17:58:12.124701Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Character level mask\n",
    "test_character_level_mask_tensor = tf.map_fn(\n",
    "    lambda sentence: tf.map_fn(\n",
    "        lambda length: generate_sequence(length, config.MAX_WORD_LENGTH),\n",
    "        sentence,\n",
    "        dtype=tf.float32),\n",
    "    tf.convert_to_tensor(test_input_words_lengths_np),\n",
    "    dtype=tf.float32)\n",
    "character_level_mask_tensor = tf.expand_dims(character_level_mask_tensor, 2)\n",
    "\n",
    "# Word level mask\n",
    "test_word_level_mask_tensor = tf.map_fn(\n",
    "    lambda length: generate_sequence(length, config.MAX_SENTENCE_LENGTH),\n",
    "    tf.convert_to_tensor(test_input_sentences_lengths_np),\n",
    "    dtype=tf.float32\n",
    ")\n",
    "test_word_level_mask_tensor = tf.expand_dims(test_word_level_mask_tensor, 1)"
   ],
   "id": "60de91be75a14ea8",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Character level mask\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m test_character_level_mask_tensor \u001B[38;5;241m=\u001B[39m \u001B[43mtf\u001B[49m\u001B[38;5;241m.\u001B[39mmap_fn(\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;28;01mlambda\u001B[39;00m sentence: tf\u001B[38;5;241m.\u001B[39mmap_fn(\n\u001B[1;32m      4\u001B[0m         \u001B[38;5;28;01mlambda\u001B[39;00m length: generate_sequence(length, config\u001B[38;5;241m.\u001B[39mMAX_WORD_LENGTH),\n\u001B[1;32m      5\u001B[0m         sentence,\n\u001B[1;32m      6\u001B[0m         dtype\u001B[38;5;241m=\u001B[39mtf\u001B[38;5;241m.\u001B[39mfloat32),\n\u001B[1;32m      7\u001B[0m     tf\u001B[38;5;241m.\u001B[39mconvert_to_tensor(test_input_words_lengths_np),\n\u001B[1;32m      8\u001B[0m     dtype\u001B[38;5;241m=\u001B[39mtf\u001B[38;5;241m.\u001B[39mfloat32)\n\u001B[1;32m      9\u001B[0m character_level_mask_tensor \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mexpand_dims(character_level_mask_tensor, \u001B[38;5;241m2\u001B[39m)\n\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m# Word level mask\u001B[39;00m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'tf' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T17:18:57.933898Z",
     "start_time": "2024-04-18T17:18:57.909128Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print('test_input_sequences_np shape:', test_input_sequences_np.shape)\n",
    "print('test_character_level_input_sequences_np shape:', test_character_level_input_sequences_np.shape)\n",
    "print('test_input_sentences_lengths_np shape:', test_input_sentences_lengths_np.shape)\n",
    "print('test_input_words_lengths_np shape:', test_input_words_lengths_np.shape)"
   ],
   "id": "848ffb90c690acfe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_input_sequences_np shape: (1, 64)\n",
      "test_character_level_input_sequences_np shape: (1, 64, 16)\n",
      "test_input_sentences_lengths_np shape: (1,)\n",
      "test_input_words_lengths_np shape: (1, 64)\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T17:20:34.925961Z",
     "start_time": "2024-04-18T17:20:28.447497Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_outputs = model.predict(\n",
    "    [[input_sequences_np, word_level_mask_tensor], [character_level_input_sequences_np, character_level_mask_tensor]])"
   ],
   "id": "dea35fe03701c927",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n",
      "Shape của word_embedding_outputs: (32, 64, 128)\n",
      "Encoder start\n",
      "input shape (64, 16, 32)\n",
      "mask shape (64, 1, 16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thanhan/.local/lib/python3.10/site-packages/keras/src/layers/layer.py:856: UserWarning: Layer 'query' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/thanhan/.local/lib/python3.10/site-packages/keras/src/layers/layer.py:856: UserWarning: Layer 'key' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/thanhan/.local/lib/python3.10/site-packages/keras/src/layers/layer.py:856: UserWarning: Layer 'value' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder end\n",
      "Shape của character_level_encoder_outputs: (32, 64, 128)\n",
      "Shape của concat_output: (32, 64, 256)\n",
      "Encoder start\n",
      "input shape (32, 64, 256)\n",
      "mask shape (32, 1, 64)\n",
      "Encoder end\n",
      "Shape của word_level_output: (32, 64, 256)\n",
      "Shape của correction_output: (32, 64, 10000)\n",
      "End\n",
      "\u001B[1m 9/10\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 188ms/stepStart\n",
      "Shape của word_embedding_outputs: (None, 64, 128)\n",
      "Encoder start\n",
      "input shape (64, 16, 32)\n",
      "mask shape (64, 1, 16)\n",
      "Encoder end\n",
      "Shape của character_level_encoder_outputs: (None, 64, 128)\n",
      "Shape của concat_output: (None, 64, 256)\n",
      "Encoder start\n",
      "input shape (None, 64, 256)\n",
      "mask shape (None, 1, 64)\n",
      "Encoder end\n",
      "Shape của word_level_output: (None, 64, 256)\n",
      "Shape của correction_output: (None, 64, 10000)\n",
      "End\n",
      "\u001B[1m10/10\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 335ms/step\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T17:20:36.670239Z",
     "start_time": "2024-04-18T17:20:36.663341Z"
    }
   },
   "cell_type": "code",
   "source": "test_outputs",
   "id": "26b0fd8f1844dd32",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1.34092160e-02, 8.15533713e-05, 8.95445293e-04, ...,\n",
       "         8.30802164e-05, 5.97702492e-05, 8.05788077e-05],\n",
       "        [1.43980421e-02, 7.85101292e-05, 8.88241571e-04, ...,\n",
       "         8.29801138e-05, 5.60226290e-05, 7.99706904e-05],\n",
       "        [1.29829124e-02, 7.26265353e-05, 8.92338809e-04, ...,\n",
       "         8.06297612e-05, 5.97115904e-05, 8.38059059e-05],\n",
       "        ...,\n",
       "        [2.03308202e-02, 8.03431467e-05, 7.39415758e-04, ...,\n",
       "         8.45377435e-05, 5.80677406e-05, 7.99034751e-05],\n",
       "        [2.03096252e-02, 8.02201321e-05, 7.40636024e-04, ...,\n",
       "         8.36973486e-05, 5.73797806e-05, 7.68717582e-05],\n",
       "        [2.02360619e-02, 7.96054228e-05, 7.44595891e-04, ...,\n",
       "         8.25097959e-05, 5.75685845e-05, 7.47031081e-05]],\n",
       "\n",
       "       [[1.34867355e-02, 7.52851483e-05, 9.59397294e-04, ...,\n",
       "         8.10614511e-05, 5.40063447e-05, 8.01358547e-05],\n",
       "        [1.34922639e-02, 7.41325057e-05, 9.10002855e-04, ...,\n",
       "         8.62505112e-05, 5.93672303e-05, 8.35198953e-05],\n",
       "        [1.32182389e-02, 7.54703142e-05, 8.45684495e-04, ...,\n",
       "         8.18388799e-05, 5.83824731e-05, 8.35413884e-05],\n",
       "        ...,\n",
       "        [1.46102011e-02, 8.02804570e-05, 8.50911776e-04, ...,\n",
       "         8.37163825e-05, 5.62435489e-05, 8.13486477e-05],\n",
       "        [1.42542496e-02, 7.36357106e-05, 8.39550688e-04, ...,\n",
       "         7.66609373e-05, 5.41566878e-05, 7.21897086e-05],\n",
       "        [1.42485546e-02, 7.35358553e-05, 8.67783383e-04, ...,\n",
       "         8.16481188e-05, 5.06697106e-05, 7.38521849e-05]],\n",
       "\n",
       "       [[1.34581784e-02, 7.62090931e-05, 9.00526298e-04, ...,\n",
       "         8.45655231e-05, 5.78724066e-05, 7.95812521e-05],\n",
       "        [1.35386772e-02, 7.31865002e-05, 8.61849869e-04, ...,\n",
       "         7.76674206e-05, 6.16138641e-05, 8.11571881e-05],\n",
       "        [1.37722567e-02, 7.97077810e-05, 8.97299964e-04, ...,\n",
       "         8.34222592e-05, 5.99775267e-05, 8.47127376e-05],\n",
       "        ...,\n",
       "        [2.03143768e-02, 8.04951633e-05, 7.37526279e-04, ...,\n",
       "         8.45654067e-05, 5.82516986e-05, 7.96959866e-05],\n",
       "        [2.02966705e-02, 8.03843941e-05, 7.38831586e-04, ...,\n",
       "         8.37562184e-05, 5.75470331e-05, 7.67043093e-05],\n",
       "        [2.02333257e-02, 7.97623652e-05, 7.42860371e-04, ...,\n",
       "         8.25549869e-05, 5.77376704e-05, 7.45583529e-05]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[1.28936460e-02, 7.41447220e-05, 9.13443509e-04, ...,\n",
       "         7.37241353e-05, 5.99971354e-05, 7.97298417e-05],\n",
       "        [1.34056760e-02, 7.45757570e-05, 8.63782305e-04, ...,\n",
       "         8.40199573e-05, 5.62209752e-05, 8.52715602e-05],\n",
       "        [1.31328404e-02, 7.57162561e-05, 9.00286133e-04, ...,\n",
       "         8.59322390e-05, 6.15844110e-05, 8.58931526e-05],\n",
       "        ...,\n",
       "        [2.03262232e-02, 8.03835210e-05, 7.41162512e-04, ...,\n",
       "         8.46814073e-05, 5.79936896e-05, 7.96726090e-05],\n",
       "        [2.02979557e-02, 8.02871582e-05, 7.41993659e-04, ...,\n",
       "         8.38181440e-05, 5.72922036e-05, 7.66353915e-05],\n",
       "        [2.02170201e-02, 7.96735694e-05, 7.45902827e-04, ...,\n",
       "         8.26187897e-05, 5.74854894e-05, 7.44557983e-05]],\n",
       "\n",
       "       [[1.33813955e-02, 7.70785628e-05, 8.92777927e-04, ...,\n",
       "         8.37722109e-05, 5.63891881e-05, 7.96149179e-05],\n",
       "        [1.24515630e-02, 7.60206385e-05, 1.16073736e-03, ...,\n",
       "         7.78123213e-05, 5.79977350e-05, 7.70807528e-05],\n",
       "        [1.35060474e-02, 7.55509100e-05, 8.80236272e-04, ...,\n",
       "         8.66830378e-05, 5.88921794e-05, 8.79839208e-05],\n",
       "        ...,\n",
       "        [2.03157905e-02, 8.01988863e-05, 7.43256765e-04, ...,\n",
       "         8.47233823e-05, 5.79127554e-05, 7.96846216e-05],\n",
       "        [2.02885810e-02, 8.00982307e-05, 7.43983779e-04, ...,\n",
       "         8.38552951e-05, 5.72187128e-05, 7.66420490e-05],\n",
       "        [2.02095956e-02, 7.95012675e-05, 7.47721700e-04, ...,\n",
       "         8.26668620e-05, 5.73910729e-05, 7.44604040e-05]],\n",
       "\n",
       "       [[1.30518964e-02, 7.42408301e-05, 8.84826120e-04, ...,\n",
       "         8.51011719e-05, 5.88704388e-05, 8.64367830e-05],\n",
       "        [1.23412423e-02, 7.31264809e-05, 9.20871447e-04, ...,\n",
       "         7.93650979e-05, 6.27150439e-05, 8.20714558e-05],\n",
       "        [1.24215595e-02, 7.50294494e-05, 8.99804640e-04, ...,\n",
       "         8.09853664e-05, 6.03419248e-05, 8.06939133e-05],\n",
       "        ...,\n",
       "        [1.50725925e-02, 7.32061089e-05, 8.66183604e-04, ...,\n",
       "         8.39339409e-05, 5.57901112e-05, 8.07176402e-05],\n",
       "        [1.46361087e-02, 7.51844054e-05, 8.60412780e-04, ...,\n",
       "         7.96137756e-05, 5.09389647e-05, 7.77760943e-05],\n",
       "        [1.38607826e-02, 7.59357863e-05, 8.11643491e-04, ...,\n",
       "         8.25255702e-05, 5.29994650e-05, 7.62633499e-05]]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T17:20:40.335903Z",
     "start_time": "2024-04-18T17:20:40.272768Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for sentence in test_outputs[:20]:\n",
    "    out = ''\n",
    "    for word in sentence:\n",
    "        index = tf.argmax(word, axis=0).numpy()\n",
    "        word_str = word_level_tokenizer.index_word.get(index)\n",
    "        if word_str is not None:\n",
    "            out += word_str + ' '\n",
    "        else:\n",
    "            out += '<UNK> '\n",
    "    print(out)"
   ],
   "id": "e9a607df2c04a1b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> \n",
      "<UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> \n",
      "<UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> \n",
      "<UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> \n",
      "<UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> \n",
      "<UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> \n",
      "<UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> \n",
      "<UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> \n",
      "<UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> \n",
      "<UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> \n",
      "<UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> \n",
      "<UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> \n",
      "<UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> \n",
      "<UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> \n",
      "<UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> \n",
      "<UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> \n",
      "<UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> \n",
      "<UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> \n",
      "<UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> \n",
      "<UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> \n"
     ]
    }
   ],
   "execution_count": 26
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
