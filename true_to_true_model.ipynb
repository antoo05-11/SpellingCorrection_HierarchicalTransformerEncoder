{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.BATCH_SIZE = 50\n",
    "        self.EPOCHS = 5\n",
    "        self.NUM_OF_INPUTS = 300\n",
    "\n",
    "        self.NUM_CHARACTER_LEVEL_LAYERS = 4\n",
    "        self.NUM_WORD_LEVEL_LAYERS = 12\n",
    "\n",
    "        self.CHARACTER_LEVEL_D_MODEL = 32\n",
    "        self.WORD_LEVEL_D_MODEL = 128\n",
    "\n",
    "        self.NUM_HEADS = 3\n",
    "        self.DFF = 256\n",
    "\n",
    "        self.MAX_WORD_LENGTH = 16\n",
    "        self.MAX_SENTENCE_LENGTH = 64\n",
    "\n",
    "        self.VOCAB_SIZE = 10000\n",
    "        self.CHARACTER_VOCAB_SIZE = 1000\n",
    "\n",
    "\n",
    "config = Config()"
   ],
   "id": "1d94a7e3e9c116fa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ],
   "id": "4d4605e96cf0317a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Tokenizer",
   "id": "25bc207531cfa5b0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Global Attention and FeedForward",
   "id": "6bf6f107d372af26"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "        self.add = tf.keras.layers.Add()"
   ],
   "id": "cd141c3599d41736",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class GlobalSelfAttention(BaseAttention):\n",
    "    def call(self, inputs):\n",
    "        x, mask = inputs[0], inputs[1]\n",
    "\n",
    "        attn_output = self.mha(query=x, value=x, key=x, attention_mask=mask)\n",
    "\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "        return x"
   ],
   "id": "d59c351bb857beb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class FeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, dff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.seq = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(dff, activation='relu'),\n",
    "            tf.keras.layers.Dense(d_model),\n",
    "            tf.keras.layers.Dropout(dropout_rate)\n",
    "        ])\n",
    "        self.add = tf.keras.layers.Add()\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.add([x, self.seq(x)])\n",
    "        x = self.layer_norm(x)\n",
    "        return x"
   ],
   "id": "7bbb59f3cd166473",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Positional Embedding layer",
   "id": "ce082eb8b3293069"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def positional_encoding(length, depth):\n",
    "    depth = depth / 2\n",
    "\n",
    "    positions = np.arange(length)[:, np.newaxis]\n",
    "    depths = np.arange(depth)[np.newaxis, :] / depth\n",
    "\n",
    "    angle_rates = 1 / (10000 ** depths)\n",
    "    angle_rads = positions * angle_rates\n",
    "\n",
    "    pos_encoding = np.concatenate([np.sin(angle_rads), np.cos(angle_rads)], axis=-1)\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "\n",
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True)\n",
    "        self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n",
    "\n",
    "    def compute_mask(self, *args, **kwargs):\n",
    "        return self.embedding.compute_mask(*args, **kwargs)\n",
    "\n",
    "    def call(self, x):\n",
    "        length = tf.shape(x)[1]\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
    "        return x"
   ],
   "id": "5094824d30ca609d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Encoder layers",
   "id": "9a0a371a77c0f8c6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, *, d_model, num_heads, dff, dropout_rate=0.1, name='EncoderLayer'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attention = GlobalSelfAttention(\n",
    "            num_heads=num_heads, key_dim=d_model, dropout=dropout_rate, name=name\n",
    "        )\n",
    "\n",
    "        self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.self_attention(inputs)\n",
    "        x = self.ffn(x)\n",
    "        return x\n"
   ],
   "id": "d14cfd02447443b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "            self, *, num_layers, d_model, num_heads, dff, vocab_size, dropout_rate=0.1,\n",
    "            name='Encoder'\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        self.enc_layers = [\n",
    "            EncoderLayer(\n",
    "                d_model=d_model, num_heads=num_heads, dff=dff,\n",
    "                dropout_rate=dropout_rate,\n",
    "                name=f'encoder_layer_{i + 1}'\n",
    "            )\n",
    "            for i in range(num_layers)\n",
    "        ]\n",
    "        # self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, mask = inputs\n",
    "        print(\"Encoder start\")\n",
    "        print(\"input shape\", x.shape)\n",
    "        print(\"mask shape\", mask.shape)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            inputs = self.enc_layers[i]((x, mask))\n",
    "        print(\"Encoder end\")\n",
    "        return inputs  # Shape (batch_size, seq_len, d_model)."
   ],
   "id": "cb1b421d1ee7f65d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Hierarchical Transformer Encoder model",
   "id": "d36df6c730339222"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "@keras.utils.register_keras_serializable()\n",
    "class HierarchicalTransformerEncoder(tf.keras.models.Model):\n",
    "    def __init__(self, *, num_character_level_layers, num_word_level_layers,\n",
    "                 character_level_d_model, word_level_d_model, num_heads, dff,\n",
    "                 max_word_length, max_sentence_length,\n",
    "                 vocab_size, character_vocab_size, dropout_rate=0.1, **kwargs):\n",
    "        super(HierarchicalTransformerEncoder, self).__init__(**kwargs)\n",
    "        \n",
    "        self.num_character_level_layers = num_character_level_layers\n",
    "        self.num_word_level_layers = num_word_level_layers\n",
    "        self.character_level_d_model = character_level_d_model\n",
    "        self.word_level_d_model = word_level_d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.dff = dff\n",
    "        self.max_word_length = max_word_length\n",
    "        self.max_sentence_length = max_sentence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.character_vocab_size = character_vocab_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.word_pos_embedding = PositionalEmbedding(vocab_size=vocab_size,\n",
    "                                                      d_model=word_level_d_model)\n",
    "        self.character_pos_embedding = PositionalEmbedding(vocab_size=character_vocab_size,\n",
    "                                                           d_model=character_level_d_model)\n",
    "\n",
    "        self.character_level_encoder = Encoder(num_layers=num_character_level_layers,\n",
    "                                               d_model=character_level_d_model,\n",
    "                                               num_heads=num_heads, dff=dff,\n",
    "                                               vocab_size=character_vocab_size,\n",
    "                                               dropout_rate=dropout_rate,\n",
    "                                               name='character_level_encoder')\n",
    "\n",
    "        self.flatten_layer = tf.keras.layers.Flatten(input_shape=(max_word_length, character_level_d_model),\n",
    "                                                     name='flatten_character_level_encoders')\n",
    "        self.linear_layer = tf.keras.layers.Dense(units=word_level_d_model, activation=None,\n",
    "                                                  name='linear_character_level_encoders')\n",
    "\n",
    "        self.combined_layer = tf.keras.layers.Concatenate(axis=-1, name='combined_character_and_word_level_encoders')\n",
    "\n",
    "        self.word_level_encoder = Encoder(num_layers=num_word_level_layers,\n",
    "                                          d_model=(word_level_d_model * 2),\n",
    "                                          num_heads=num_heads, dff=dff,\n",
    "                                          vocab_size=vocab_size,\n",
    "                                          dropout_rate=dropout_rate,\n",
    "                                          name='word_level_encoder')\n",
    "\n",
    "        self.correction_layer = tf.keras.layers.Dense(vocab_size, activation='softmax', name='correction_layer', )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        print(\"Start\")\n",
    "        (word_level_inputs, word_level_mask_inputs), (character_level_inputs, character_level_mask_inputs) = inputs\n",
    "\n",
    "        word_embedding_outputs = self.word_pos_embedding(word_level_inputs)  # (batch_size, max_len, word_level_d_model)\n",
    "        print(\"Shape của word_embedding_outputs:\", word_embedding_outputs.shape)\n",
    "\n",
    "        character_level_encoder_outputs = tf.map_fn(\n",
    "            lambda sentence: self.character_level_encoder(\n",
    "                (self.character_pos_embedding(sentence[0]), sentence[1])),\n",
    "            (character_level_inputs, character_level_mask_inputs),\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "        # (batch_size, max_sen_len, max_word_length, character_level_d_model)\n",
    "\n",
    "        character_level_encoder_outputs = tf.map_fn(\n",
    "            lambda sentence: tf.map_fn(\n",
    "                lambda word: tf.squeeze(self.linear_layer(self.flatten_layer(tf.expand_dims(word, axis=0))), axis=0),\n",
    "                sentence,\n",
    "                dtype=tf.float32\n",
    "            ),\n",
    "            character_level_encoder_outputs,\n",
    "            dtype=tf.float32)\n",
    "        # (batch_size, max_sen_len, word_level_d_model)\n",
    "        print(\"Shape của character_level_encoder_outputs:\", character_level_encoder_outputs.shape)\n",
    "\n",
    "        concat_output = self.combined_layer([word_embedding_outputs, character_level_encoder_outputs])\n",
    "        # (batch_size, max_sen_len, word_level_d_model * 2)\n",
    "        print(\"Shape của concat_output:\", concat_output.shape)\n",
    "\n",
    "        word_level_output = self.word_level_encoder((concat_output, word_level_mask_inputs))\n",
    "        # (batch_size, max_sen_len, word_level_d_model + character_level_d_model)\n",
    "        print(\"Shape của word_level_output:\", word_level_output.shape)\n",
    "\n",
    "        correction_output = self.correction_layer(word_level_output)  # (batch_size, max_sen_len, vocab_size)\n",
    "        print(\"Shape của correction_output:\", correction_output.shape)\n",
    "\n",
    "        # detection_output = tf.squeeze(self.detection_layer(word_level_output), axis=-1)  # (batch_size, max_sen_len)\n",
    "        # print(\"Shape của detection_output:\", detection_output.shape)\n",
    "        print(\"End\")\n",
    "        return correction_output\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'num_character_level_layers': self.num_character_level_layers,\n",
    "            'num_word_level_layers': self.num_word_level_layers,\n",
    "            'character_level_d_model': self.character_level_d_model,\n",
    "            'word_level_d_model': self.word_level_d_model,\n",
    "            'num_heads': self.num_heads,\n",
    "            'dff': self.dff,\n",
    "            'max_word_length': self.max_word_length,\n",
    "            'max_sentence_length': self.max_sentence_length,\n",
    "            'vocab_size': self.vocab_size,\n",
    "            'character_vocab_size': self.character_vocab_size,\n",
    "            'dropout_rate': self.dropout_rate\n",
    "        }\n",
    "        base_config = super(HierarchicalTransformerEncoder, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def training_step(model, optimizer, x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        correction_output = model(x)\n",
    "        loss1 = correction_loss(y, correction_output)\n",
    "        # loss2 = detection_loss(y[1], detection_output)\n",
    "        total_loss = loss1\n",
    "\n",
    "    gradients = tape.gradient(total_loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "def correction_loss(true_outputs, pred_outputs):\n",
    "    softmax_loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)(true_outputs, pred_outputs)\n",
    "    return softmax_loss\n",
    "\n",
    "\n",
    "def detection_loss(true_detection_infos, pred_detection_infos):\n",
    "    sigmoid_loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)(true_detection_infos, pred_detection_infos)\n",
    "    return sigmoid_loss"
   ],
   "id": "d86e3b5b919c916a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data preprocessing",
   "id": "c52ce7c4a47ea323"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Custom Tokenizer",
   "id": "779e25cbfa313332"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class CustomTokenizer:\n",
    "    def __init__(self, word_vocab_size, max_word_len, max_sentence_len):\n",
    "        self.index_word = {1: '[UNK]'}\n",
    "        self.word_count = {'[UNK]': 1}\n",
    "        self.word_index = {'[UNK]': 1}\n",
    "        self.word_vocab_size = word_vocab_size\n",
    "\n",
    "        self.character_index = {'[UNK]': 1}\n",
    "        self.index_character = {1: '[UNK]'}\n",
    "\n",
    "        self.signs = [',', '.', '!', '\\\"', \"\\'\", \"?\", \";\"]\n",
    "\n",
    "        self.max_word_len = max_word_len\n",
    "        self.max_sentence_len = max_sentence_len\n",
    "\n",
    "    def preprocess_texts(self, texts):\n",
    "        texts_preprocessed = []\n",
    "        for text in texts:\n",
    "            for sign in self.signs:\n",
    "                text = text.replace(sign, ' ')\n",
    "\n",
    "            text = text.lower()\n",
    "            texts_preprocessed.append(text)\n",
    "        return texts_preprocessed\n",
    "\n",
    "    def sort(self):\n",
    "        self.word_count = sorted(list(self.word_count.items())[1:], key=lambda x: x[1], reverse=True)\n",
    "        for i, (word, count) in enumerate(self.word_count):\n",
    "            self.index_word[i + 2] = word\n",
    "            self.word_index[word] = i + 2\n",
    "\n",
    "        self.word_count = dict(self.word_count)\n",
    "\n",
    "    def texts_to_sequences(self, texts):\n",
    "        word_level_sequences = []\n",
    "        character_level_sequences = []\n",
    "\n",
    "        sentence_lengths = []\n",
    "        word_lengths = []\n",
    "\n",
    "        texts = self.preprocess_texts(texts)\n",
    "        for text in texts:\n",
    "            words = text.split(' ')\n",
    "            words = [i for i in words if i != '']\n",
    "\n",
    "            tokens = [self.word_index.get(word, 1)\n",
    "                      if (self.word_index.get(word, 1) <= self.word_vocab_size) else 1\n",
    "                      for word in words]\n",
    "            word_level_sequences.append(tokens)\n",
    "            sentence_lengths.append(min(len(tokens), self.max_sentence_len))\n",
    "\n",
    "            sentence_tokens = []\n",
    "            word_length = []\n",
    "            for word in words:\n",
    "                tokens = [self.character_index.get(i, 1) for i in word]\n",
    "                word_length.append(min(len(tokens), self.max_word_len))\n",
    "                sentence_tokens.append(tokens)\n",
    "            character_level_sequences.append(sentence_tokens)\n",
    "            word_lengths.append(word_length)\n",
    "\n",
    "        return (word_level_sequences, sentence_lengths), (character_level_sequences, word_lengths)\n",
    "\n",
    "    def sequences_to_texts(self, sequences):\n",
    "        texts = []\n",
    "        for sequence in sequences:\n",
    "            texts.append([self.index_word.get(token, 1) for token in sequence])\n",
    "        return texts\n",
    "\n",
    "    def fit_on_texts(self, texts):\n",
    "\n",
    "        texts = self.preprocess_texts(texts)\n",
    "\n",
    "        for text in texts:\n",
    "\n",
    "            # Update word vocab.\n",
    "            words = text.split(' ')\n",
    "            for word in words:\n",
    "                if word == '': continue\n",
    "                self.word_count[word] = self.word_count.get(word, 0) + 1\n",
    "\n",
    "            # Update character vocab\n",
    "            for character in text:\n",
    "                if character == ' ': continue\n",
    "                if self.character_index.get(character, -1) == -1:\n",
    "                    self.character_index[character] = len(self.character_index) + 1\n",
    "\n",
    "                self.index_character[self.character_index[character]] = character\n",
    "\n",
    "        self.sort()"
   ],
   "id": "fdcb553bf57b6307",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data = pd.read_csv('data/vi_processed.csv')\n",
    "\n",
    "input_sentences = []\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    if len(input_sentences) == config.NUM_OF_INPUTS: break\n",
    "    input_sentences.append(row.correct_text)\n",
    "\n",
    "correct_texts = input_sentences[:config.NUM_OF_INPUTS]"
   ],
   "id": "dc8b24f26c3728a9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "word_level_tokenizer = CustomTokenizer(word_vocab_size=config.VOCAB_SIZE, max_word_len=config.MAX_WORD_LENGTH,\n",
    "                                       max_sentence_len=config.MAX_SENTENCE_LENGTH)\n",
    "word_level_tokenizer.fit_on_texts(input_sentences)"
   ],
   "id": "b04c7f90b2c479e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "(input_word_level_sequences, input_sentence_lengths), (\n",
    "    input_character_level_sequences, input_word_lengths) = word_level_tokenizer.texts_to_sequences(input_sentences)\n",
    "\n",
    "input_word_level_sequences = pad_sequences(input_word_level_sequences, maxlen=config.MAX_SENTENCE_LENGTH,\n",
    "                                           padding='post', truncating='post')\n",
    "\n",
    "tmp_input_character_level_sequences = []\n",
    "\n",
    "for input_character_level_sequence in input_character_level_sequences:\n",
    "    while len(input_character_level_sequence) < config.MAX_SENTENCE_LENGTH:\n",
    "        input_character_level_sequence.append([])\n",
    "    while len(input_character_level_sequence) > config.MAX_SENTENCE_LENGTH:\n",
    "        del input_character_level_sequence[-1]\n",
    "    tmp_input_character_level_sequences.append(\n",
    "        pad_sequences(input_character_level_sequence, maxlen=config.MAX_WORD_LENGTH,\n",
    "                      padding='post', truncating='post'))\n",
    "\n",
    "input_character_level_sequences = tmp_input_character_level_sequences\n",
    "\n",
    "input_word_lengths = pad_sequences(input_word_lengths, maxlen=config.MAX_SENTENCE_LENGTH,\n",
    "                                   padding='post', truncating='post')"
   ],
   "id": "84713dff10f19386",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "input_character_level_sequences[2]",
   "id": "2230f5e6eb6840b4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "input_word_level_sequences_np = np.array(input_word_level_sequences)\n",
    "input_character_level_sequences_np = np.array(input_character_level_sequences)\n",
    "input_sentence_lengths_np = np.array(input_sentence_lengths)\n",
    "input_word_lengths_np = np.array(input_word_lengths)"
   ],
   "id": "8efe2f717d2fdebd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def generate_sequence(length, max_length, mask_ratio=0.85):\n",
    "    one_tensor_length = tf.cast(tf.cast(length, tf.float32) * mask_ratio, tf.int32)\n",
    "\n",
    "    masked_sequence = tf.random.shuffle(tf.sequence_mask(one_tensor_length, maxlen=length, dtype=tf.float32))\n",
    "\n",
    "    padded_sequence = tf.pad(masked_sequence, paddings=[[0, max_length - length]], mode='CONSTANT',\n",
    "                             constant_values=0.0)\n",
    "\n",
    "    return padded_sequence\n",
    "\n",
    "\n",
    "# Character level mask\n",
    "character_level_mask_tensor = tf.map_fn(\n",
    "    lambda sentence: tf.map_fn(\n",
    "        lambda length: generate_sequence(length, config.MAX_WORD_LENGTH),\n",
    "        sentence,\n",
    "        dtype=tf.float32),\n",
    "    tf.convert_to_tensor(input_word_lengths_np),\n",
    "    dtype=tf.float32)\n",
    "character_level_mask_tensor = tf.expand_dims(character_level_mask_tensor, 2)\n",
    "\n",
    "# Word level mask\n",
    "word_level_mask_tensor = tf.map_fn(\n",
    "    lambda length: generate_sequence(length, config.MAX_SENTENCE_LENGTH),\n",
    "    tf.convert_to_tensor(input_sentence_lengths_np),\n",
    "    dtype=tf.float32\n",
    ")\n",
    "word_level_mask_tensor = tf.expand_dims(word_level_mask_tensor, 1)"
   ],
   "id": "d516a590c38aa7ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model building",
   "id": "f42308d87fe8aa3f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = HierarchicalTransformerEncoder(num_character_level_layers=config.NUM_CHARACTER_LEVEL_LAYERS,\n",
    "                                       num_word_level_layers=config.NUM_WORD_LEVEL_LAYERS,\n",
    "                                       character_level_d_model=config.CHARACTER_LEVEL_D_MODEL,\n",
    "                                       word_level_d_model=config.WORD_LEVEL_D_MODEL,\n",
    "                                       num_heads=config.NUM_HEADS, dff=config.DFF,\n",
    "                                       max_word_length=config.MAX_WORD_LENGTH,\n",
    "                                       max_sentence_length=config.MAX_SENTENCE_LENGTH,\n",
    "                                       vocab_size=config.VOCAB_SIZE,\n",
    "                                       character_vocab_size=config.CHARACTER_VOCAB_SIZE)"
   ],
   "id": "82d3104dfc17ed18",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "@keras.utils.register_keras_serializable()\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, dtype=tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            'd_model': float(self.d_model),\n",
    "            'warmup_steps': self.warmup_steps\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "\n",
    "\n",
    "learning_rate = CustomSchedule(config.WORD_LEVEL_D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
    "                                     epsilon=1e-9)"
   ],
   "id": "fb68d56c770a68ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# from tqdm import tqdm\n",
    "# \n",
    "# for e in range(config.EPOCHS):\n",
    "#     pbar = tqdm(range(0, len(input_sequences_np), config.BATCH_SIZE))\n",
    "# \n",
    "#     for i in pbar:\n",
    "#         input_batch = [\n",
    "#             [input_sequences_np[i:i + config.BATCH_SIZE], input_sentences_lengths_np[i:i + config.BATCH_SIZE]],\n",
    "#             [character_level_input_sequences_np[i:i + config.BATCH_SIZE],\n",
    "#              input_words_lengths_np[i:i + config.BATCH_SIZE]]\n",
    "#         ]\n",
    "#         output_batch = input_sequences_np[i:i + config.BATCH_SIZE]\n",
    "# \n",
    "#         total_loss = training_step(model, optimizer, input_batch, output_batch)\n",
    "# \n",
    "#         pbar.set_description(f'Epoch {e + 1}/{config.EPOCHS}')\n",
    "#         pbar.set_postfix_str(f'loss: {total_loss:.4f}')\n",
    "#         pbar.refresh()\n"
   ],
   "id": "3d89bd411edb5399",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print('input_sequences_np shape:', input_word_level_sequences_np.shape)\n",
    "print('character_level_input_sequences_np shape:', input_character_level_sequences_np.shape)\n",
    "print('word_level_mask_tensor shape:', word_level_mask_tensor.shape)\n",
    "print('character_level_mask_tensor shape: ', character_level_mask_tensor.shape)"
   ],
   "id": "7db3ce6b17e007e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model.compile(optimizer=optimizer, loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=['acc'])\n",
    "\n",
    "model.fit(\n",
    "    [[input_word_level_sequences_np, word_level_mask_tensor],\n",
    "     [input_character_level_sequences_np, character_level_mask_tensor]],\n",
    "    input_word_level_sequences_np, epochs=config.EPOCHS,\n",
    "    batch_size=config.BATCH_SIZE)"
   ],
   "id": "280aa628ac5ce4ac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Test model",
   "id": "2f0ddc1ee52a8868"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_input_sentences = [\n",
    "    'Nếu làm được như vậy thì chắc chắn xẽ không còn trường nào tùy tiện thu tiền cao, gây sự lo lắng của phụ huynh và ai không có tiền thì không cần đóng.']\n",
    "(test_input_word_level_sequences, test_input_sentence_lengths), (\n",
    "    test_input_character_level_sequences, test_input_word_lengths) = word_level_tokenizer.texts_to_sequences(\n",
    "    test_input_sentences)\n",
    "\n",
    "test_input_word_level_sequences = pad_sequences(test_input_word_level_sequences, maxlen=config.MAX_SENTENCE_LENGTH,\n",
    "                                                padding='post', truncating='post')\n",
    "\n",
    "test_tmp_input_character_level_sequences = []\n",
    "\n",
    "for test_input_character_level_sequence in test_input_character_level_sequences:\n",
    "    while len(test_input_character_level_sequence) < config.MAX_SENTENCE_LENGTH:\n",
    "        test_input_character_level_sequence.append([])\n",
    "    while len(test_input_character_level_sequence) > config.MAX_SENTENCE_LENGTH:\n",
    "        del test_input_character_level_sequence[-1]\n",
    "    test_tmp_input_character_level_sequences.append(\n",
    "        pad_sequences(test_input_character_level_sequence, maxlen=config.MAX_WORD_LENGTH,\n",
    "                      padding='post', truncating='post'))\n",
    "\n",
    "test_input_character_level_sequences = test_tmp_input_character_level_sequences\n",
    "\n",
    "test_input_word_lengths = pad_sequences(test_input_word_lengths, maxlen=config.MAX_SENTENCE_LENGTH,\n",
    "                                        padding='post', truncating='post')"
   ],
   "id": "887a9ddaccd357ca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_input_word_level_sequences_np = np.array(test_input_word_level_sequences)\n",
    "test_input_character_level_sequences_np = np.array(test_input_character_level_sequences)\n",
    "test_input_sentence_lengths_np = np.array(test_input_sentence_lengths)\n",
    "test_input_word_lengths_np = np.array(test_input_word_lengths)"
   ],
   "id": "a76bc4c924f16e94",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Character level mask\n",
    "test_character_level_mask_tensor = tf.map_fn(\n",
    "    lambda sentence: tf.map_fn(\n",
    "        lambda length: generate_sequence(length, config.MAX_WORD_LENGTH, mask_ratio=1),\n",
    "        sentence,\n",
    "        dtype=tf.float32),\n",
    "    tf.convert_to_tensor(test_input_word_lengths_np),\n",
    "    dtype=tf.float32)\n",
    "test_character_level_mask_tensor = tf.expand_dims(test_character_level_mask_tensor, 2)\n",
    "\n",
    "# Word level mask\n",
    "test_word_level_mask_tensor = tf.map_fn(\n",
    "    lambda length: generate_sequence(length, config.MAX_SENTENCE_LENGTH, mask_ratio=1),\n",
    "    tf.convert_to_tensor(test_input_sentence_lengths),\n",
    "    dtype=tf.float32\n",
    ")\n",
    "test_word_level_mask_tensor = tf.expand_dims(test_word_level_mask_tensor, 1)"
   ],
   "id": "60de91be75a14ea8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print('input_sequences_np shape:', test_input_word_level_sequences_np.shape)\n",
    "print('character_level_input_sequences_np shape:', test_input_character_level_sequences_np.shape)\n",
    "print('word_level_mask_tensor shape:', test_word_level_mask_tensor.shape)\n",
    "print('character_level_mask_tensor shape: ', test_character_level_mask_tensor.shape)"
   ],
   "id": "848ffb90c690acfe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_outputs = model.predict(\n",
    "    [[test_input_word_level_sequences_np, test_word_level_mask_tensor],\n",
    "     [test_input_character_level_sequences_np, test_character_level_mask_tensor]])"
   ],
   "id": "dea35fe03701c927",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for sentence in test_outputs[:20]:\n",
    "    out = ''\n",
    "    for word in sentence:\n",
    "        index = tf.argmax(word, axis=0).numpy()\n",
    "        word_str = word_level_tokenizer.index_word.get(index)\n",
    "        if word_str is not None:\n",
    "            out += word_str + ' '\n",
    "        else:\n",
    "            out += '<UNK> '\n",
    "    print(out)"
   ],
   "id": "e9a607df2c04a1b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model.save('model.keras')\n",
   "id": "66f68273100fa981",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "new_model = tf.keras.models.load_model('model.keras')",
   "id": "de77ed94ea1d1ddd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_outputs = new_model.predict(\n",
    "    [[test_input_word_level_sequences_np, test_word_level_mask_tensor],\n",
    "     [test_input_character_level_sequences_np, test_character_level_mask_tensor]])"
   ],
   "id": "7f8ca18d8f521b1e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
