{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from Config import Config\n",
    "\n",
    "config = Config()"
   ],
   "id": "b174dea5b591cfc7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "data = pd.read_csv('data/vi_processed.csv')\n",
    "\n",
    "correct_texts = []\n",
    "error_texts = []\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    if len(correct_texts) == 100: break\n",
    "    correct_texts.append(row.correct_text)\n",
    "    error_texts.append(row.error_text)\n",
    "\n",
    "correct_texts = correct_texts[:config.NUM_OF_INPUTS]\n",
    "error_texts = error_texts[:config.NUM_OF_INPUTS]"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "word_level_tokenizer = Tokenizer(num_words=config.VOCAB_SIZE, oov_token='<UNK>', lower=True, split=' ', )\n",
    "\n",
    "word_unk_level_tokenizer = Tokenizer(oov_token='<UNK>', lower=True, split=' ', )\n",
    "\n",
    "character_level_tokenizer = Tokenizer(num_words=config.CHARACTER_VOCAB_SIZE, lower=True, char_level=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b3752f20d0d04fc5",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "word_level_tokenizer.fit_on_texts(correct_texts)\n",
    "word_unk_level_tokenizer.fit_on_texts(error_texts)\n",
    "character_level_tokenizer.fit_on_texts(error_texts)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "da68b19fddda92a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "input_sequences = word_level_tokenizer.texts_to_sequences(error_texts)\n",
    "output_sequences = word_level_tokenizer.texts_to_sequences(correct_texts)\n",
    "unk_input_sequences = word_unk_level_tokenizer.texts_to_sequences(error_texts)\n",
    "\n",
    "# Get character-level words lengths.\n",
    "input_words_lengths = []\n",
    "\n",
    "# Get character-level sequences.\n",
    "character_level_input_sequences = []\n",
    "\n",
    "for sequence in unk_input_sequences:\n",
    "    character_level_input_sequence = []\n",
    "    words_lengths = []\n",
    "    for word_token in sequence:\n",
    "        word = word_unk_level_tokenizer.index_word[word_token]\n",
    "        word = character_level_tokenizer.texts_to_sequences(word)\n",
    "        word_chars = [each[0] for each in word]\n",
    "        character_level_input_sequence.append(word_chars)\n",
    "        words_lengths.append((len(word_chars) if len(word_chars) <= config.MAX_WORD_LENGTH\n",
    "                              else config.MAX_WORD_LENGTH))\n",
    "\n",
    "    # Add padding for each word.\n",
    "    character_level_input_sequence = pad_sequences(character_level_input_sequence, maxlen=config.MAX_WORD_LENGTH,\n",
    "                                                   padding='post', truncating='post')\n",
    "\n",
    "    character_level_input_sequences.append(character_level_input_sequence)\n",
    "\n",
    "    input_words_lengths.append(words_lengths)\n",
    "\n",
    "# Get word-level sentences lengths.\n",
    "input_sentences_lengths = []\n",
    "for sequence in input_sequences: input_sentences_lengths.append(\n",
    "    (len(sequence) if len(sequence) <= config.MAX_SENTENCE_LENGTH\n",
    "     else config.MAX_SENTENCE_LENGTH))\n",
    "\n",
    "# Add padding for each.\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=config.MAX_SENTENCE_LENGTH, padding='post', truncating='post')\n",
    "output_sequences = pad_sequences(output_sequences, maxlen=config.MAX_SENTENCE_LENGTH, padding='post', truncating='post')\n",
    "character_level_input_sequences = pad_sequences(character_level_input_sequences, maxlen=config.MAX_SENTENCE_LENGTH,\n",
    "                                                padding='post', truncating='post')\n",
    "input_words_lengths = pad_sequences(input_words_lengths, maxlen=config.MAX_SENTENCE_LENGTH, padding='post',\n",
    "                                    truncating='post')\n",
    "\n",
    "input_sequences_np = np.array(input_sequences)\n",
    "character_level_input_sequences_np = np.array(character_level_input_sequences)\n",
    "output_sequences_np = np.array(output_sequences)\n",
    "\n",
    "input_words_lengths_np = np.array(input_words_lengths)\n",
    "input_sentences_lengths_np = np.array(input_sentences_lengths)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "40b825572d89fe75",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "(input_sentences_lengths[0])",
   "metadata": {
    "collapsed": false
   },
   "id": "1d73054d0b834a9f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# def prepare_batch(input, output):\n",
    "#     word_level_input = word_level_tokenizer.texts_to_sequences(input)\n",
    "#     word_level_input = tf.ragged.constant(word_level_input)\n",
    "#     word_level_input = word_level_input[:, :MAX_WORD_LEVEL_TOKENS]\n",
    "#     word_level_input = word_level_input.to_tensor()\n",
    "# \n",
    "#     unk_input_sequences = word_unk_level_tokenizer.texts_to_sequences(input)\n",
    "#     character_level_input_sequences = []\n",
    "# \n",
    "#     for sequence in unk_input_sequences:\n",
    "#         character_level_input_sequence = []\n",
    "#         for word_token in sequence:\n",
    "#             word = word_unk_level_tokenizer.index_word[word_token]\n",
    "#             word = character_level_tokenizer.texts_to_sequences(word)\n",
    "#             word_chars = [each[0] for each in word]\n",
    "#             character_level_input_sequence.append(word_chars)\n",
    "#         character_level_input_sequence = tf.ragged.constant(character_level_input_sequence)\n",
    "#         character_level_input_sequence = character_level_input_sequence[\n",
    "#                                          :MAX_SENTENCE_LENGTH,\n",
    "#                                          :MAX_WORD_LENGTH]\n",
    "#         character_level_input_sequence = character_level_input_sequence.to_tensor()\n",
    "#         character_level_input_sequences.append(character_level_input_sequence)\n",
    "# \n",
    "#     print(character_level_input_sequences)\n",
    "#     character_level_input_sequences = tf.ragged.constant(character_level_input_sequences)\n",
    "# \n",
    "#     output = word_level_tokenizer.texts_to_sequences(output)\n",
    "#     output = tf.ragged.constant(output)\n",
    "#     output = output[:, :MAX_SENTENCE_LENGTH]\n",
    "#     output = output.to_tensor()\n",
    "# \n",
    "#     return (word_level_input, character_level_input_sequences), output"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "23d34deb433228db",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from HierarchicalTransformerEncoder import HierarchicalTransformerEncoder\n",
    "\n",
    "model = HierarchicalTransformerEncoder(num_character_level_layers=config.NUM_CHARACTER_LEVEL_LAYERS,\n",
    "                                       num_word_level_layers=config.NUM_WORD_LEVEL_LAYERS,\n",
    "                                       character_level_d_model=config.CHARACTER_LEVEL_D_MODEL,\n",
    "                                       word_level_d_model=config.WORD_LEVEL_D_MODEL,\n",
    "                                       num_heads=config.NUM_HEADS, dff=config.DFF,\n",
    "                                       max_word_length=config.MAX_WORD_LENGTH,\n",
    "                                       max_sentence_length=config.MAX_SENTENCE_LENGTH,\n",
    "                                       vocab_size=config.VOCAB_SIZE,\n",
    "                                       character_vocab_size=config.CHARACTER_VOCAB_SIZE)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7588ce8d87f47c5b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from CustomSchedule import CustomSchedule\n",
    "\n",
    "learning_rate = CustomSchedule(config.WORD_LEVEL_D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
    "                                     epsilon=1e-9)"
   ],
   "id": "58c4565b240a0060",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from HierarchicalTransformerEncoder import custom_loss\n",
    "\n",
    "# word_input_shape = (config.BATCH_SIZE, config.MAX_SENTENCE_LENGTH)\n",
    "# char_input_shape = (config.BATCH_SIZE, config.MAX_SENTENCE_LENGTH, config.MAX_WORD_LENGTH)\n",
    "# \n",
    "# model.build(input_shape=[word_input_shape, char_input_shape])\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "# model.summary()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "19369c0f61882e99",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ffb0038c5420ac9b"
  },
  {
   "cell_type": "code",
   "source": [
    "model.fit(\n",
    "    [[input_sequences_np, input_sentences_lengths_np], [character_level_input_sequences_np, input_words_lengths_np]],\n",
    "    output_sequences_np, epochs=config.EPOCHS,\n",
    "    batch_size=config.BATCH_SIZE)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d959e3830400be73",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Test model",
   "id": "e5cadd1c41806fb9"
  },
  {
   "cell_type": "code",
   "source": "test_output = model.predict([[input_sequences_np, input_sentences_lengths_np], [character_level_input_sequences_np, input_words_lengths_np]])",
   "metadata": {
    "collapsed": false
   },
   "id": "97fb992fbe1f2992",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "for sentence in test_output[:100]:\n",
    "    out = ''\n",
    "    for word in sentence:\n",
    "        index = tf.argmax(word, axis=0).numpy()\n",
    "        word_str = word_level_tokenizer.index_word.get(index)\n",
    "        if word_str is not None:\n",
    "            out += word_str + ' '\n",
    "        else: out += '<UNK> ' \n",
    "    print(out)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c63689e16ef959dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model.save('model.tf')",
   "id": "ba3e76255182823a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "loaded_model = tf.keras.models.load_model('model.tf', custom_objects={'CustomSchedule':                   \n",
    "CustomSchedule})"
   ],
   "id": "2c429e3bdb1c3a2d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "loaded_model.predict([[input_sequences_np, input_sentences_lengths_np], [character_level_input_sequences_np, input_words_lengths_np]])",
   "id": "1cbfb444397f6084",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
