{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "data = pd.read_csv('data/vi_processed.csv')\n",
    "\n",
    "correct_texts = []\n",
    "error_texts = []\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    if len(correct_texts) == 100: break\n",
    "    correct_texts.append(row.correct_text)\n",
    "    error_texts.append(row.error_text)\n",
    "\n",
    "correct_texts = correct_texts[:10000]\n",
    "error_texts = error_texts[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "vocab_size = 100000\n",
    "word_level_tokenizer = Tokenizer(num_words=vocab_size, oov_token='<UNK>', lower=True, split=' ', )\n",
    "\n",
    "word_unk_level_tokenizer = Tokenizer(oov_token='<UNK>', lower=True, split=' ', )\n",
    "\n",
    "character_vocab_size = 10000\n",
    "character_level_tokenizer = Tokenizer(num_words=character_vocab_size, lower=True, char_level=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b3752f20d0d04fc5",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "word_level_tokenizer.fit_on_texts(correct_texts)\n",
    "word_unk_level_tokenizer.fit_on_texts(error_texts)\n",
    "character_level_tokenizer.fit_on_texts(error_texts)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "da68b19fddda92a7",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "MAX_WORD_LENGTH = 16\n",
    "MAX_SENTENCE_LENGTH = 64"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca35672e7370971f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "input_sequences = word_level_tokenizer.texts_to_sequences(error_texts)\n",
    "output_sequences = word_level_tokenizer.texts_to_sequences(correct_texts)\n",
    "unk_input_sequences = word_unk_level_tokenizer.texts_to_sequences(error_texts)\n",
    "\n",
    "character_level_input_sequences = []\n",
    "\n",
    "for sequence in unk_input_sequences:\n",
    "    character_level_input_sequence = []\n",
    "    for word_token in sequence:\n",
    "        word = word_unk_level_tokenizer.index_word[word_token]\n",
    "        word = character_level_tokenizer.texts_to_sequences(word)\n",
    "        word_chars = [each[0] for each in word]\n",
    "        character_level_input_sequence.append(word_chars)\n",
    "    character_level_input_sequence = pad_sequences(character_level_input_sequence, maxlen=MAX_WORD_LENGTH,\n",
    "                                                   padding='post', truncating='post')\n",
    "    character_level_input_sequences.append(character_level_input_sequence)\n",
    "\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=MAX_SENTENCE_LENGTH, padding='post', truncating='post')\n",
    "output_sequences = pad_sequences(output_sequences, maxlen=MAX_SENTENCE_LENGTH, padding='post', truncating='post')\n",
    "\n",
    "character_level_input_sequences = pad_sequences(character_level_input_sequences, maxlen=MAX_SENTENCE_LENGTH,\n",
    "                                                padding='post',\n",
    "                                                truncating='post')\n",
    "\n",
    "input_sequences_np = np.array(input_sequences)\n",
    "character_level_input_sequences_np = np.array(character_level_input_sequences)\n",
    "output_sequences_np = np.array(output_sequences)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "40b825572d89fe75",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "output_sequences_np[0]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1d73054d0b834a9f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# def prepare_batch(input, output):\n",
    "#     word_level_input = word_level_tokenizer.texts_to_sequences(input)\n",
    "#     word_level_input = tf.ragged.constant(word_level_input)\n",
    "#     word_level_input = word_level_input[:, :MAX_WORD_LEVEL_TOKENS]\n",
    "#     word_level_input = word_level_input.to_tensor()\n",
    "# \n",
    "#     unk_input_sequences = word_unk_level_tokenizer.texts_to_sequences(input)\n",
    "#     character_level_input_sequences = []\n",
    "# \n",
    "#     for sequence in unk_input_sequences:\n",
    "#         character_level_input_sequence = []\n",
    "#         for word_token in sequence:\n",
    "#             word = word_unk_level_tokenizer.index_word[word_token]\n",
    "#             word = character_level_tokenizer.texts_to_sequences(word)\n",
    "#             word_chars = [each[0] for each in word]\n",
    "#             character_level_input_sequence.append(word_chars)\n",
    "#         character_level_input_sequence = tf.ragged.constant(character_level_input_sequence)\n",
    "#         character_level_input_sequence = character_level_input_sequence[\n",
    "#                                          :MAX_SENTENCE_LENGTH,\n",
    "#                                          :MAX_WORD_LENGTH]\n",
    "#         character_level_input_sequence = character_level_input_sequence.to_tensor()\n",
    "#         character_level_input_sequences.append(character_level_input_sequence)\n",
    "# \n",
    "#     print(character_level_input_sequences)\n",
    "#     character_level_input_sequences = tf.ragged.constant(character_level_input_sequences)\n",
    "# \n",
    "#     output = word_level_tokenizer.texts_to_sequences(output)\n",
    "#     output = tf.ragged.constant(output)\n",
    "#     output = output[:, :MAX_SENTENCE_LENGTH]\n",
    "#     output = output.to_tensor()\n",
    "# \n",
    "#     return (word_level_input, character_level_input_sequences), output"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "23d34deb433228db",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 20000\n",
    "BATCH_SIZE = 200"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "99c7a4166f1a5a5b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from HierarchicalTransformerEncoder import HierarchicalTransformerEncoder\n",
    "\n",
    "model = HierarchicalTransformerEncoder(num_character_level_layers=4,\n",
    "                                       num_word_level_layers=12,\n",
    "                                       character_level_d_model=64,\n",
    "                                       word_level_d_model=128,\n",
    "                                       num_heads=3, dff=512,\n",
    "                                       max_word_length=MAX_WORD_LENGTH,\n",
    "                                       max_sentence_length=MAX_SENTENCE_LENGTH,\n",
    "                                       vocab_size=vocab_size,\n",
    "                                       character_vocab_size=character_vocab_size)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7588ce8d87f47c5b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from HierarchicalTransformerEncoder import custom_loss\n",
    "\n",
    "word_input_shape = (BATCH_SIZE, MAX_SENTENCE_LENGTH)\n",
    "char_input_shape = (BATCH_SIZE, MAX_SENTENCE_LENGTH, MAX_WORD_LENGTH)\n",
    "\n",
    "model.build(input_shape=[word_input_shape, char_input_shape])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "19369c0f61882e99",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ffb0038c5420ac9b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# model.fit([input_sequences_np, character_level_input_sequences_np], output_sequences_np, epochs=20, batch_size=BATCH_SIZE)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d959e3830400be73",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "test_output = model.predict([input_sequences_np, character_level_input_sequences_np])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "97fb992fbe1f2992",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for sentence in test_output[:]:\n",
    "    out = ''\n",
    "    for word in sentence:\n",
    "        out += word_level_tokenizer.index_word.get(tf.argmax(word, axis=0).numpy()) + ' '\n",
    "    print(out)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c63689e16ef959dd",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
