{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T16:02:29.892552Z",
     "start_time": "2024-04-23T16:02:27.826557Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow import keras\n",
    "import random\n",
    "import os"
   ],
   "id": "2876b881a660e34e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-23 23:02:28.310856: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-23 23:02:28.364033: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-23 23:02:29.314933: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T16:02:29.896720Z",
     "start_time": "2024-04-23T16:02:29.893876Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def write_dict_data_to_file(file_path, data, indent=2):\n",
    "    with open(file_path, \"w\", encoding='utf-8') as outfile:\n",
    "        json.dump(data, outfile, ensure_ascii=False, indent=indent)"
   ],
   "id": "ce90c9cd2473998f",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T16:02:29.912161Z",
     "start_time": "2024-04-23T16:02:29.897840Z"
    }
   },
   "cell_type": "code",
   "source": [
    "os.makedirs('model', exist_ok=True)\n",
    "os.makedirs('model/tokenizer', exist_ok=True)"
   ],
   "id": "a1cd467041c92bc6",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Set up config.",
   "id": "462a75eb54d159c3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T16:02:29.922923Z",
     "start_time": "2024-04-23T16:02:29.913401Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.BATCH_SIZE = 100\n",
    "        self.EPOCHS = 10\n",
    "        self.NUM_OF_INPUTS = 5000\n",
    "\n",
    "        self.NUM_CHARACTER_LEVEL_LAYERS = 4\n",
    "        self.NUM_WORD_LEVEL_LAYERS = 12\n",
    "\n",
    "        self.CHARACTER_LEVEL_D_MODEL = 64\n",
    "        self.WORD_LEVEL_D_MODEL = 512\n",
    "\n",
    "        self.NUM_HEADS = 3\n",
    "        self.DFF = 256\n",
    "\n",
    "        self.MAX_WORD_LENGTH = 8\n",
    "        self.MAX_SENTENCE_LENGTH = 64\n",
    "\n",
    "        self.VOCAB_SIZE = 8000\n",
    "        self.CHARACTER_VOCAB_SIZE = 500\n",
    "\n",
    "    def save_config(self):\n",
    "        write_dict_data_to_file(\"model/config.json\", {\n",
    "            \"BATCH_SIZE\": self.BATCH_SIZE,\n",
    "            \"EPOCHS\": self.EPOCHS,\n",
    "            \"NUM_OF_INPUTS\": self.NUM_OF_INPUTS,\n",
    "\n",
    "            \"NUM_CHARACTER_LEVEL_LAYERS\": self.NUM_CHARACTER_LEVEL_LAYERS,\n",
    "            \"NUM_WORD_LEVEL_LAYERS\": self.NUM_WORD_LEVEL_LAYERS,\n",
    "\n",
    "            \"WORD_LEVEL_D_MODEL\": self.WORD_LEVEL_D_MODEL,\n",
    "            \"CHARACTER_LEVEL_D_MODEL\": self.CHARACTER_LEVEL_D_MODEL,\n",
    "\n",
    "            \"NUM_HEADS\": self.NUM_HEADS,\n",
    "            \"DFF\": self.DFF,\n",
    "\n",
    "            \"MAX_WORD_LENGTH\": self.MAX_WORD_LENGTH,\n",
    "            \"MAX_SENTENCE_LENGTH\": self.MAX_SENTENCE_LENGTH,\n",
    "\n",
    "            \"VOCAB_SIZE\": self.VOCAB_SIZE,\n",
    "            \"CHARACTER_VOCAB_SIZE\": self.CHARACTER_VOCAB_SIZE,\n",
    "        }, indent=4)\n",
    "        return self\n",
    "\n",
    "    @staticmethod\n",
    "    def load_config():\n",
    "        instance = Config()\n",
    "        config = json.load(open('model/config.json'))\n",
    "\n",
    "        instance.BATCH_SIZE = config[\"BATCH_SIZE\"]\n",
    "        instance.EPOCHS = config[\"EPOCHS\"]\n",
    "\n",
    "        instance.NUM_OF_INPUTS = config[\"NUM_OF_INPUTS\"]\n",
    "        instance.NUM_CHARACTER_LEVEL_LAYERS = config[\"NUM_CHARACTER_LEVEL_LAYERS\"]\n",
    "        instance.NUM_WORD_LEVEL_LAYERS = config[\"NUM_WORD_LEVEL_LAYERS\"]\n",
    "\n",
    "        instance.WORD_LEVEL_D_MODEL = config[\"WORD_LEVEL_D_MODEL\"]\n",
    "        instance.CHARACTER_LEVEL_D_MODEL = config[\"CHARACTER_LEVEL_D_MODEL\"]\n",
    "\n",
    "        instance.NUM_HEADS = config[\"NUM_HEADS\"]\n",
    "        instance.DFF = config[\"DFF\"]\n",
    "\n",
    "        instance.MAX_WORD_LENGTH = config[\"MAX_WORD_LENGTH\"]\n",
    "        instance.MAX_SENTENCE_LENGTH = config[\"MAX_SENTENCE_LENGTH\"]\n",
    "\n",
    "        instance.VOCAB_SIZE = config[\"VOCAB_SIZE\"]\n",
    "        instance.CHARACTER_VOCAB_SIZE = config[\"CHARACTER_VOCAB_SIZE\"]\n",
    "\n",
    "        return instance\n"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Positional Embedding",
   "id": "4f76ade9e9d26bb2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T16:02:29.933963Z",
     "start_time": "2024-04-23T16:02:29.925016Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def positional_encoding(length, depth):\n",
    "    depth = depth / 2\n",
    "\n",
    "    positions = np.arange(length)[:, np.newaxis]\n",
    "    depths = np.arange(depth)[np.newaxis, :] / depth\n",
    "\n",
    "    angle_rates = 1 / (10000 ** depths)\n",
    "    angle_rads = positions * angle_rates\n",
    "\n",
    "    pos_encoding = np.concatenate([np.sin(angle_rads), np.cos(angle_rads)], axis=-1)\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "\n",
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, d_model, positional=True):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True)\n",
    "        self.positional = positional\n",
    "        if positional is True:\n",
    "            self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n",
    "\n",
    "    def compute_mask(self, *args, **kwargs):\n",
    "        return self.embedding.compute_mask(*args, **kwargs)\n",
    "\n",
    "    def call(self, x):\n",
    "        length = tf.shape(x)[1]\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        if self.positional is True:\n",
    "            x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
    "        return x"
   ],
   "id": "c4664e5c71ea18ee",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Attention",
   "id": "c2cd451d53f195c6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T16:02:29.942315Z",
     "start_time": "2024-04-23T16:02:29.935204Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "        self.add = tf.keras.layers.Add()"
   ],
   "id": "9a32a87051ac1f0",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T16:02:29.950255Z",
     "start_time": "2024-04-23T16:02:29.943457Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GlobalSelfAttention(BaseAttention):\n",
    "    def call(self, inputs):\n",
    "        x, mask = inputs[0], inputs[1]\n",
    "\n",
    "        attn_output = self.mha(query=x, value=x, key=x, attention_mask=mask)\n",
    "\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "        return x"
   ],
   "id": "48f2e980470d763d",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# FeedForward and Encoders",
   "id": "60bfcfbe0426dd05"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T16:02:29.960532Z",
     "start_time": "2024-04-23T16:02:29.951384Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, dff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.seq = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(dff, activation='relu'),\n",
    "            tf.keras.layers.Dense(d_model),\n",
    "            tf.keras.layers.Dropout(dropout_rate)\n",
    "        ])\n",
    "        self.add = tf.keras.layers.Add()\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.add([x, self.seq(x)])\n",
    "        x = self.layer_norm(x)\n",
    "        return x"
   ],
   "id": "733c8d76e6b604f7",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T16:02:29.968339Z",
     "start_time": "2024-04-23T16:02:29.961755Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, *, d_model, num_heads, dff, dropout_rate=0.1, name='EncoderLayer'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attention = GlobalSelfAttention(\n",
    "            num_heads=num_heads, key_dim=d_model, dropout=dropout_rate, name=name\n",
    "        )\n",
    "\n",
    "        self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.self_attention(inputs)\n",
    "        x = self.ffn(x)\n",
    "        return x"
   ],
   "id": "2104ad729dcf14c4",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T16:02:29.982399Z",
     "start_time": "2024-04-23T16:02:29.969506Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "            self, *, num_layers, d_model, num_heads, dff, vocab_size, dropout_rate=0.1,\n",
    "            name='Encoder'\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        self.enc_layers = [\n",
    "            EncoderLayer(\n",
    "                d_model=d_model, num_heads=num_heads, dff=dff,\n",
    "                dropout_rate=dropout_rate,\n",
    "                name=f'encoder_layer_{i + 1}'\n",
    "            )\n",
    "            for i in range(num_layers)\n",
    "        ]\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, mask = inputs\n",
    "        x = self.dropout(x)\n",
    "        for i in range(self.num_layers):\n",
    "            inputs = self.enc_layers[i]((x, mask))\n",
    "        return inputs  # Shape (batch_size, seq_len, d_model)."
   ],
   "id": "54ba33d2bca42eaf",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Construct Model.",
   "id": "f51f3d69c5f2f683"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T16:02:30.000229Z",
     "start_time": "2024-04-23T16:02:29.983663Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@keras.utils.register_keras_serializable()\n",
    "class HierarchicalTransformerEncoderModel(tf.keras.models.Model):\n",
    "    def __init__(self, *, num_character_level_layers, num_word_level_layers,\n",
    "                 character_level_d_model, word_level_d_model, num_heads, dff,\n",
    "                 max_word_length, max_sentence_length,\n",
    "                 vocab_size, character_vocab_size, dropout_rate=0.1, **kwargs):\n",
    "        super(HierarchicalTransformerEncoderModel, self).__init__(**kwargs)\n",
    "\n",
    "        self.num_character_level_layers = num_character_level_layers\n",
    "        self.num_word_level_layers = num_word_level_layers\n",
    "        self.character_level_d_model = character_level_d_model\n",
    "        self.word_level_d_model = word_level_d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.dff = dff\n",
    "        self.max_word_length = max_word_length\n",
    "        self.max_sentence_length = max_sentence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.character_vocab_size = character_vocab_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.word_pos_embedding = PositionalEmbedding(vocab_size=vocab_size,\n",
    "                                                      d_model=word_level_d_model)\n",
    "        self.character_pos_embedding = PositionalEmbedding(vocab_size=character_vocab_size,\n",
    "                                                           d_model=character_level_d_model,\n",
    "                                                           positional=False)\n",
    "\n",
    "        self.character_level_encoder = Encoder(num_layers=num_character_level_layers,\n",
    "                                               d_model=character_level_d_model,\n",
    "                                               num_heads=num_heads, dff=dff,\n",
    "                                               vocab_size=character_vocab_size,\n",
    "                                               dropout_rate=dropout_rate,\n",
    "                                               name='character_level_encoder')\n",
    "\n",
    "        self.flatten_layer = tf.keras.layers.Flatten(input_shape=(max_word_length, character_level_d_model),\n",
    "                                                     name='flatten_character_level_encoders')\n",
    "        self.linear_layer = tf.keras.layers.Dense(units=word_level_d_model, activation=None,\n",
    "                                                  name='linear_character_level_encoders')\n",
    "\n",
    "        self.combined_layer = tf.keras.layers.Concatenate(axis=-1, name='combined_character_and_word_level_encoders')\n",
    "\n",
    "        self.word_level_encoder = Encoder(num_layers=num_word_level_layers,\n",
    "                                          d_model=(word_level_d_model * 2),\n",
    "                                          num_heads=num_heads, dff=dff,\n",
    "                                          vocab_size=vocab_size,\n",
    "                                          dropout_rate=dropout_rate,\n",
    "                                          name='word_level_encoder')\n",
    "\n",
    "        self.correction_layer = tf.keras.layers.Dense(vocab_size, activation='softmax', name='correction_layer', )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # print(\"Start\")\n",
    "        word_level_inputs, character_level_inputs = inputs\n",
    "\n",
    "        word_embedding_outputs = self.word_pos_embedding(word_level_inputs)  # (batch_size, max_len, word_level_d_model)\n",
    "        word_level_mask = self.word_pos_embedding.compute_mask(word_level_inputs)\n",
    "        word_level_mask = word_level_mask[:, tf.newaxis, :]\n",
    "\n",
    "        character_level_encoder_outputs = tf.map_fn(\n",
    "            lambda sentence: self.character_level_encoder(\n",
    "                (self.character_pos_embedding(sentence),\n",
    "                 self.character_pos_embedding.compute_mask(sentence)[:, tf.newaxis, :])),\n",
    "            character_level_inputs,\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "        # (batch_size, max_sen_len, max_word_length, character_level_d_model)\n",
    "\n",
    "        character_level_encoder_outputs = tf.map_fn(\n",
    "            lambda sentence: tf.map_fn(\n",
    "                lambda word: tf.squeeze(self.linear_layer(self.flatten_layer(tf.expand_dims(word, axis=0))), axis=0),\n",
    "                sentence,\n",
    "                dtype=tf.float32\n",
    "            ),\n",
    "            character_level_encoder_outputs,\n",
    "            dtype=tf.float32)\n",
    "        # (batch_size, max_sen_len, word_level_d_model)\n",
    "        # print(\"Shape của character_level_encoder_outputs:\", character_level_encoder_outputs.shape)\n",
    "\n",
    "        concat_output = self.combined_layer([word_embedding_outputs, character_level_encoder_outputs])\n",
    "        # (batch_size, max_sen_len, word_level_d_model * 2)\n",
    "        # print(\"Shape của concat_output:\", concat_output.shape)\n",
    "\n",
    "        word_level_output = self.word_level_encoder((concat_output, word_level_mask))\n",
    "        # (batch_size, max_sen_len, word_level_d_model + character_level_d_model)\n",
    "        # print(\"Shape của word_level_output:\", word_level_output.shape)\n",
    "\n",
    "        correction_output = self.correction_layer(word_level_output)  # (batch_size, max_sen_len, vocab_size)\n",
    "        # print(\"Shape của correction_output:\", correction_output.shape)\n",
    "\n",
    "        # detection_output = tf.squeeze(self.detection_layer(word_level_output), axis=-1)  # (batch_size, max_sen_len)\n",
    "        # print(\"Shape của detection_output:\", detection_output.shape)\n",
    "        # print(\"End\")\n",
    "        return correction_output\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'num_character_level_layers': self.num_character_level_layers,\n",
    "            'num_word_level_layers': self.num_word_level_layers,\n",
    "            'character_level_d_model': self.character_level_d_model,\n",
    "            'word_level_d_model': self.word_level_d_model,\n",
    "            'num_heads': self.num_heads,\n",
    "            'dff': self.dff,\n",
    "            'max_word_length': self.max_word_length,\n",
    "            'max_sentence_length': self.max_sentence_length,\n",
    "            'vocab_size': self.vocab_size,\n",
    "            'character_vocab_size': self.character_vocab_size,\n",
    "            'dropout_rate': self.dropout_rate\n",
    "        }\n",
    "        base_config = super(HierarchicalTransformerEncoderModel, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def training_step(model, optimizer, x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        correction_output = model(x)\n",
    "        loss1 = correction_loss(y, correction_output)\n",
    "        # loss2 = detection_loss(y[1], detection_output)\n",
    "        total_loss = loss1\n",
    "\n",
    "    gradients = tape.gradient(total_loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "def correction_loss(true_outputs, pred_outputs):\n",
    "    softmax_loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)(true_outputs, pred_outputs)\n",
    "    return softmax_loss\n",
    "\n",
    "\n",
    "def detection_loss(true_detection_infos, pred_detection_infos):\n",
    "    sigmoid_loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)(true_detection_infos, pred_detection_infos)\n",
    "    return sigmoid_loss"
   ],
   "id": "7db1856a68e9401b",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Dataset control",
   "id": "4e7a2e6dc01a4e22"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T16:02:30.011221Z",
     "start_time": "2024-04-23T16:02:30.001570Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Dataset:\n",
    "    def __init__(self, sentences, tokenizer, config):\n",
    "        self.sentences = sentences\n",
    "        self.tokenizer = tokenizer\n",
    "        self.config = config\n",
    "\n",
    "    def build_dataset(self):\n",
    "        (word_level_sequences, character_level_sequences) = self.tokenizer.texts_to_sequences(self.sentences)\n",
    "\n",
    "        target_word_level_sequences = pad_sequences(word_level_sequences.copy(), maxlen=self.config.MAX_SENTENCE_LENGTH,\n",
    "                                                    padding='post', truncating='post')\n",
    "\n",
    "        tmp_word_level_sequences = []\n",
    "        for sequence in word_level_sequences:\n",
    "            sequence = self.random_word(sequence)\n",
    "            tmp_word_level_sequences.append(sequence)\n",
    "\n",
    "        # Get word_level sequences\n",
    "        word_level_sequences = tmp_word_level_sequences\n",
    "        word_level_sequences = pad_sequences(word_level_sequences, maxlen=self.config.MAX_SENTENCE_LENGTH,\n",
    "                                             padding='post', truncating='post')\n",
    "\n",
    "        # Get character_level mask matrix.\n",
    "        tmp_character_level_sequences = []\n",
    "        for character_level_sequence in character_level_sequences:\n",
    "            sentence_tmp = []\n",
    "            # With each sentence\n",
    "            for word in character_level_sequence:\n",
    "                # With each word\n",
    "                character_level_sequence = self.random_word(word, word_level=False)\n",
    "                sentence_tmp.append(character_level_sequence)\n",
    "\n",
    "            while len(sentence_tmp) < self.config.MAX_SENTENCE_LENGTH:\n",
    "                sentence_tmp.append([])\n",
    "            while len(sentence_tmp) > self.config.MAX_SENTENCE_LENGTH:\n",
    "                del sentence_tmp[-1]\n",
    "            tmp_character_level_sequences.append(\n",
    "                pad_sequences(sentence_tmp, maxlen=self.config.MAX_WORD_LENGTH,\n",
    "                              padding='post', truncating='post'))\n",
    "\n",
    "        character_level_sequences = tf.convert_to_tensor(tmp_character_level_sequences)\n",
    "\n",
    "        return (word_level_sequences, character_level_sequences), target_word_level_sequences\n",
    "\n",
    "    def random_word(self, sequence, word_level=True):\n",
    "        output = []\n",
    "\n",
    "        # 15% of the tokens would be replaced\n",
    "        for token in sequence:\n",
    "            prob = random.random()\n",
    "\n",
    "            if prob < 0.15:\n",
    "                prob /= 0.15\n",
    "\n",
    "                # 80% chance change token to mask token\n",
    "                if prob < 0.8:\n",
    "                    output.append(0)\n",
    "\n",
    "                # 10% chance change token to random token\n",
    "                elif prob < 0.9:\n",
    "                    if word_level:\n",
    "                        random_token = random.randrange(len(self.tokenizer.word_index) + 1)\n",
    "                        if random_token > self.tokenizer.word_vocab_size:\n",
    "                            random_token = 0\n",
    "                    else:\n",
    "                        random_token = random.randrange(len(self.tokenizer.character_index) + 1)\n",
    "\n",
    "                    output.append(random_token)\n",
    "\n",
    "                # 10% chance change token to current token\n",
    "                else:\n",
    "                    output.append(token)\n",
    "\n",
    "            else:\n",
    "                output.append(token)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def build_test_data(self, sentences):\n",
    "        word_level_sequences, character_level_sequences = self.tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "        word_level_sequences = pad_sequences(word_level_sequences, maxlen=self.config.MAX_SENTENCE_LENGTH,\n",
    "                                             padding='post', truncating='post')\n",
    "\n",
    "        tmp_character_level_sequences = []\n",
    "        for character_level_sequence in character_level_sequences:\n",
    "            while len(character_level_sequence) < self.config.MAX_SENTENCE_LENGTH:\n",
    "                character_level_sequence.append([])\n",
    "            while len(character_level_sequence) > self.config.MAX_SENTENCE_LENGTH:\n",
    "                del character_level_sequence[-1]\n",
    "            tmp_character_level_sequences.append(\n",
    "                pad_sequences(character_level_sequence, maxlen=self.config.MAX_WORD_LENGTH,\n",
    "                              padding='post', truncating='post'))\n",
    "\n",
    "        character_level_sequences = tf.convert_to_tensor(tmp_character_level_sequences)\n",
    "        return word_level_sequences, character_level_sequences\n"
   ],
   "id": "77425712ac3d8ae4",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Custom tokenizer.",
   "id": "268451f9cc9b1e75"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T16:02:30.024092Z",
     "start_time": "2024-04-23T16:02:30.012357Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CustomTokenizer:\n",
    "\n",
    "    def __init__(self, word_vocab_size=0, max_word_len=0, max_sentence_len=0):\n",
    "        self.index_word = {1: '[UNK]'}\n",
    "        self.word_count = {'[UNK]': 1}\n",
    "        self.word_index = {'[UNK]': 1}\n",
    "        self.word_vocab_size = word_vocab_size\n",
    "\n",
    "        self.character_index = {'[UNK]': 1}\n",
    "        self.index_character = {1: '[UNK]'}\n",
    "\n",
    "        self.signs = [',', '.', '!', '\\\"', \"\\'\", \"?\", \";\", \")\", \"(\", ':', \"/\", \"+\", \"-\", \"=\", \"`\", \"~\", \"*\", \"^\", \"@\",\n",
    "                      \"%\", \"&\", \"_\"]\n",
    "\n",
    "        self.max_word_len = max_word_len\n",
    "        self.max_sentence_len = max_sentence_len\n",
    "\n",
    "    def preprocess_texts(self, texts):\n",
    "        texts_preprocessed = []\n",
    "        for text in texts:\n",
    "            for sign in self.signs:\n",
    "                text = text.replace(sign, ' ')\n",
    "\n",
    "            text = text.lower()\n",
    "            texts_preprocessed.append(text)\n",
    "        return texts_preprocessed\n",
    "\n",
    "    def sort(self):\n",
    "        self.word_count = sorted(list(self.word_count.items())[1:], key=lambda x: x[1], reverse=True)\n",
    "        for i, (word, count) in enumerate(self.word_count):\n",
    "            self.index_word[i + 2] = word\n",
    "            self.word_index[word] = i + 2\n",
    "\n",
    "        self.word_count = dict(self.word_count)\n",
    "\n",
    "    def texts_to_sequences(self, texts):\n",
    "        word_level_sequences = []\n",
    "        character_level_sequences = []\n",
    "\n",
    "        texts = self.preprocess_texts(texts)\n",
    "        for text in texts:\n",
    "            words = text.split(' ')\n",
    "            words = [i for i in words if i != '']\n",
    "\n",
    "            tokens = [self.word_index.get(word, 1)\n",
    "                      if (self.word_index.get(word, 1) <= self.word_vocab_size) else 1\n",
    "                      for word in words]\n",
    "            word_level_sequences.append(tokens)\n",
    "\n",
    "            sentence_tokens = []\n",
    "            for word in words:\n",
    "                tokens = [self.character_index.get(i, 1) for i in word]\n",
    "                sentence_tokens.append(tokens)\n",
    "            character_level_sequences.append(sentence_tokens)\n",
    "\n",
    "        return word_level_sequences, character_level_sequences\n",
    "\n",
    "    def sequences_to_texts(self, sequences):\n",
    "        texts = []\n",
    "        for sequence in sequences:\n",
    "            texts.append([self.index_word.get(token, 1) for token in sequence])\n",
    "        return texts\n",
    "\n",
    "    def fit_on_texts(self, texts):\n",
    "\n",
    "        texts = self.preprocess_texts(texts)\n",
    "\n",
    "        for text in texts:\n",
    "\n",
    "            # Update word vocab.\n",
    "            words = text.split(' ')\n",
    "            for word in words:\n",
    "                if word == '': continue\n",
    "                self.word_count[word] = self.word_count.get(word, 0) + 1\n",
    "\n",
    "            # Update character vocab\n",
    "            for character in text:\n",
    "                if character == ' ': continue\n",
    "                if self.character_index.get(character, -1) == -1:\n",
    "                    self.character_index[character] = len(self.character_index) + 1\n",
    "\n",
    "                self.index_character[self.character_index[character]] = character\n",
    "\n",
    "        self.sort()\n",
    "        self.save_tokenizer()\n",
    "\n",
    "    def save_tokenizer(self):\n",
    "        write_dict_data_to_file(\"model/tokenizer/word_index.json\", self.word_index)\n",
    "        write_dict_data_to_file(\"model/tokenizer/index_word.json\", self.index_word)\n",
    "        write_dict_data_to_file(\"model/tokenizer/index_character.json\", self.index_character)\n",
    "        write_dict_data_to_file(\"model/tokenizer/character_index.json\", self.character_index)\n",
    "        write_dict_data_to_file(\"model/tokenizer/config.json\", {\n",
    "            'word_vocab_size': self.word_vocab_size,\n",
    "            'max_word_len': self.max_word_len,\n",
    "            'max_sentence_len': self.max_sentence_len,\n",
    "            'signs': self.signs\n",
    "        }, indent=4)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_tokenizer():\n",
    "        instance = CustomTokenizer()\n",
    "\n",
    "        config = json.load(open('model/tokenizer/config.json'))\n",
    "        instance.word_vocab_size = config['word_vocab_size']\n",
    "        instance.max_word_len = config['max_word_len']\n",
    "        instance.max_sentence_len = config['max_sentence_len']\n",
    "        instance.signs = config['signs']\n",
    "\n",
    "        instance.word_index = json.load(open('model/tokenizer/word_index.json'))\n",
    "        instance.index_word = json.load(open('model/tokenizer/index_word.json'))\n",
    "        instance.character_index = json.load(open('model/tokenizer/character_index.json'))\n",
    "        instance.index_character = json.load(open('model/tokenizer/index_character.json'))\n",
    "\n",
    "        return instance"
   ],
   "id": "8bcfacc8999a783a",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Custom Schedule",
   "id": "d13bc13dcf8224a7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T16:02:30.035189Z",
     "start_time": "2024-04-23T16:02:30.027503Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@keras.utils.register_keras_serializable()\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, dtype=tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            'd_model': float(self.d_model),\n",
    "            'warmup_steps': self.warmup_steps\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)"
   ],
   "id": "8a409751ef4a49c1",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Init config and build dataset.",
   "id": "12d0b44d4bf74ca6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T16:02:46.338747Z",
     "start_time": "2024-04-23T16:02:30.036142Z"
    }
   },
   "cell_type": "code",
   "source": [
    "config = Config().save_config()\n",
    "\n",
    "# Read data.\n",
    "data = pd.read_csv('data/vi_processed.csv')\n",
    "input_sentences = []\n",
    "for index, row in data.iterrows():\n",
    "    input_sentences.append(row.correct_text)"
   ],
   "id": "745391402f8f3fb8",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Preprocessing and build dataset.",
   "id": "5b533bc279255e2b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T16:02:58.181994Z",
     "start_time": "2024-04-23T16:02:46.339918Z"
    }
   },
   "cell_type": "code",
   "source": [
    "word_level_tokenizer = CustomTokenizer(word_vocab_size=config.VOCAB_SIZE, max_word_len=config.MAX_WORD_LENGTH,\n",
    "                                       max_sentence_len=config.MAX_SENTENCE_LENGTH)\n",
    "word_level_tokenizer.fit_on_texts(input_sentences)\n",
    "\n",
    "input_sentences = input_sentences[:config.NUM_OF_INPUTS]\n",
    "dataset = Dataset(input_sentences, word_level_tokenizer, config)\n",
    "data = dataset.build_dataset()\n",
    "\n",
    "(input_word_level_sequences, input_character_level_sequences), target_sequences = data"
   ],
   "id": "78bb7fc658fa92aa",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Build and train model.",
   "id": "d9d3e870cb18c93c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T16:02:58.405252Z",
     "start_time": "2024-04-23T16:02:58.182936Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = HierarchicalTransformerEncoderModel(num_character_level_layers=config.NUM_CHARACTER_LEVEL_LAYERS,\n",
    "                                            num_word_level_layers=config.NUM_WORD_LEVEL_LAYERS,\n",
    "                                            character_level_d_model=config.CHARACTER_LEVEL_D_MODEL,\n",
    "                                            word_level_d_model=config.WORD_LEVEL_D_MODEL,\n",
    "                                            num_heads=config.NUM_HEADS, dff=config.DFF,\n",
    "                                            max_word_length=config.MAX_WORD_LENGTH,\n",
    "                                            max_sentence_length=config.MAX_SENTENCE_LENGTH,\n",
    "                                            vocab_size=config.VOCAB_SIZE,\n",
    "                                            character_vocab_size=config.CHARACTER_VOCAB_SIZE)\n",
    "\n",
    "learning_rate = CustomSchedule(config.WORD_LEVEL_D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
    "                                     epsilon=1e-9)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=['acc'])"
   ],
   "id": "b6a0cb80ed6a420c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thanhan/.local/lib/python3.10/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T16:04:20.270909Z",
     "start_time": "2024-04-23T16:02:58.406828Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_size = int(config.NUM_OF_INPUTS * 0.8)\n",
    "model.fit(\n",
    "    [input_word_level_sequences[:train_size], input_character_level_sequences[:train_size]],\n",
    "    target_sequences[:train_size], epochs=config.EPOCHS,\n",
    "    batch_size=config.BATCH_SIZE,\n",
    "    validation_data=([input_word_level_sequences[train_size:], input_character_level_sequences[train_size:]],\n",
    "                     target_sequences[train_size:]))\n",
    "\n",
    "model.save('model/model.keras')"
   ],
   "id": "d8313d6b8226697c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "WARNING:tensorflow:From /home/thanhan/.local/lib/python3.10/site-packages/tensorflow/python/util/deprecation.py:660: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thanhan/.local/lib/python3.10/site-packages/keras/src/layers/layer.py:856: UserWarning: Layer 'query' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/thanhan/.local/lib/python3.10/site-packages/keras/src/layers/layer.py:856: UserWarning: Layer 'key' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/thanhan/.local/lib/python3.10/site-packages/keras/src/layers/layer.py:856: UserWarning: Layer 'value' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/thanhan/.local/lib/python3.10/site-packages/keras/src/optimizers/base_optimizer.py:583: UserWarning: Gradients do not exist for variables ['kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m2/2\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m31s\u001B[0m 4s/step - acc: 0.0000e+00 - loss: 9.1180 - val_acc: 0.0000e+00 - val_loss: 9.0821\n",
      "Epoch 2/7\n",
      "\u001B[1m2/2\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 2s/step - acc: 0.0000e+00 - loss: 9.1100 - val_acc: 0.0000e+00 - val_loss: 9.0697\n",
      "Epoch 3/7\n",
      "\u001B[1m2/2\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 2s/step - acc: 0.0000e+00 - loss: 9.0964 - val_acc: 0.0000e+00 - val_loss: 9.0475\n",
      "Epoch 4/7\n",
      "\u001B[1m2/2\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 3s/step - acc: 0.0000e+00 - loss: 9.0792 - val_acc: 0.0000e+00 - val_loss: 9.0156\n",
      "Epoch 5/7\n",
      "\u001B[1m2/2\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 2s/step - acc: 0.0000e+00 - loss: 9.0447 - val_acc: 0.0000e+00 - val_loss: 8.9738\n",
      "Epoch 6/7\n",
      "\u001B[1m2/2\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 2s/step - acc: 0.0000e+00 - loss: 9.0082 - val_acc: 0.0000e+00 - val_loss: 8.9223\n",
      "Epoch 7/7\n",
      "\u001B[1m2/2\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 2s/step - acc: 0.0000e+00 - loss: 8.9546 - val_acc: 0.0000e+00 - val_loss: 8.8610\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T16:04:21.915311Z",
     "start_time": "2024-04-23T16:04:20.279626Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_outputs = model.predict(\n",
    "    [input_word_level_sequences[train_size:], input_character_level_sequences[train_size:]])"
   ],
   "id": "ef80e7b05b5df974",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 2s/step\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T16:04:21.955066Z",
     "start_time": "2024-04-23T16:04:21.916948Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def print_outputs(outputs):\n",
    "    for sentence in outputs:\n",
    "        out = ''\n",
    "        for word in sentence:\n",
    "            max_index = tf.argmax(word, axis=0).numpy()\n",
    "            word_str = word_level_tokenizer.index_word.get(max_index)\n",
    "            if word_str is not None:\n",
    "                out += word_str + ' '\n",
    "            else:\n",
    "                out += '<UNK> '\n",
    "        print(out)\n",
    "\n",
    "\n",
    "print_outputs(test_outputs[:10])"
   ],
   "id": "24cd2ff7f26dc537",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alô mòn thấm thoát xua roi nhúng thấm thấm thấm 1989 hai 1m lôi lẽo linus hênh 146 khắp khắp đao đao đao đao đao dậy rehhagel đao đao đao 146 146 146 đao đao đao 146 146 đao đao đao đạt đạt tuân tuân tuân tuân tuân tuân rehhagel rehhagel rehhagel đạt đạt đạt khẳng khẳng thắm thắm thắm javier javier javier javier \n",
      "g 146 channel khiêm thấm đao torino javier thấm thinh lẽo hàn blood quẩn đao fi vé tobacco hênh tobacco nỗ đao caneira năng 146 caneira đao caneira đao đao dương mượn ăng đao spears dương aac đao coca đao đao đao dyer robot boot đàm linus đao reader đao ms chợ đao thấm cellulite chứ news kaka 190 javier sứ thắm javier đao \n",
      "blood đao thấm miết thấm kênh tuân thấm đao jeonbuk đàm jeonbuk quy javier coca javier 146 146 khắp khắp đao đao đao đao đao javier javier đao đao đao 146 146 146 đao đao đao 146 146 đao đao đạt đạt đạt tuân tuân tuân tuân tuân tuân rehhagel rehhagel rehhagel đạt đạt đạt khẳng khẳng thắm thắm thắm javier javier javier javier \n",
      "mòn nhỡ đàm dominik sít beige zambrotta caneira năng 190 190 mathieu rapper vitamin vnexpress nghệ bố javier nhẽo thấm puerto tobacco mụn khi torino đao trạc đao vitamin espn chợ dương chèo dương ăng spears spears trụi đao blood blood blood dubai năng thấm nm javier bắp tuân tuân rehhagel rehhagel đạt đạt đạt khẳng khẳng thắm thắm thắm javier javier javier javier \n",
      "đao nedved ăng thấm yevgen đao đao đao xua channel 1m blood javier javier javier javier khắp khắp khắp khắp dậy đao đao đao đao javier javier đao đao đao 146 146 146 đao đao đao 146 146 đao đao đạt đạt đạt tuân tuân tuân tuân tuân tuân rehhagel rehhagel rehhagel đạt đạt đạt khẳng khẳng thắm thắm thắm javier javier javier javier \n",
      "veo javier chợ javier thấm cellulite coi caneira treviso yichen johansson 190 trend ăng sạt javier dyer acrobat vết tobacco năng hai caneira đao đao trạc rehhagel bỏng roi uae tắt đao nỗ hấu chứ lôi thấy thấy trạc spalletti xuề 2 blog trạc mms coca thấy đàm tuyền blood tuân hầm thấm caneira đạt khẳng khẳng thắm thắm thắm javier javier javier javier \n",
      "disneyland tab program linus linus đao program hơp caneira tuân torino caneira lille vì đao lốt euro smart tắt 190 trạc psg đao đao coca evans coca đao thấm đao dyer spears news thấm thấm coca phàn 146 đao đao đao đạt đạt tuân tuân tuân tuân tuân tuân rehhagel rehhagel rehhagel đạt đạt đạt khẳng khẳng thắm thắm thắm javier javier javier javier \n",
      "hênh downing caneira hướng moon channel andrade torino đao đàm torino sứ hênh dương nỗ dương dai khắp khắp khắp đao đao đao đao đao javier javier đao đao đao đao 146 đao đao đao đao 146 146 đao đao đao đạt đạt tuân tuân tuân tuân tuân tuân rehhagel rehhagel rehhagel đạt đạt đạt khẳng khẳng thắm thắm thắm javier javier javier javier \n",
      "truỵ lôi mathieu đao torino thấm thấm tobacco tô cellulite tuân ăng nguời chỉn đàm 330 business 190 ngần sứ dương thấm đao đao rapper hamburg đao trạc 146 đao 146 năng đao đao đao đao 146 146 đao đao đao đạt đạt tuân tuân tuân tuân tuân tuân rehhagel rehhagel rehhagel đạt đạt đạt khẳng khẳng thắm thắm thắm javier javier javier javier \n",
      "vết trám lôi đao đao chỉn thấy xua rapper yichen giàng javier ăng ăng hênh caneira ha g channel hênh đao lôi đao đao đao javier javier đao đao đao 146 146 146 đao đao đao 146 146 đao đao đao đạt đạt tuân tuân tuân tuân tuân tuân rehhagel rehhagel rehhagel đạt đạt đạt khẳng khẳng thắm thắm thắm javier javier javier javier \n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Test loaded model.",
   "id": "460070dd1125979c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T16:06:15.327011Z",
     "start_time": "2024-04-23T16:04:21.956179Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = CustomTokenizer.load_tokenizer()\n",
    "loaded_model = tf.keras.models.load_model('model/model.keras')\n",
    "config = Config.load_config()\n",
    "dataset = Dataset(None, tokenizer, config)"
   ],
   "id": "92ce500a205e2016",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T16:06:15.330679Z",
     "start_time": "2024-04-23T16:06:15.328103Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def print_outputs(outputs):\n",
    "    for sentence in outputs:\n",
    "        out = ''\n",
    "        for word in sentence:\n",
    "            max_index = tf.argmax(word, axis=0).numpy()\n",
    "            word_str = tokenizer.index_word.get(str(max_index))\n",
    "            if word_str is not None:\n",
    "                out += word_str + ' '\n",
    "            else:\n",
    "                out += '<UNK> '\n",
    "        print(out)"
   ],
   "id": "a7101e5d7c7cb92d",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T16:06:16.427309Z",
     "start_time": "2024-04-23T16:06:15.331546Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_sentences = [\n",
    "    'Nếu làm được như vậy thì chắc chắn xẽ không còn trường nào tùy tiện thu tiền cao, gây sự lo lắng củb phụ huynh và ai không có tiền thì hhông cần đóng.']\n",
    "test_inputs = dataset.build_test_data(test_sentences)\n",
    "test_outputs = loaded_model.predict(test_inputs)\n",
    "print_outputs(test_outputs)"
   ],
   "id": "d5c0e53187d1abca",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 1s/step\n",
      "tô lẫn vé chỉn đao thấm thấm thấm birmingham đao linus giàng linus đao caneira chài smart nkt coca hai nỗ [UNK] năng uae chỉn đao đao đao linus hành thấm nhđá đao đao đao đao 146 146 146 đao đao đạt đạt tuân tuân tuân tuân tuân tuân rehhagel rehhagel rehhagel đạt đạt đạt khẳng khẳng thắm thắm thắm javier javier javier javier \n"
     ]
    }
   ],
   "execution_count": 23
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
