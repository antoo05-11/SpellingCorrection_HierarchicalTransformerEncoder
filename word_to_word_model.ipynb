{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T17:30:16.150146Z",
     "start_time": "2024-04-06T17:30:11.801183Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from Config import Config\n",
    "\n",
    "# Read data.\n",
    "json_sentences = []\n",
    "\n",
    "with open('data/VSEC.jsonl', 'r') as file:\n",
    "    for line in file:\n",
    "        json_obj = json.loads(line[:-1])\n",
    "        json_sentences.append(json_obj)\n",
    "\n",
    "error_texts = []\n",
    "correct_texts = []\n",
    "correct_infos = []\n",
    "\n",
    "for json_sentence in json_sentences:\n",
    "    error_text = []\n",
    "    correct_text = []\n",
    "    correct_info = []\n",
    "    for word in json_sentence['annotations']:\n",
    "        correct_info.append(word['is_correct'])\n",
    "\n",
    "        current_word = word['current_syllable'].lower()\n",
    "        error_text.append(current_word)\n",
    "\n",
    "        if word['is_correct'] is True:\n",
    "            correct_text.append(current_word)\n",
    "        else:\n",
    "            correct_text.append(word['alternative_syllables'][0].lower())\n",
    "            # if len(word['alternative_syllables']) > 1: print(word)\n",
    "    error_texts.append(error_text)\n",
    "    correct_texts.append(correct_text)\n",
    "    correct_infos.append(correct_info)\n",
    "# Main\n",
    "config = Config()\n",
    "\n",
    "word_level_tokenizer = Tokenizer(num_words=config.VOCAB_SIZE, oov_token='<UNK>', lower=True)\n",
    "word_unk_level_tokenizer = Tokenizer(oov_token='<UNK>', lower=True)\n",
    "character_level_tokenizer = Tokenizer(lower=True, char_level=True)\n",
    "\n",
    "word_level_tokenizer.fit_on_texts(correct_texts)\n",
    "word_unk_level_tokenizer.fit_on_texts(error_texts)\n",
    "\n",
    "import itertools\n",
    "\n",
    "flattened_sentences = list(itertools.chain(*(error_texts + correct_texts)))\n",
    "character_level_tokenizer.fit_on_texts(flattened_sentences)"
   ],
   "id": "dc8b24f26c3728a9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-07 00:30:12.044841: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-07 00:30:12.046984: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-07 00:30:12.081948: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-07 00:30:12.082555: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-07 00:30:12.945548: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T17:30:16.157346Z",
     "start_time": "2024-04-06T17:30:16.151551Z"
    }
   },
   "cell_type": "code",
   "source": "len(correct_texts)",
   "id": "22aacf012ec834ce",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9341"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T17:30:17.778788Z",
     "start_time": "2024-04-06T17:30:16.158795Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "input_sequences = word_level_tokenizer.texts_to_sequences(error_texts)\n",
    "output_sequences = word_level_tokenizer.texts_to_sequences(correct_texts)\n",
    "unk_input_sequences = word_unk_level_tokenizer.texts_to_sequences(error_texts)\n",
    "\n",
    "# Get character-level words lengths.\n",
    "input_words_lengths = []\n",
    "\n",
    "# Get character-level sequences.\n",
    "character_level_input_sequences = []\n",
    "\n",
    "for sequence in unk_input_sequences:\n",
    "    character_level_input_sequence = []\n",
    "    words_lengths = []\n",
    "    for word_token in sequence:\n",
    "        word = word_unk_level_tokenizer.index_word[word_token]\n",
    "        word = character_level_tokenizer.texts_to_sequences(word)\n",
    "        word_chars = [each[0] for each in word]\n",
    "        character_level_input_sequence.append(word_chars)\n",
    "        words_lengths.append((len(word_chars) if len(word_chars) <= config.MAX_WORD_LENGTH\n",
    "                              else config.MAX_WORD_LENGTH))\n",
    "\n",
    "    # Add padding for each word.\n",
    "    character_level_input_sequence = pad_sequences(character_level_input_sequence, maxlen=config.MAX_WORD_LENGTH,\n",
    "                                                   padding='post', truncating='post')\n",
    "\n",
    "    character_level_input_sequences.append(character_level_input_sequence)\n",
    "\n",
    "    input_words_lengths.append(words_lengths)\n",
    "\n",
    "# Get word-level sentences lengths.\n",
    "input_sentences_lengths = []\n",
    "for sequence in input_sequences: input_sentences_lengths.append(\n",
    "    (len(sequence) if len(sequence) <= config.MAX_SENTENCE_LENGTH\n",
    "     else config.MAX_SENTENCE_LENGTH))\n",
    "\n",
    "# Add padding for each.\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=config.MAX_SENTENCE_LENGTH, padding='post', truncating='post')\n",
    "output_sequences = pad_sequences(output_sequences, maxlen=config.MAX_SENTENCE_LENGTH, padding='post', truncating='post')\n",
    "character_level_input_sequences = pad_sequences(character_level_input_sequences, maxlen=config.MAX_SENTENCE_LENGTH,\n",
    "                                                padding='post', truncating='post')\n",
    "input_words_lengths = pad_sequences(input_words_lengths, maxlen=config.MAX_SENTENCE_LENGTH, padding='post',\n",
    "                                    truncating='post')\n",
    "correct_infos = pad_sequences(correct_infos, maxlen=config.MAX_SENTENCE_LENGTH, padding='post', value=1)\n",
    "\n",
    "input_sequences_np = np.array(input_sequences)\n",
    "character_level_input_sequences_np = np.array(character_level_input_sequences)\n",
    "output_sequences_np = np.array(output_sequences)\n",
    "\n",
    "input_words_lengths_np = np.array(input_words_lengths)\n",
    "input_sentences_lengths_np = np.array(input_sentences_lengths)\n",
    "correct_infos_np = np.array(correct_infos)"
   ],
   "id": "b04c7f90b2c479e4",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T17:30:17.887370Z",
     "start_time": "2024-04-06T17:30:17.779722Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from HierarchicalTransformerEncoder import HierarchicalTransformerEncoder\n",
    "\n",
    "model = HierarchicalTransformerEncoder(num_character_level_layers=config.NUM_CHARACTER_LEVEL_LAYERS,\n",
    "                                       num_word_level_layers=config.NUM_WORD_LEVEL_LAYERS,\n",
    "                                       character_level_d_model=config.CHARACTER_LEVEL_D_MODEL,\n",
    "                                       word_level_d_model=config.WORD_LEVEL_D_MODEL,\n",
    "                                       num_heads=config.NUM_HEADS, dff=config.DFF,\n",
    "                                       max_word_length=config.MAX_WORD_LENGTH,\n",
    "                                       max_sentence_length=config.MAX_SENTENCE_LENGTH,\n",
    "                                       vocab_size=config.VOCAB_SIZE,\n",
    "                                       character_vocab_size=config.CHARACTER_VOCAB_SIZE)"
   ],
   "id": "82d3104dfc17ed18",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T17:30:17.919924Z",
     "start_time": "2024-04-06T17:30:17.888878Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from CustomSchedule import CustomSchedule\n",
    "\n",
    "learning_rate = CustomSchedule(config.WORD_LEVEL_D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
    "                                     epsilon=1e-9)"
   ],
   "id": "fb68d56c770a68ce",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T17:30:17.922737Z",
     "start_time": "2024-04-06T17:30:17.920834Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# word_input_shape = (config.BATCH_SIZE, config.MAX_SENTENCE_LENGTH)\n",
    "# char_input_shape = (config.BATCH_SIZE, config.MAX_SENTENCE_LENGTH, config.MAX_WORD_LENGTH)\n",
    "# sentence_lengths_shape = (config.BATCH_SIZE, config.MAX_SENTENCE_LENGTH)\n",
    "# word_lengths_shape = (config.BATCH_SIZE, config.MAX_SENTENCE_LENGTH, config.MAX_WORD_LENGTH)\n",
    "\n",
    "# model.build(input_shape=[[word_input_shape, sentence_lengths_shape], [char_input_shape, word_lengths_shape]])\n",
    "# model.compile(optimizer=optimizer, metrics=['acc'])"
   ],
   "id": "2e767574e8c599a7",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-04-06T17:30:17.923575Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from HierarchicalTransformerEncoder import training_step\n",
    "from tqdm import tqdm\n",
    "\n",
    "for e in range(config.EPOCHS):\n",
    "    pbar = tqdm(range(0, len(input_sequences_np), config.BATCH_SIZE))\n",
    "\n",
    "    for i in pbar:\n",
    "        input_batch = [\n",
    "            [input_sequences_np[i:i + config.BATCH_SIZE], input_sentences_lengths_np[i:i + config.BATCH_SIZE]],\n",
    "            [character_level_input_sequences_np[i:i + config.BATCH_SIZE],\n",
    "             input_words_lengths_np[i:i + config.BATCH_SIZE]]\n",
    "        ]\n",
    "        output_batch = [\n",
    "            output_sequences_np[i:i + config.BATCH_SIZE],\n",
    "            correct_infos_np[i:i + config.BATCH_SIZE]\n",
    "        ]\n",
    "\n",
    "        total_loss, loss1, loss2= training_step(model, optimizer, input_batch, output_batch)\n",
    "        \n",
    "        pbar.set_description(f'Epoch {e + 1}/{config.EPOCHS}')\n",
    "        pbar.set_postfix_str(f'loss: {total_loss:.4f}')\n",
    "        pbar.refresh()\n"
   ],
   "id": "3d89bd411edb5399",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/187 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/thanhan/.local/lib/python3.10/site-packages/tensorflow/python/util/deprecation.py:648: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hierarchical_transformer_encoder/encoder_1/encoder_layer_3/global_self_attention_3/encoder_layer_1/query/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_3/global_self_attention_3/encoder_layer_1/query/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_3/global_self_attention_3/encoder_layer_1/key/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_3/global_self_attention_3/encoder_layer_1/key/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_3/global_self_attention_3/encoder_layer_1/value/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_3/global_self_attention_3/encoder_layer_1/value/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_3/global_self_attention_3/encoder_layer_1/attention_output/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_3/global_self_attention_3/encoder_layer_1/attention_output/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_3/global_self_attention_3/layer_normalization_6/gamma:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_3/global_self_attention_3/layer_normalization_6/beta:0', 'dense_6/kernel:0', 'dense_6/bias:0', 'dense_7/kernel:0', 'dense_7/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_3/feed_forward_3/layer_normalization_7/gamma:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_3/feed_forward_3/layer_normalization_7/beta:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_4/global_self_attention_4/encoder_layer_2/query/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_4/global_self_attention_4/encoder_layer_2/query/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_4/global_self_attention_4/encoder_layer_2/key/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_4/global_self_attention_4/encoder_layer_2/key/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_4/global_self_attention_4/encoder_layer_2/value/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_4/global_self_attention_4/encoder_layer_2/value/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_4/global_self_attention_4/encoder_layer_2/attention_output/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_4/global_self_attention_4/encoder_layer_2/attention_output/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_4/global_self_attention_4/layer_normalization_8/gamma:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_4/global_self_attention_4/layer_normalization_8/beta:0', 'dense_8/kernel:0', 'dense_8/bias:0', 'dense_9/kernel:0', 'dense_9/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_4/feed_forward_4/layer_normalization_9/gamma:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_4/feed_forward_4/layer_normalization_9/beta:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_5/global_self_attention_5/encoder_layer_3/query/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_5/global_self_attention_5/encoder_layer_3/query/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_5/global_self_attention_5/encoder_layer_3/key/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_5/global_self_attention_5/encoder_layer_3/key/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_5/global_self_attention_5/encoder_layer_3/value/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_5/global_self_attention_5/encoder_layer_3/value/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_5/global_self_attention_5/encoder_layer_3/attention_output/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_5/global_self_attention_5/encoder_layer_3/attention_output/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_5/global_self_attention_5/layer_normalization_10/gamma:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_5/global_self_attention_5/layer_normalization_10/beta:0', 'dense_10/kernel:0', 'dense_10/bias:0', 'dense_11/kernel:0', 'dense_11/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_5/feed_forward_5/layer_normalization_11/gamma:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_5/feed_forward_5/layer_normalization_11/beta:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_6/global_self_attention_6/encoder_layer_4/query/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_6/global_self_attention_6/encoder_layer_4/query/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_6/global_self_attention_6/encoder_layer_4/key/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_6/global_self_attention_6/encoder_layer_4/key/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_6/global_self_attention_6/encoder_layer_4/value/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_6/global_self_attention_6/encoder_layer_4/value/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_6/global_self_attention_6/encoder_layer_4/attention_output/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_6/global_self_attention_6/encoder_layer_4/attention_output/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_6/global_self_attention_6/layer_normalization_12/gamma:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_6/global_self_attention_6/layer_normalization_12/beta:0', 'dense_12/kernel:0', 'dense_12/bias:0', 'dense_13/kernel:0', 'dense_13/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_6/feed_forward_6/layer_normalization_13/gamma:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_6/feed_forward_6/layer_normalization_13/beta:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_7/global_self_attention_7/encoder_layer_5/query/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_7/global_self_attention_7/encoder_layer_5/query/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_7/global_self_attention_7/encoder_layer_5/key/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_7/global_self_attention_7/encoder_layer_5/key/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_7/global_self_attention_7/encoder_layer_5/value/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_7/global_self_attention_7/encoder_layer_5/value/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_7/global_self_attention_7/encoder_layer_5/attention_output/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_7/global_self_attention_7/encoder_layer_5/attention_output/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_7/global_self_attention_7/layer_normalization_14/gamma:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_7/global_self_attention_7/layer_normalization_14/beta:0', 'dense_14/kernel:0', 'dense_14/bias:0', 'dense_15/kernel:0', 'dense_15/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_7/feed_forward_7/layer_normalization_15/gamma:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_7/feed_forward_7/layer_normalization_15/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hierarchical_transformer_encoder/encoder_1/encoder_layer_3/global_self_attention_3/encoder_layer_1/query/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_3/global_self_attention_3/encoder_layer_1/query/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_3/global_self_attention_3/encoder_layer_1/key/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_3/global_self_attention_3/encoder_layer_1/key/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_3/global_self_attention_3/encoder_layer_1/value/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_3/global_self_attention_3/encoder_layer_1/value/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_3/global_self_attention_3/encoder_layer_1/attention_output/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_3/global_self_attention_3/encoder_layer_1/attention_output/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_3/global_self_attention_3/layer_normalization_6/gamma:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_3/global_self_attention_3/layer_normalization_6/beta:0', 'dense_6/kernel:0', 'dense_6/bias:0', 'dense_7/kernel:0', 'dense_7/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_3/feed_forward_3/layer_normalization_7/gamma:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_3/feed_forward_3/layer_normalization_7/beta:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_4/global_self_attention_4/encoder_layer_2/query/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_4/global_self_attention_4/encoder_layer_2/query/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_4/global_self_attention_4/encoder_layer_2/key/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_4/global_self_attention_4/encoder_layer_2/key/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_4/global_self_attention_4/encoder_layer_2/value/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_4/global_self_attention_4/encoder_layer_2/value/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_4/global_self_attention_4/encoder_layer_2/attention_output/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_4/global_self_attention_4/encoder_layer_2/attention_output/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_4/global_self_attention_4/layer_normalization_8/gamma:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_4/global_self_attention_4/layer_normalization_8/beta:0', 'dense_8/kernel:0', 'dense_8/bias:0', 'dense_9/kernel:0', 'dense_9/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_4/feed_forward_4/layer_normalization_9/gamma:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_4/feed_forward_4/layer_normalization_9/beta:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_5/global_self_attention_5/encoder_layer_3/query/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_5/global_self_attention_5/encoder_layer_3/query/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_5/global_self_attention_5/encoder_layer_3/key/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_5/global_self_attention_5/encoder_layer_3/key/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_5/global_self_attention_5/encoder_layer_3/value/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_5/global_self_attention_5/encoder_layer_3/value/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_5/global_self_attention_5/encoder_layer_3/attention_output/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_5/global_self_attention_5/encoder_layer_3/attention_output/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_5/global_self_attention_5/layer_normalization_10/gamma:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_5/global_self_attention_5/layer_normalization_10/beta:0', 'dense_10/kernel:0', 'dense_10/bias:0', 'dense_11/kernel:0', 'dense_11/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_5/feed_forward_5/layer_normalization_11/gamma:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_5/feed_forward_5/layer_normalization_11/beta:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_6/global_self_attention_6/encoder_layer_4/query/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_6/global_self_attention_6/encoder_layer_4/query/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_6/global_self_attention_6/encoder_layer_4/key/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_6/global_self_attention_6/encoder_layer_4/key/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_6/global_self_attention_6/encoder_layer_4/value/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_6/global_self_attention_6/encoder_layer_4/value/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_6/global_self_attention_6/encoder_layer_4/attention_output/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_6/global_self_attention_6/encoder_layer_4/attention_output/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_6/global_self_attention_6/layer_normalization_12/gamma:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_6/global_self_attention_6/layer_normalization_12/beta:0', 'dense_12/kernel:0', 'dense_12/bias:0', 'dense_13/kernel:0', 'dense_13/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_6/feed_forward_6/layer_normalization_13/gamma:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_6/feed_forward_6/layer_normalization_13/beta:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_7/global_self_attention_7/encoder_layer_5/query/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_7/global_self_attention_7/encoder_layer_5/query/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_7/global_self_attention_7/encoder_layer_5/key/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_7/global_self_attention_7/encoder_layer_5/key/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_7/global_self_attention_7/encoder_layer_5/value/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_7/global_self_attention_7/encoder_layer_5/value/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_7/global_self_attention_7/encoder_layer_5/attention_output/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_7/global_self_attention_7/encoder_layer_5/attention_output/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_7/global_self_attention_7/layer_normalization_14/gamma:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_7/global_self_attention_7/layer_normalization_14/beta:0', 'dense_14/kernel:0', 'dense_14/bias:0', 'dense_15/kernel:0', 'dense_15/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_7/feed_forward_7/layer_normalization_15/gamma:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_7/feed_forward_7/layer_normalization_15/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hierarchical_transformer_encoder/encoder_1/encoder_layer_3/global_self_attention_3/encoder_layer_1/query/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_3/global_self_attention_3/encoder_layer_1/query/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_3/global_self_attention_3/encoder_layer_1/key/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_3/global_self_attention_3/encoder_layer_1/key/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_3/global_self_attention_3/encoder_layer_1/value/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_3/global_self_attention_3/encoder_layer_1/value/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_3/global_self_attention_3/encoder_layer_1/attention_output/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_3/global_self_attention_3/encoder_layer_1/attention_output/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_3/global_self_attention_3/layer_normalization_6/gamma:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_3/global_self_attention_3/layer_normalization_6/beta:0', 'dense_6/kernel:0', 'dense_6/bias:0', 'dense_7/kernel:0', 'dense_7/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_3/feed_forward_3/layer_normalization_7/gamma:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_3/feed_forward_3/layer_normalization_7/beta:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_4/global_self_attention_4/encoder_layer_2/query/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_4/global_self_attention_4/encoder_layer_2/query/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_4/global_self_attention_4/encoder_layer_2/key/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_4/global_self_attention_4/encoder_layer_2/key/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_4/global_self_attention_4/encoder_layer_2/value/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_4/global_self_attention_4/encoder_layer_2/value/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_4/global_self_attention_4/encoder_layer_2/attention_output/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_4/global_self_attention_4/encoder_layer_2/attention_output/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_4/global_self_attention_4/layer_normalization_8/gamma:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_4/global_self_attention_4/layer_normalization_8/beta:0', 'dense_8/kernel:0', 'dense_8/bias:0', 'dense_9/kernel:0', 'dense_9/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_4/feed_forward_4/layer_normalization_9/gamma:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_4/feed_forward_4/layer_normalization_9/beta:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_5/global_self_attention_5/encoder_layer_3/query/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_5/global_self_attention_5/encoder_layer_3/query/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_5/global_self_attention_5/encoder_layer_3/key/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_5/global_self_attention_5/encoder_layer_3/key/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_5/global_self_attention_5/encoder_layer_3/value/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_5/global_self_attention_5/encoder_layer_3/value/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_5/global_self_attention_5/encoder_layer_3/attention_output/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_5/global_self_attention_5/encoder_layer_3/attention_output/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_5/global_self_attention_5/layer_normalization_10/gamma:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_5/global_self_attention_5/layer_normalization_10/beta:0', 'dense_10/kernel:0', 'dense_10/bias:0', 'dense_11/kernel:0', 'dense_11/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_5/feed_forward_5/layer_normalization_11/gamma:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_5/feed_forward_5/layer_normalization_11/beta:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_6/global_self_attention_6/encoder_layer_4/query/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_6/global_self_attention_6/encoder_layer_4/query/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_6/global_self_attention_6/encoder_layer_4/key/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_6/global_self_attention_6/encoder_layer_4/key/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_6/global_self_attention_6/encoder_layer_4/value/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_6/global_self_attention_6/encoder_layer_4/value/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_6/global_self_attention_6/encoder_layer_4/attention_output/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_6/global_self_attention_6/encoder_layer_4/attention_output/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_6/global_self_attention_6/layer_normalization_12/gamma:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_6/global_self_attention_6/layer_normalization_12/beta:0', 'dense_12/kernel:0', 'dense_12/bias:0', 'dense_13/kernel:0', 'dense_13/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_6/feed_forward_6/layer_normalization_13/gamma:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_6/feed_forward_6/layer_normalization_13/beta:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_7/global_self_attention_7/encoder_layer_5/query/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_7/global_self_attention_7/encoder_layer_5/query/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_7/global_self_attention_7/encoder_layer_5/key/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_7/global_self_attention_7/encoder_layer_5/key/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_7/global_self_attention_7/encoder_layer_5/value/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_7/global_self_attention_7/encoder_layer_5/value/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_7/global_self_attention_7/encoder_layer_5/attention_output/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_7/global_self_attention_7/encoder_layer_5/attention_output/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_7/global_self_attention_7/layer_normalization_14/gamma:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_7/global_self_attention_7/layer_normalization_14/beta:0', 'dense_14/kernel:0', 'dense_14/bias:0', 'dense_15/kernel:0', 'dense_15/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_7/feed_forward_7/layer_normalization_15/gamma:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_7/feed_forward_7/layer_normalization_15/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hierarchical_transformer_encoder/encoder_1/encoder_layer_3/global_self_attention_3/encoder_layer_1/query/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_3/global_self_attention_3/encoder_layer_1/query/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_3/global_self_attention_3/encoder_layer_1/key/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_3/global_self_attention_3/encoder_layer_1/key/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_3/global_self_attention_3/encoder_layer_1/value/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_3/global_self_attention_3/encoder_layer_1/value/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_3/global_self_attention_3/encoder_layer_1/attention_output/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_3/global_self_attention_3/encoder_layer_1/attention_output/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_3/global_self_attention_3/layer_normalization_6/gamma:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_3/global_self_attention_3/layer_normalization_6/beta:0', 'dense_6/kernel:0', 'dense_6/bias:0', 'dense_7/kernel:0', 'dense_7/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_3/feed_forward_3/layer_normalization_7/gamma:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_3/feed_forward_3/layer_normalization_7/beta:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_4/global_self_attention_4/encoder_layer_2/query/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_4/global_self_attention_4/encoder_layer_2/query/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_4/global_self_attention_4/encoder_layer_2/key/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_4/global_self_attention_4/encoder_layer_2/key/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_4/global_self_attention_4/encoder_layer_2/value/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_4/global_self_attention_4/encoder_layer_2/value/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_4/global_self_attention_4/encoder_layer_2/attention_output/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_4/global_self_attention_4/encoder_layer_2/attention_output/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_4/global_self_attention_4/layer_normalization_8/gamma:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_4/global_self_attention_4/layer_normalization_8/beta:0', 'dense_8/kernel:0', 'dense_8/bias:0', 'dense_9/kernel:0', 'dense_9/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_4/feed_forward_4/layer_normalization_9/gamma:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_4/feed_forward_4/layer_normalization_9/beta:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_5/global_self_attention_5/encoder_layer_3/query/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_5/global_self_attention_5/encoder_layer_3/query/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_5/global_self_attention_5/encoder_layer_3/key/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_5/global_self_attention_5/encoder_layer_3/key/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_5/global_self_attention_5/encoder_layer_3/value/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_5/global_self_attention_5/encoder_layer_3/value/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_5/global_self_attention_5/encoder_layer_3/attention_output/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_5/global_self_attention_5/encoder_layer_3/attention_output/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_5/global_self_attention_5/layer_normalization_10/gamma:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_5/global_self_attention_5/layer_normalization_10/beta:0', 'dense_10/kernel:0', 'dense_10/bias:0', 'dense_11/kernel:0', 'dense_11/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_5/feed_forward_5/layer_normalization_11/gamma:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_5/feed_forward_5/layer_normalization_11/beta:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_6/global_self_attention_6/encoder_layer_4/query/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_6/global_self_attention_6/encoder_layer_4/query/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_6/global_self_attention_6/encoder_layer_4/key/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_6/global_self_attention_6/encoder_layer_4/key/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_6/global_self_attention_6/encoder_layer_4/value/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_6/global_self_attention_6/encoder_layer_4/value/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_6/global_self_attention_6/encoder_layer_4/attention_output/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_6/global_self_attention_6/encoder_layer_4/attention_output/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_6/global_self_attention_6/layer_normalization_12/gamma:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_6/global_self_attention_6/layer_normalization_12/beta:0', 'dense_12/kernel:0', 'dense_12/bias:0', 'dense_13/kernel:0', 'dense_13/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_6/feed_forward_6/layer_normalization_13/gamma:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_6/feed_forward_6/layer_normalization_13/beta:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_7/global_self_attention_7/encoder_layer_5/query/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_7/global_self_attention_7/encoder_layer_5/query/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_7/global_self_attention_7/encoder_layer_5/key/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_7/global_self_attention_7/encoder_layer_5/key/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_7/global_self_attention_7/encoder_layer_5/value/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_7/global_self_attention_7/encoder_layer_5/value/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_7/global_self_attention_7/encoder_layer_5/attention_output/kernel:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_7/global_self_attention_7/encoder_layer_5/attention_output/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_7/global_self_attention_7/layer_normalization_14/gamma:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_7/global_self_attention_7/layer_normalization_14/beta:0', 'dense_14/kernel:0', 'dense_14/bias:0', 'dense_15/kernel:0', 'dense_15/bias:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_7/feed_forward_7/layer_normalization_15/gamma:0', 'hierarchical_transformer_encoder/encoder_1/encoder_layer_7/feed_forward_7/layer_normalization_15/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:  37%|███▋      | 69/187 [00:37<00:52,  2.24it/s, loss: 9.1607] "
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "model.fit(\n",
    "    [[input_sequences_np, input_sentences_lengths_np], [character_level_input_sequences_np, input_words_lengths_np]],\n",
    "    [output_sequences_np, correct_infos_np], epochs=config.EPOCHS,\n",
    "    batch_size=config.BATCH_SIZE)"
   ],
   "id": "280aa628ac5ce4ac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "test_output = model.predict([[input_sequences_np[:100], input_sentences_lengths_np[:100]],\n",
    "                             [character_level_input_sequences_np[:100], input_words_lengths_np[:100]]])\n",
    "\n",
    "for sentence in test_output[:3][0]:\n",
    "    out = ''\n",
    "    for word in sentence:\n",
    "        index = tf.argmax(word, axis=0).numpy()\n",
    "        word_str = word_level_tokenizer.index_word.get(index)\n",
    "        if word_str is not None:\n",
    "            out += word_str + ' '\n",
    "        else:\n",
    "            out += '<UNK> '\n",
    "    print(out)\n",
    "\n",
    "print(test_output[1][11])"
   ],
   "id": "845b01dd701239d0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "correct_infos_np[11]",
   "id": "887a9ddaccd357ca",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
