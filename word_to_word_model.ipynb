{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T08:30:55.383463Z",
     "start_time": "2024-04-17T08:30:46.118622Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from Config import Config\n",
    "\n",
    "json_sentences = []\n",
    "\n",
    "with open('data/VSEC.jsonl', 'r') as file:\n",
    "    for line in file:\n",
    "        json_obj = json.loads(line[:-1])\n",
    "        json_sentences.append(json_obj)\n",
    "\n",
    "error_texts = []\n",
    "correct_texts = []\n",
    "correct_infos = []\n",
    "\n",
    "for json_sentence in json_sentences:\n",
    "    error_text = []\n",
    "    correct_text = []\n",
    "    correct_info = []\n",
    "    for word in json_sentence['annotations']:\n",
    "        correct_info.append(word['is_correct'])\n",
    "\n",
    "        current_word = word['current_syllable'].lower()\n",
    "        error_text.append(current_word)\n",
    "\n",
    "        if word['is_correct'] is True:\n",
    "            correct_text.append(current_word)\n",
    "        else:\n",
    "            correct_text.append(word['alternative_syllables'][0].lower())\n",
    "            # if len(word['alternative_syllables']) > 1: print(word)\n",
    "    error_texts.append(error_text)\n",
    "    correct_texts.append(correct_text)\n",
    "    correct_infos.append(correct_info)\n",
    "# Main\n",
    "config = Config()\n",
    "\n",
    "word_level_tokenizer = Tokenizer(num_words=config.VOCAB_SIZE, oov_token='<UNK>', lower=True)\n",
    "word_unk_level_tokenizer = Tokenizer(oov_token='<UNK>', lower=True)\n",
    "character_level_tokenizer = Tokenizer(lower=True, char_level=True)\n",
    "\n",
    "word_level_tokenizer.fit_on_texts(correct_texts)\n",
    "word_unk_level_tokenizer.fit_on_texts(error_texts)\n",
    "\n",
    "import itertools\n",
    "\n",
    "flattened_sentences = list(itertools.chain(*(error_texts + correct_texts)))\n",
    "character_level_tokenizer.fit_on_texts(flattened_sentences)"
   ],
   "id": "dc8b24f26c3728a9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-17 15:30:46.645818: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-17 15:30:46.754482: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-17 15:30:48.463166: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T08:30:55.398878Z",
     "start_time": "2024-04-17T08:30:55.385751Z"
    }
   },
   "cell_type": "code",
   "source": "len(correct_texts)",
   "id": "22aacf012ec834ce",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9341"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T08:30:59.390297Z",
     "start_time": "2024-04-17T08:30:55.401649Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "input_sequences = word_level_tokenizer.texts_to_sequences(error_texts)\n",
    "output_sequences = word_level_tokenizer.texts_to_sequences(correct_texts)\n",
    "unk_input_sequences = word_unk_level_tokenizer.texts_to_sequences(error_texts)\n",
    "\n",
    "# Get character-level words lengths.\n",
    "input_words_lengths = []\n",
    "\n",
    "# Get character-level sequences.\n",
    "character_level_input_sequences = []\n",
    "\n",
    "for sequence in unk_input_sequences:\n",
    "    character_level_input_sequence = []\n",
    "    words_lengths = []\n",
    "    for word_token in sequence:\n",
    "        word = word_unk_level_tokenizer.index_word[word_token]\n",
    "        word = character_level_tokenizer.texts_to_sequences(word)\n",
    "        word_chars = [each[0] for each in word]\n",
    "        character_level_input_sequence.append(word_chars)\n",
    "        words_lengths.append((len(word_chars) if len(word_chars) <= config.MAX_WORD_LENGTH\n",
    "                              else config.MAX_WORD_LENGTH))\n",
    "\n",
    "    # Add padding for each word.\n",
    "    character_level_input_sequence = pad_sequences(character_level_input_sequence, maxlen=config.MAX_WORD_LENGTH,\n",
    "                                                   padding='post', truncating='post')\n",
    "\n",
    "    character_level_input_sequences.append(character_level_input_sequence)\n",
    "\n",
    "    input_words_lengths.append(words_lengths)\n",
    "\n",
    "# Get word-level sentences lengths.\n",
    "input_sentences_lengths = []\n",
    "for sequence in input_sequences: input_sentences_lengths.append(\n",
    "    (len(sequence) if len(sequence) <= config.MAX_SENTENCE_LENGTH\n",
    "     else config.MAX_SENTENCE_LENGTH))\n",
    "\n",
    "# Add padding for each.\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=config.MAX_SENTENCE_LENGTH, padding='post', truncating='post')\n",
    "output_sequences = pad_sequences(output_sequences, maxlen=config.MAX_SENTENCE_LENGTH, padding='post', truncating='post')\n",
    "character_level_input_sequences = pad_sequences(character_level_input_sequences, maxlen=config.MAX_SENTENCE_LENGTH,\n",
    "                                                padding='post', truncating='post')\n",
    "input_words_lengths = pad_sequences(input_words_lengths, maxlen=config.MAX_SENTENCE_LENGTH, padding='post',\n",
    "                                    truncating='post')\n",
    "correct_infos = pad_sequences(correct_infos, maxlen=config.MAX_SENTENCE_LENGTH, padding='post', value=1)\n",
    "\n",
    "input_sequences_np = np.array(input_sequences)\n",
    "character_level_input_sequences_np = np.array(character_level_input_sequences)\n",
    "output_sequences_np = np.array(output_sequences)\n",
    "\n",
    "input_words_lengths_np = np.array(input_words_lengths)\n",
    "input_sentences_lengths_np = np.array(input_sentences_lengths)\n",
    "correct_infos_np = np.array(correct_infos)"
   ],
   "id": "b04c7f90b2c479e4",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T08:31:00.347722Z",
     "start_time": "2024-04-17T08:31:00.076700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from HierarchicalTransformerEncoder import HierarchicalTransformerEncoder\n",
    "\n",
    "model = HierarchicalTransformerEncoder(num_character_level_layers=config.NUM_CHARACTER_LEVEL_LAYERS,\n",
    "                                       num_word_level_layers=config.NUM_WORD_LEVEL_LAYERS,\n",
    "                                       character_level_d_model=config.CHARACTER_LEVEL_D_MODEL,\n",
    "                                       word_level_d_model=config.WORD_LEVEL_D_MODEL,\n",
    "                                       num_heads=config.NUM_HEADS, dff=config.DFF,\n",
    "                                       max_word_length=config.MAX_WORD_LENGTH,\n",
    "                                       max_sentence_length=config.MAX_SENTENCE_LENGTH,\n",
    "                                       vocab_size=config.VOCAB_SIZE,\n",
    "                                       character_vocab_size=config.CHARACTER_VOCAB_SIZE)"
   ],
   "id": "82d3104dfc17ed18",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thanhan/.local/lib/python3.10/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T08:31:02.014496Z",
     "start_time": "2024-04-17T08:31:01.978205Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from CustomSchedule import CustomSchedule\n",
    "\n",
    "learning_rate = CustomSchedule(config.WORD_LEVEL_D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
    "                                     epsilon=1e-9)"
   ],
   "id": "fb68d56c770a68ce",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T08:22:48.270379Z",
     "start_time": "2024-04-17T08:22:48.256425Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# word_input_shape = (config.BATCH_SIZE, config.MAX_SENTENCE_LENGTH)\n",
    "# char_input_shape = (config.BATCH_SIZE, config.MAX_SENTENCE_LENGTH, config.MAX_WORD_LENGTH)\n",
    "# sentence_lengths_shape = (config.BATCH_SIZE, config.MAX_SENTENCE_LENGTH)\n",
    "# word_lengths_shape = (config.BATCH_SIZE, config.MAX_SENTENCE_LENGTH, config.MAX_WORD_LENGTH)\n",
    "\n",
    "# model.build(input_shape=[[word_input_shape, sentence_lengths_shape], [char_input_shape, word_lengths_shape]])\n",
    "# model.compile(optimizer=optimizer, metrics=['acc'])"
   ],
   "id": "2e767574e8c599a7",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T09:02:01.611283Z",
     "start_time": "2024-04-17T08:31:08.931736Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from HierarchicalTransformerEncoder import training_step\n",
    "from tqdm import tqdm\n",
    "\n",
    "for e in range(config.EPOCHS):\n",
    "    pbar = tqdm(range(0, len(input_sequences_np), config.BATCH_SIZE))\n",
    "\n",
    "    for i in pbar:\n",
    "        input_batch = [\n",
    "            [input_sequences_np[i:i + config.BATCH_SIZE], input_sentences_lengths_np[i:i + config.BATCH_SIZE]],\n",
    "            [character_level_input_sequences_np[i:i + config.BATCH_SIZE],\n",
    "             input_words_lengths_np[i:i + config.BATCH_SIZE]]\n",
    "        ]\n",
    "        output_batch = [\n",
    "            output_sequences_np[i:i + config.BATCH_SIZE],\n",
    "            correct_infos_np[i:i + config.BATCH_SIZE]\n",
    "        ]\n",
    "\n",
    "        total_loss = training_step(model, optimizer, input_batch, output_batch)\n",
    "\n",
    "        pbar.set_description(f'Epoch {e + 1}/{config.EPOCHS}')\n",
    "        pbar.set_postfix_str(f'loss: {total_loss:.4f}')\n",
    "        pbar.refresh()\n"
   ],
   "id": "3d89bd411edb5399",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/47 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/thanhan/.local/lib/python3.10/site-packages/tensorflow/python/util/deprecation.py:660: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thanhan/.local/lib/python3.10/site-packages/keras/src/layers/layer.py:856: UserWarning: Layer 'query' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/thanhan/.local/lib/python3.10/site-packages/keras/src/layers/layer.py:856: UserWarning: Layer 'key' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/thanhan/.local/lib/python3.10/site-packages/keras/src/layers/layer.py:856: UserWarning: Layer 'value' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/thanhan/.local/lib/python3.10/site-packages/keras/src/optimizers/base_optimizer.py:583: UserWarning: Gradients do not exist for variables ['kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  warnings.warn(\n",
      "Epoch 1/10: 100%|██████████| 47/47 [03:35<00:00,  4.58s/it, loss: 9.3254] \n",
      "Epoch 2/10: 100%|██████████| 47/47 [02:52<00:00,  3.66s/it, loss: 8.6950]\n",
      "Epoch 3/10: 100%|██████████| 47/47 [02:55<00:00,  3.74s/it, loss: 7.9965]\n",
      "Epoch 4/10: 100%|██████████| 47/47 [03:09<00:00,  4.03s/it, loss: 7.3039]\n",
      "Epoch 5/10: 100%|██████████| 47/47 [03:04<00:00,  3.93s/it, loss: 6.5428]\n",
      "Epoch 6/10: 100%|██████████| 47/47 [03:05<00:00,  3.95s/it, loss: 5.7288]\n",
      "Epoch 7/10: 100%|██████████| 47/47 [03:03<00:00,  3.90s/it, loss: 4.8023]\n",
      "Epoch 8/10: 100%|██████████| 47/47 [03:07<00:00,  3.98s/it, loss: 3.8613]\n",
      "Epoch 9/10: 100%|██████████| 47/47 [02:55<00:00,  3.74s/it, loss: 3.1106]\n",
      "Epoch 10/10: 100%|██████████| 47/47 [03:03<00:00,  3.90s/it, loss: 2.4582]\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# model.fit(\n",
    "#     [[input_sequences_np, input_sentences_lengths_np], [character_level_input_sequences_np, input_words_lengths_np]],\n",
    "#     [output_sequences_np, correct_infos_np], epochs=config.EPOCHS,\n",
    "#     batch_size=config.BATCH_SIZE)"
   ],
   "id": "280aa628ac5ce4ac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_sentence_inputs = ['hom nay troi dep qua']\n",
    "test_word_level_sequences = word_level_tokenizer.texts_to_sequences(test_sentence_inputs)\n",
    "test_sentence_lengths = [(len(sentence) if len(sentence) < config.MAX_SENTENCE_LENGTH else config.MAX_SENTENCE_LENGTH)\n",
    "                         for sentence in test_word_level_sequences]\n",
    "\n",
    "test_output = model.predict([[input_sequences_np[:100], input_sentences_lengths_np[:100]],\n",
    "                             [character_level_input_sequences_np[:100], input_words_lengths_np[:100]]])\n",
    "\n",
    "for sentence in test_output[0][10:11]:\n",
    "    out = ''\n",
    "    for word in sentence:\n",
    "        index = tf.argmax(word, axis=0).numpy()\n",
    "        print(index)\n",
    "        word_str = word_level_tokenizer.index_word.get(index)\n",
    "        if word_str is not None:\n",
    "            out += word_str + ' '\n",
    "        else:\n",
    "            out += '<UNK> '\n",
    "    print(out)\n",
    "print(tf.argmax(test_output[0][10][5], axis=0).numpy())\n"
   ],
   "id": "845b01dd701239d0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T09:04:34.131101Z",
     "start_time": "2024-04-17T09:04:34.094537Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_sentence_inputs = ['việc điều khiển xe sẽ trở nên rễ dàng hơn nếu các bánh xe được đặt theo một góc chính xác theo yêu cầu thiết kế, việc đặt saii góc sẽ tạo ra tính ổn định khi lái kém và khiến lốp xe nhanh mòn.']\n",
    "\n",
    "test_word_level_sequences = word_level_tokenizer.texts_to_sequences(test_sentence_inputs)\n",
    "test_sentence_lengths = [(len(sentence) if len(sentence) < config.MAX_SENTENCE_LENGTH else config.MAX_SENTENCE_LENGTH)\n",
    "                         for sentence in test_word_level_sequences]\n",
    "test_word_level_sequences = pad_sequences(test_word_level_sequences, maxlen=config.MAX_SENTENCE_LENGTH, padding='post',\n",
    "                                          truncating='post')\n",
    "\n",
    "character_level_tokenizer.fit_on_texts(test_sentence_inputs)\n",
    "word_unk_level_tokenizer.fit_on_texts(test_sentence_inputs)\n",
    "test_unk_input_sequences = word_unk_level_tokenizer.texts_to_sequences(test_sentence_inputs)\n",
    "\n",
    "test_character_level_input_sequences = []\n",
    "test_words_lengths = []\n",
    "for sequence in test_unk_input_sequences:\n",
    "    character_level_input_sequence = []\n",
    "    words_lengths = []\n",
    "    for word_token in sequence:\n",
    "        word = word_unk_level_tokenizer.index_word[word_token]\n",
    "        word = character_level_tokenizer.texts_to_sequences(word)\n",
    "        word_chars = [each[0] for each in word]\n",
    "        character_level_input_sequence.append(word_chars)\n",
    "        words_lengths.append((len(word_chars) if len(word_chars) <= config.MAX_WORD_LENGTH\n",
    "                              else config.MAX_WORD_LENGTH))\n",
    "\n",
    "    character_level_input_sequence = pad_sequences(character_level_input_sequence, maxlen=config.MAX_WORD_LENGTH,\n",
    "                                                   padding='post', truncating='post')\n",
    "\n",
    "    test_character_level_input_sequences.append(character_level_input_sequence)\n",
    "\n",
    "    test_words_lengths.append(words_lengths)\n",
    "\n",
    "test_character_level_input_sequences = pad_sequences(test_character_level_input_sequences, maxlen=config.MAX_SENTENCE_LENGTH,\n",
    "                                                padding='post', truncating='post')\n",
    "test_words_lengths = pad_sequences(test_words_lengths, maxlen=config.MAX_SENTENCE_LENGTH, padding='post',\n",
    "                                   truncating='post')"
   ],
   "id": "887a9ddaccd357ca",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T09:04:37.903442Z",
     "start_time": "2024-04-17T09:04:37.897110Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_character_level_input_sequences = np.array(test_character_level_input_sequences)\n",
    "test_words_lengths = np.array(test_words_lengths)\n",
    "test_sentence_lengths = np.array(test_sentence_lengths)\n",
    "test_word_level_sequences = np.array(test_word_level_sequences)"
   ],
   "id": "c4bd5623cac4449f",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T09:04:39.244357Z",
     "start_time": "2024-04-17T09:04:39.103311Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_outputs = model.predict(\n",
    "    [[test_word_level_sequences, test_sentence_lengths], [test_character_level_input_sequences, test_words_lengths]])"
   ],
   "id": "dea35fe03701c927",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 45ms/step\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T09:04:40.524888Z",
     "start_time": "2024-04-17T09:04:40.504822Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for sentence in test_outputs[0]:\n",
    "    out = ''\n",
    "    for word in sentence:\n",
    "        index = tf.argmax(word, axis=0).numpy()\n",
    "        word_str = word_level_tokenizer.index_word.get(index)\n",
    "        if word_str is not None:\n",
    "            out += word_str + ' '\n",
    "        else:\n",
    "            out += '<UNK> '\n",
    "    print(out)"
   ],
   "id": "e9a607df2c04a1b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "việc điều phân về sẽ trở nên đến chính hơn nếu các từ xe được đặt theo một góc chính xác theo yêu cầu thiết kế việc đặt <UNK> sự sẽ tạo ra tính ổn định khi lại về và không các về vào về <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> \n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "test_outputs[1]",
   "id": "45d3183852c77930",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "output_sequences_np[1]",
   "id": "9394e6287f869afc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "input_sequences_np[1]",
   "id": "7a85d91df66ef258",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
